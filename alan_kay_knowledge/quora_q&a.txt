What are your thoughts on a future where code is represented as a structured model, rather than text? Do you think that AI-powered coding assistants benefit from that?
First, it’s worth noting that the “syntax recognition” phase of most compilers already does build a “structured model”, often in what used to be called a “canonical form” (an example of this might be a “pseudo-function tree” where every elementary process description is put into the same form — so both “a + b” and “for i := 1 to x do […]” are rendered as  plus(a, b)      for(i, 1, x, […])  in structures, such as:     +                     for    / \    and            ⁄ / | \    a   b                i 1  x  []  i.e. “operator like things” at the nodes …  Another canonical form could be Lisp S-expressions, etc.  These structures are made precisely to allow programs to “reason” about some parts of lower level meaning, and in many cases to rearrange the structure to preserve meaning but to make the eventual code that is generated more efficient.  Long ago in the 50s this was even thought of as a kind of “AI” and this association persisted into the 60s. Several Turing Awards were given for progress on this kind of “machine reasoning”.  Most coding assistants — with or without “modern “AI” — also do reasoning and manipulation of structures.  It’s important to realize that “modern “AI” doesn’t understand human level meanings any better today (in many cases: worse!). So it is not going to be able to serve as much of a helper in a general coding assistant.  A slogan that might help you get past the current fads is:  NOT DATA … BUT MEANING!



Why did Japan’s Fifth Generation Computer Systems project fail commercially? Are any of its concepts and advances presently being utilized?
The answer by Marcus Triska sums up much of the history and context rather well Why did the Japanese "Fifth Generation" project to reinvent computing based on parallel logic programming "fail"? And what can be learned from it? Are there results that might be useful to us now? (Eg. interesting operating systems to look at etc?) https://www.quora.com/Why-did-the-Japanese-Fifth-Generation-project-to-reinvent-computing-based-on-parallel-logic-programming-fail-And-what-can-be-learned-from-it-Are-there-results-that-might-be-useful-to-us-now-Eg-interesting-operating?no_redirect=1 I will just add to this a few impressions from the early years of this project. The Japanese project leaders were quite open about the project and invited Ed Feigenbaum to select 20 or 30 American computer scientists to visit periodically and discuss the project, etc. This group included Ed, John McCarthy, Marvin Minsky, many other luminaries in the field, and a “fringe member” (myself). These visits were quite fun, and the American group itself got along quite well (we all liked to argue in a fairly reasonable way). The Japanese scientists in the 80s adopted the sensible approach of ARPA/Parc in the 60s and 70s of trying to define and write enough SW to be able to design HW that would optimize difficult important parts that were not served by conventional architectures. The large scale and ability of the Japanese government to both fund and “coax” companies to cooperate at various levels and standards started off well. Some of the details were doubtful (IMO, etc.). Some of us felt that it would be a better idea to invent a new language that would be more suitable than PROLOG as a base for the large goals (there were a number of ideas in PROLOG that were quite good, and some that needed to be reinvented better). One of the eventual fallouts was Shapiro’s “Concurrent Prolog”, which was a much cleaner, better, way to approach this genre of ideas. I meant to add this in the original answer: Every computerist should know about and have read with a “research eye” Kazunori Ueda’s excellent paper: “Logic/Constraint Programming and Concurrency: The hard-won lessons of the Fifth Generation Computer project”. Many of the issues and suggestions here are very worth pondering in the light of today’s problems (i.e. you will get a lot from reading this regardless of whether you agree or not). A perhaps too simple bottom line is that quite a bit of good research got done — but there was also a lot of constriction to move quickly to showing practicality. It might be possible to do this at the pace that was attempted, but I’ve never witnessed it. The real learning curves in my own experience actually — unfortunately — take much longer than the pace of business. I think this project — overall — helped computer science in general and Japanese computer science in particular. To me it was a research project, and did turn up considerable knowledge of various useful kinds.



How does Alan Kay describe the nature of his way of collaboration? Would he be willing to work with teams that are below average, which could diminish the quality of his work and lower his motivation?
I don’t know the answer to this (I’ve always tried to work with colleagues who are more able than I am — going back to playing in bands, theatre, tennis, science, etc.). Also, ARPA-IPTO had the interesting idea of creating groups of smart people where no-one was the smartest in the room (I think most of us still thought it was Butler Lampson).

I don’t know how one would go about determining “below average” … a much better idea and question could be about “less able”, where the goal would be to help the growth of “more able”. This is definitely a better way to think about children, and I think it works for adults also (in other words, keep the potential of others in mind and alive).

A key notion of “team” is that every member winds up able to get over the bar (because every member is helping every member get over the bar).

A big shift I made in my 20s was from changing my self rating as to how I was doing based on “results” to assessing “quality of effort”. This worked much better because I was much more in control of “quality of effort”, and not worrying about “results” plus higher “quality of effort” tended to produce higher quality results (but now I didn’t care so much about the latter, etc.)

Sorry for the not very good answer.



Since von Neumann invented the merge sort algo in 1945, just before ENIAC was introduced in 1946, what does Alan Kay think motivated him to explore sorting algos when computers were still so new and HLL didn't yet exist?
Richard Norman has already provided a nice short accurate answer to this question.

The notion of “algorithm” goes back at least to the early Greeks (and several are given by Euclid).

Another area that extensively used algorithms before programmable digital computers was “Relaxation Methods in Engineering and Science” — the classic Southwell books date back to the 1940s.

Many of the early processes that ENIAC ran were ballistic calculations that had been done for years by humans using desktop numerical calculators.

HLLs are not needed for programming or for carrying out algorithms. However, a kind of HLL (Flowcharting) was used in many areas to provide an HL portrayal of various calculation schemes.




"Great visions are not goals, but the ultimate cat toy". What does this idea mean to Alan Kay? And could it relate to your belief that ‘you don’t truly understand something if you only understand it one way’?"
I’m trying to understand this question. I used the quoted phrase above in several talks (many are on YouTube), to explain how JCR Licklider in the 60s, was able to get so many highly talented researchers to work on interactive computing and world-wide networking.

The lead-in to this phrase used two fun videos I found on YouTube. The first was a take-off on westerns showing cowboys trying to herd cats.

I then showed a video made by two smart children showing how to get cats to do things.

(In other words, come up with cat toys if you want cats to do things.)

(Researchers are Artists, and thus/also rather like cats.)

Come up with a vision that attracts them and they will start working on it — etc.

(I think you can see that this line of attack is only peripherally related to the second quote (which was by Marvin Minsky) and which relates in some way to most other ideas …. )



What does Alan Kay suggest here: "Computing is not about clocks, it's about biology". How come the essence of computing is more about the way living organisms evolve?
I wouldn’t have said the “quoted phrase” in the way it is stated (so the quotes likely mean something else). And I wouldn’t have said “The essence of computing is more about the way living things evolve” (I would have used a term like “work”, instead of “evolve”). In many talks, long ago, I have shown a picture with the title “From Gears To Biology” as a metaphor for a qualitative shift in thinking about computer systems — from early-bound, tightly coupled rather small mechanisms — like clocks — to late-bound, loosely coupled rather large mechanisms — like living organisms. In computing, this happened within a single lifetime, and many parts of computing are still stuck in the earlier metaphor. We can see the difficulties that general computing has had in adjusting to the great scalings brought by Moore’s Law. For example, the “going off the Internet of Facebook/Meta” and the recent “Crowdstrike” (what a strange title for a computer system!) debacle. Pondering the qualitative differences between gear-like systems and biology-like systems can be very helpful in thinking more clearly about dealing with great scalings in computing systems.



What does Alan Kay think about ADHD in adults? Should it be controlled with medication?
I don’t have the medical background to answer generally. However, I have many creative close friends who have ADHD. Most of them use the drugs to help (they do help greatly) to cope with many actions in the world that are difficult for those with ADHD. When they are deeply within creative phases of their life that work well off the drugs, but not well with them (for example, acting in theatre), they often go off the drugs. I have a very close friend who uses the drug in the morning, and lets it wear off for theatre rehearsals in the evening. These are just annecdotes, and certainly don’t cover the general case. However, all of my ADHD friends look at the drugs as a godsend, and all whom I know use the drugs wisely.



What is Alan Kay’s view on analytic idealism by Bernardo Kastrup?
I’m sure that I need to read his forthcoming book in order to attempt an answer:

Analytic Idealism in a Nutshell: A Straightforward Summary of the 21st Century's Only Plausible Metaphysics

I’ve ordered it — it is not out yet — due to arrive Nov 1st this year (2024).

However, I should reveal a personal prejudice against what I understand is the basic idea. As with most prejudices, it doesn’t have much substantive behind it, but I’ll be trying to keep this in mind when I read the book.

See you in November …

Here are my reactions as of Nov 6th.

I got his book — Analytic Idealism in a Nutshell: A Straightforward Summary of the 21st Century's Only Plausible Metaphysics — and have read it.

The ideas in it are put forth as an essay into Philosophy, particularly focused on Metaphysics (the nature of Being itself).

Wikipedia’s definition is good enough:

Metaphysics is the study of the most general features of reality, including existence, objects and their properties, possibility and necessity, space and time, change, causation, and the relation between matter and mind. It is one of the oldest branches of philosophy.

I wonder if I understand enough about the general subject area — Philosophy in general, and Metaphysics in particular — to make it worthwhile to share my opinions? My thought patterns are primarily within the general outlooks of science, math, engineering, and some of the arts: musical, visual, theatric, literate, etc.

I like — and subscribe to — Einstein’s reminder to scientists in his 1921 talk in Berlin:

" ... as far as the propositions of mathematics refer to reality, they are not certain; and as far as they are certain, they do not refer to reality."

Geometry and Experience, Lecture before the Prussian Academy of Sciences, January 27, 1921

Kastrup starts out with his version of this idea — one I’ve also used many times in talks — that

what we think of as “out there” is actually going on in our brains: “in here”, between our ears.
We create a kind of a map that is its own internal world, and — if we are sophisticated — we realize that our map should not be called “reality”, and at best we have to negotiate between the limitations of our mappings and the phenomenal evidence we can detect. This internal world each human has is sometimes called our “Private Universe”.

Kastrup likens the internal map to a dashboard inside a hermetically sealed airplane where the instruments provide enough information to fly the plane, but do not at all resemble what we’d see if we could look outside the plane (this is a quite good example/analogy).

Kastrup adds several other postulates in his Introduction. Here are his additional four (partially quoting):

There is an external world out there, beyond our physical minds
This world unfolds on its own, according to its own inherent dispositions, and reveals some phenomena our senses (and sense aids) can pick up
Human reason can recognize and model what it thinks are regularities in this phenomena, and in some cases can predict future phenomena
Complex phenomena can be “sufficiently accounted for in terms of simpler ones” (basically non-linear reductionism).
I think most scientists — including me — would agree that these five are highly likely. Kastrup calls these realism.

The writing style of this book is nicely clear, but very repetitious. There is an air of “I need to explain this many times because you are probably not understanding”. The last part might very well be the case, but repetition doesn’t help.

And, he starts arguing right away. It is not at all clear to me that his arguments (a) work, and/or (b) perhaps can be made at all. I am prejudiced in favor of essays which spend a lot of their front matter in exposition and follow this groundwork by argument. This is not done here.

I will try to fit the rest of this within a Quora-sized (albeit one still too long) answer.

One way to approach this is to ask whether his initial premises — which I agree with — actually allow his thesis — that Being is a kind of universal mentality that is very unlike the internal mappings that physical scientists try to make of Nature — to be successfully argued.

In Philosophical terms, the 5 premises above are essentially epistemological. Wikipedia again:

Epistemology, the philosophical study of the nature, origin, and limits of human knowledge

Taken together, it looks as though these premises of Analytic Idealism make it difficult to do more than claim any ultimate knowledge about anything “out there”. I.e. I think that Kastrup can claim his thesis as a proposition — but, given the premises, I don’t think he can substantiate his claim. As Einstein pointed out, logic/math/language/inference with the aim of “certainly” with regard to chains of thought will not refer to “reality” but only — if done as well as possible — to the consistency of the arguments.

For example, an argument of the form “because the thing in question is not this, this or this, it must be *that*” only works in reasoning/logic/math, etc., if you can first show that you have enumerated all the candidates and eliminated all but one.

The needed enumeration is not done here, and I don’t think it can be done.

A book that I’ve enjoyed very much — and which provoked much mulling — was Julian Jaynes’ “The Origin Of Consciousness In The Breakdown Of The Bicameral Mind” (TOOCITBOTBM). It is perhaps my favorite of this kind of book. I doubt its conclusions, but thinking about the issues, evidence, and forms of argument have. quite widened my thoughts over the years.

Analytic Idealism In A Nutshell is also a book that provokes mulling. The subject matter is a few levels more murky and abstract than TOOCITBOTBM, but trying to understand what is being attempted and pondering whether its arguments actually hold water can be quite enjoyable, and to some extent, illuminating.



How does Alan Kay view Don Norman's perspective on humanity-centered design?
I’ve known Don Norman as a colleague and friend for more than 50 years, and I value all his perspectives whether I agree (most of the time I do) or not (sometimes I don’t). Don has had a long illustrious career centered in Cognitive Psychology and associated “human factors”. Don Norman - Wikipedia American researcher, professor, and writer (born 1935) Donald Arthur Norman (born December 25, 1935) [ 2 ] [ 3 ] is an American researcher, professor, and author. Norman is the director of The Design Lab at University of California, San Diego . [ 4 ] He is best known for his books on design, especially The Design of Everyday Things . He is widely regarded for his expertise in the fields of design , usability engineering , and cognitive science , [ 4 ] and has shaped the development of the field of cognitive systems engineering . [ 5 ] He is a co-founder of the Nielsen Norman Group , along with Jakob Nielsen . He is also an IDEO fellow and a member of the Board of Trustees of IIT Institute of Design in Chicago. He also holds the title of Professor Emeritus of Cognitive Science at the University of California, San Diego . Norman is an active Distinguished Visiting Professor at the Korea Advanced Institute of Science and Technology (KAIST), where he spends two months a year teaching. [ when? ] Much of Norman's work involves the advocacy of user-centered design . [ 6 ] His books all have the underlying purpose of furthering the field of design, from doors to computers. Norman has taken a controversial stance in saying that the design research community has had little impact in the innovation of products, and that while academics can help in refining existing products, it is technologists that accomplish the breakthroughs. [ 7 ] To this end, Norman named his website with the initialism JND (just-noticeable difference) to signify his endeavors to make a difference. [ 1 ] In 1957, Norman received a B.S. degree in electrical engineering from Massachusetts Institute of Technology (MIT). [ 8 ] Norman received an M.S. degree in electrical engineering from the University of Pennsylvania. [ 9 ] He received a PhD in psychology from the University of Pennsylvania . [ 9 ] He was one of the earliest graduates from the Mathematical Psychology group at University of Pennsylvania and his advisor was Duncan Luce . [ 9 ] After graduating, Norman took up a postdoctoral fellowship at the Center for Cognitive Studies at Harvard University [ 10 ] [ 11 ] and within a year became a lecturer. After four years with the Center, Norman took a position as an associate professor in the Psychology Department at University of California, San Diego (UCSD). Norman applied his training as an engineer and computer scientist, and as an experimental and mathematical psychologist, to the emerging discipline of cognitive science . Norman eventually became founding chair of the Department of Cognitive Science and chair of the Department of Psychology. At UCSD, Norman was a founder of the Institute for Cognitive Science and one of the organizers of the Cognitive Science Society (along with Roger Schank , Allan Collins , and others), which held its first meeting at the UCSD campus in 1979. [ 12 ] [ non-primary source needed ] Together with psychologist Tim Shallice , Norman proposed a framewo https://en.wikipedia.org/wiki/Don_Norman Don was a consultant for “The Learning Research Group” that I set up at Parc, and contributed greatly to many parts of our research thinking. He liked to argue (in good ways), and so did we, and this wound up covering lots of ground. He and I also worked together at Apple in the 80s. We have remained friends. Etc. A bottom line is that Don’s ideas are a must for anyone thinking about systems which involve people, and especially for systems where attempts are made to better fit them to human users and uses. I’m being a bit elliptical here because the area of “actual UI design” — though there are a few priniciples — is still done best by many experiments with many design variations. This means that a new UI designer needs to absorb an enormous amount of lore — especially from great designers — *and* has to be willing to do the new large amount of work to bring a new UI to useful life. Don’s perspectives are thus deeply important to learn and understand.



Why didn't Doug Engelbart's chording keyset become a standard interface like the mouse did? It requires more training, but would have a dramatic impact on user performance.
Here’s the classic Engelbart NLS interface: This was the result of testing many alternative designs. A basic principle was that the hands should be “out” for navigation and “small text” (so “out” required all text to be able to be entered with the five finger keyboard and three buttons on the mouse), and the hands should be “in” for “longer text” (paragraphs etc) using the standard keyboard. I used to be able to use this system, so I’m trying to remember some of the interesting details. One of these was that “command accept” <A> was the far right button on the mouse. This left five buttons for chords with the left hand, and the left two buttons on the mouse for “shifts”. The “no-shift” default was for commands to the system. So “move word” from one place to another was done as: MW <point and click> <point and click> <A> This is a procedural syntax, is thus modey, and requires an explicit <do it> to terminate (an “object syntax” would select first and drag the word and not need the rest). One of the reasons they stayed with this was that the analog nature of the underlying displays was “drifty” (they used small electrostatic CRTs for better quality with an 875 line TV camera strapped onto the front to blow the image up to a reasonable size — this was called “Mexican scan conversion” back then — now non-PC — it worked well enough). Telling the system what you were trying to point at greatly helped the locator algorithms to find targets in its drifty displays. Learning the system was kinesthetic, and took about 10–20 hours over a few days to internalize. For example, at the start it wasn’t about learning the chord for “M” or “W”, but the muscular motion for “Move Word”. At first when there was an opportunity to type, one would go to the keyboard — but very soon one would just try the chord keyboard — etc. Fluent users could type up to about 30 words a minute with “hands out”. In practical use, this scheme worked pretty well — much of the time was spent with “hands out” navigating, typing “small things”, etc. and the response of the system was sub-second, which encouraged one to get expert. The chord keyboard and two left mouse buttons collected “downs” and sent them as a 7 bit character when all went back “up”. This worked well. The experience was like low level flying at high speed. Note, that the initial ramp is a steep learning curve, but in a week or so you wind up with a “violin” rather than a “kazoo”. The system didn’t have anything built in to help beginners (it was an experimental system, and they were writing code for expert use). It could have been learned by children via a game interface, but that was never done. One of the plans at Parc ca 1971 was to “do NLS on a network of minicomputers” — and there were several notions of how to do this. For example, every Alto came with a three button mouse and a five finger keyboard. The Parc Alto with chord keyboard and 3 button mouse Bill Duvall did a terrific subset of NLS on a Nova 800 with Parc’s experimental display HW, and it rocked. He later did an Alto version when the Altos started to become available. Larry Tesler also used the chord keyboard — not in the way NLS did — for his word processor experiments. However, there was never a complete — really useful — system done at Parc that made central use of the chord keyboard, even though it was an excellent idea. You had to learn it to use NLS at SRI but you didn’t have to use it for the most used systems at Parc — so it didn’t get deeply incorporated into any of the day to day Parc systems. One of the reasons was that quite a lot of the UI research and design at Parc was concerned with general novice end-users, and not with experts who would have explicit training as part of their introduction to the system. It’s worth noting that text entry on an iPhone is not a good way to do it for expert use — so we are given a novice scheme, that — ironically — the end-users then put many many hours into optimizing (but optimizing a bad scheme almost never gives you a good scheme — what you get is a slightly faster version of the bad scheme). This is a quite general phenomenon — and is found in easy start up musical instruments (like guitars, etc.). Many “pretty good” guitar players retain poor technique that has been optimized (for example, playing lines without the pinky on the left hand, and never including the pinky as they get better — at some point this creates enormous barriers for further progress). We find it in many school subjects, where the early versions are presented so badly that what is needed later seems like an entirely new subject (a good example is that “algebra” is really a kind of arithmetic that focuses on what “=” really implies — but for most kids it seems like something they have never seen before). It is not unfair to point out that “marketing people” do not like to sell anything that has a real learning curve — and this has gotten even more restrictive over the years. If we extend that idea to schooling (most teachers and educational institutions don’t like to teach subjects that have real learning curves), then we can see why so many children and adults are “in deep stew” today …



How valuable is it to apply concepts like the Curry-Howard correspondence (proofs as programs) and inductive proofs to the development of dynamic/complex systems, considering their correlation with type systems?
One of Marvin Minsky’s many thought provoking sayings was “The trouble with ‘New Math’ is that you have to understand it every time you use it!”. (Meaning, there is an important pragmatic place for mentally caching “rules that work” rather than working through the reasons for every use. Both are important, and some separation helps.)

Also, I will admit to a prejudice against trying to align multiple languages with multiple ways to assign meanings (this was — and is still — a big problem with formal proofs of regular programming languages).

So, I’d argue for trying to design single languages that can serve the dual purpose of being “good to program in” *and* that automatically try to verify the intentions of the programmer.

I think this is a really interesting and important problem for computerists to work on. If we think in older terms — the spectrum between assembly/C type languages and Lisp/FP type languages — the latter work better, but I think in the end are too “pristine” and not very good at systems — and the former are not expressive enough and are full of pitfalls.

Some form of “real AI” is likely to be too annoying to really help, but it’s hard to imagine pulling off the goals here without an expert system of some kind to help.

Going way back, Ivan Sutherland’s Sketchpad continuously showed you the consequences of the intentions you expressed. A lot of the time, this would lead to finding the requirements you were trying to express, winding up with what you wanted. But *some of the time* it would lead to guessing and superstitions and “poking at your system with a stick”.

An interesting wrinkle here is that Sketchpad partially succeeded by including the idea of “engineering/scientific *tolerance*” — its inferences were not exact, but the processes were set up to minimize global error measurements. This allowed it to have inherently non-linear relationships, etc.

So: an extremely important “Grand Challenge” for (real) Computer Science and (real) computer scientists!



What is the importance of Christopher Alexander’s work for system/software designers?
I think the most useful of the Alexander books is the first one “Notes on a Synthesis of Form”. It was his PhD thesis at Harvard, and has thought provoking discussions of *systems* and *design*. I don’t think his books are useful as direct guides to how to go about things in computing — and that the popular “Pattern Language” books are a bit misleading when attempts are made to apply them to computing. In NOASOF, his main example was to try to design a new village in India by gathering many hundreds of constraints of all kinds — physical, biological, social, aesthetic, etc. — and see how to wind up with a coherent design. This thought process — including ideas in the book that work well (and those which don’t) — is invaluable for computerists to go through. Worth the effort. Petre Dămoc asked such a good question as a comment that I’m copying it here: What about his magnum opus (Nature of Order)? Especially the 15 principles described in Book One. My intuition is telling me that these principles are extremely important for interaction design. My reply was: I should have mentioned these four books. IMO the best part of these books are very like the best parts of NOASOF — extremely valuable to motivate one’s own pondering. I think of them in the same vein as I do Julian Jaynes’ “The origin of consciousness in the breakdown of the bicameral mind” — i.e. probably “not ‘true’ “, but invaluable for provoking thought. My advice here is to take the *forms* of these thought provoking books and — instead of following the contents — make up new principles to see how the idea space can be covered (and it’s certainly OK to use some from the ideas of others). My favorite modern “computery thinker” is Bret Victor — partly for his ideas per se, but also that he has been able to use what was valuable from the past, yet to recast the past in the light of where we are right now and his own high quality instincts, to indicate how we should be probing into the future. I don’t think that Bret has missed much that is valuable, but what he does is still “New” (rather than incremental “News”). Prompted by a comment of Ali Krgn, here is an overall observation: Architecture is definitely a good source of analogies for other subjects, including computing. However, I very much prefer “comfortable surroundings” to “comfortable ideas” (I’d rather be around “uncomfortable ideas” much of the time, but would not like this much in my surroundings). This is why I take C. Alexander with grains of salt — too much analogy can start missing the main points by miles.



What is the difference between soft computing and nano computing?
I had to look up both of these terms (neither was in use back when I was most active). You could look them up yourself, but as long as I’m already writing … (a) “soft computing” is defined as “employing the kinds of thinking methods used by humans and other animals”. This used to be called “heuristic methods”, “fuzzy logic”, etc. (b) “nano computing” is defined completely differently: the use of very small components making a very small computer. (I thought it would relate to some modern version of “microcoding”, but apparently not.



How much RAM did computers typically have when you first started using them?
I realize that I didn’t completely answer this question. I’ve now split it into 2 parts: [1] my personal experience in 1962, and [2] the more general state of things in the US at that time. [1] The first real computer that I programmed as a job was the IBM 1401 in 1962. It had 8K* 6-bit characters (each character had a 7th “word bit”). * because it was a decimal machine, 8K meant 8,000 not 8192 … I have written elsewhere on Quora about the 1401— but Quora search is “kind of abysmal”. I have answered less than 700 questions, and one would think that I (or you) should easily be able to search them …). However, until I locate these **, here is the Wikipedia article. It is pretty informative, but lacks: (a) that the 1401 was originally an “outlaw project” done in a back room, then grudgingly sold, then becoming spectacularly successful, (b) our experiences (and meta experiences) with a small one in the US Air Force Air Training Command computer center in the early 60s, (c) mentioning at least one truly significant work of meta software *** done on the 1401. IBM 1401 - Wikipedia 1960s decimal computer IBM 1401 IBM 1401 Demo Lab, Computer History Museum, California Designer IBM Bits 6-bits plus word mark and parity Introduced 1959 Design CISC Type Memory-Memory Encoding Variable Branching Branch instruction with modifier character Endianness Big Registers 3 index, in memory, optional The IBM 1401 is a variable-wordlength decimal computer that was announced by IBM on October 5, 1959. The first member of the highly successful IBM 1400 series , it was aimed at replacing unit record equipment for processing data stored on punched cards and at providing peripheral services for larger computers. [ 1 ] The 1401 is considered by IBM to be the Ford Model-T of the computer industry due to its mass appeal. [ 2 ] Over 12,000 units were produced and many were leased or resold after they were replaced with newer technology. The 1401 was withdrawn on February 8, 1971. IBM 1401 Data Processing System. From the left: 1402 Card Read-Punch, 1401 Processing Unit, 1403 Printer. The 1401 project evolved from an IBM project named "World Wide Accounting Machine" (WWAM), which in turn was a reaction to the success of the Bull Gamma 3 . [ 3 ] The 1401 was used as an independent system in conjunction with IBM punched card equipment. It was also operated as auxiliary equipment to IBM 700 or 7000 series systems. [ 4 ] Monthly rental for 1401 configurations started at US$2,500 (worth about $26,100 today). [ 5 ] Demand exceeded expectations. "IBM was pleasantly surprised (perhaps shocked) to receive 5,200 orders in just the first five weeks – more than predicted for the entire life of the machine!" [ 6 ] By late 1961, the 2000 units installed in the USA were about one quarter of all electronic stored-program computers by all manufacturers. The number of installed 1401s peaked above 10,000 in the mid-1960s. "In all, by the mid-1960s nearly half of all computer systems in the world were 1401-type systems." [ 6 ] The system was marketed until February 1971. [ 7 ] Commonly used by small businesses as their primary data processing machines, the 1401 was also frequently used as an off-line peripheral controller for mainframe computers . In such installations, with an IBM 7090 for example, the mainframe computers used only magnetic tape for input-output. It was the 1401 that transferred input data from slow peripherals (such as the IBM 1402 Card Read-Punch) to tape, and transferred output data from tape to the card punch, the IBM 1403 Printer, or other peripherals. This allowed the mainframe's throughput to not be limited by the speed of a card reader or printer. (For more information, see spooling .) Some later installations (e.g., at NASA) included the 1401 as a front-end peripherals controller to an IBM 7094 in a Direct Coupled System (DCS). Elements within IBM, notably John Haanstra , an executive in charge of 1401 deployment, supported its continuation in larger models for evolving needs (e.g., the IBM 1410 ) but the 1964 decision at the top to focus r https://en.wikipedia.org/wiki/IBM_1401 ** Thanks to Mark Cleary here is one of my previous Quora answers about the 1401: Profile photo for Alan Kay Alan Kay · 4y What was it like programming an IBM 1401 back in the 60’s? The IBM 1401 was the first computer I programmed as part of a real job (an enlisted man in the US Air Force at Air Training Command (ATG), Randolph AFB, San Antonio, Texas). In the early 60s, the USAF had a quite difficult aptitude test made for them by IBM to screen potential programmers for any of their computers. I took it for fun in early 1962, passed it, and was assigned to Randolph AFB, and IBM training school for the 1401. The training was done in one intense week, wall to wall. The range of HW architectures was much wider back then, but the 1401 was still thought to be “odd”, and today would be considered very unusual. It had been developed to gradually subsume “punched card accounting machine” (PCAM) tasks (for which IBM was the world’s largest company). As with many businesses back then, Air Training Command had a number of “shops” of such machines, with perhaps 50 to 100 machines each laid out over 0.5 to an acre. — I can’t believe that Google won’t yield a picture of one of these large “shops” — perhaps someone can find one — A very small PCAM “shop” — imagine with 50+ machines in an enormous room … The 1401 computer was a “character machine”, with a memory of 5K to 16K characters of 6 info bits plus a “word mark” bit each — these were set to indicate ends of “words” (a sequence of characters in memory addressed by the low order character. A “data character” was (most generally) an encoding of a 12-row punched card column: 4 bits to encode 0–9 and the other two bits to encode the “zone row bits”. Note that this just gives you 40 combinations, but other tricks were used … To add two numbers, the 1401 essentially did elementary school arithmetic, progressing character by character until one or both word marks were reached. (This meant that the 1401 could be set up to add one half of memory to the other half.) The 1401’s memory cycle for one character was 11.5 microseconds … so by computer standards of today it was both tiny and a snail. By PCAM standards, and tape standards, it was quite fast enough. It was essentially a two address machine, but the address registers persisted, so you could deal with “fields” on “cards” sequentially by e.g. doing one two address add — an opcode and two three digit addresses (which would be terminated by the word marks), and the next add would only require the opcode because it would use where the address registers wound up from the previous operation. (Instructions were processed in the left to right direction with word marks at the opcodes.) The 1401 was thus essentially a variable sized word and variable sized instruction machine. Getting lots of stuff done was greatly rewarded by careful planning. IBM 1401 - Wikipedia A nicely detailed article. The IBM Reference Manual we used (the version here is accidentally from early 1962, just about exactly when I started programming the 1401). The punched cards to be worked on were read by a card reader, converted to tapes, processed, the results went back to tapes, were printed, and/or punched back into card form. IBM eventually made and sold well over 10,000 of these machines (the first mass produced computer in history). The IBM 1401 “mainframe”. 1403 Printer 1402 Card reader/punch (good for putting program listings on top of) IBM 729 Tape Drives — with vacuum tape loop buffering The 1401 we worked on at ATC had 8K bytes of memory and six very fast tape drives for those days. There was no OS, and most debugging was “desk checking”. You could get “3 minutes” once a day to try a program, but you couldn’t touch the machine — you put your listing on top of the card reader and asked the operator to do this and that (making friends with the machine operators was critical). The basic learning process was to do the one week hardware school, after which you were quite able to program anything on the 1401 (this was one of the fruits of the difficult aptitude test — it probably overscreened, but everyone who did pass it had absolutely no trouble learning a whole tricky machine and its machine code successfully in a week). Then you spent some months “programming” — which was called “coding”. What was called “programming” back then was design and flowcharting the design. The PCAM machines were all flowcharted, and these were dataflow schemes. These had to be serialized a bit to be converted to computer processing. A “coder” was thus a human “compiler” — whose job it was to convert the higher level programming language of flowcharts to working machine code. After a few months of this “on the job training” and assessment one was allowed to do some design, programming, and then coding for new tasks. There were already seasoned (a year or so) programmers — also mostly enlisted men — who were generally very helpful. There was another larger computer there — the Burroughs 220 — and learning it and doing similar kind of work on it improved the programming and coding for both machines. A few other things I should mention. It was possible to be extremely clever with the 1401, and it should be obvious that the wordmark scheme allowed clever memory allocation schemes, overlays, etc. (It is well worth your time to take a look at Val Schorre’s 1964 “Meta II” system at UCLA, a compiler-compiler that would run in an 8K 1401. I wish I had known about this when I was a 1401er, but its invention happened after I went back to school.) We were able to make a batch operating system that would run all the shop jobs, and which fit into the top few hundred characters of memory (and using tape to help). IBM had an extremely capable “tailored macro” assembler — Autocoder — that allowed a wide range of expression and “conditional assembly” to produce highly optimized and compact code. IBM already had a policy to “make everything run on everything”, and this meant that both the relatively new FORTRAN, and the even newer COBOL had to be compilable and runnable, even on the smaller configurations. The latter never made it into the ATC shop while I was there, but there was a FORTRAN “for curiosity’s sake” (this was because the compiler took about 100 passes (or “pulls” of the tape drive) i.e. a very long time). Reports Program Generator (RPG) was used for a few jobs (I tried it a few times along with others). It made a few vanilla tasks quicker, but the macro library we had developed was generally more convenient, and only a little more dangerous. The older larger slower vacuum tube Burroughs 220 did have something really interesting: the BALGOL compiler (Algol 58 with a few Burroughs touches). It had one of the most beautiful manuals, and this got me to read it. One of the officers had been a CalTech grad and had used it there. At ATC, it was also a “curiosity” but as a much “sweeter and cleaner” example of a higher level language. The next machine in line for ATC was to be a Burroughs B5000, a truly amazing machine whose hardware was made directly for an Algolic language (Algol 58 really), and all of whose software — including the OS (it had one) — was written in higher level form. I learned this machine from the documentation, only understood about half of it, and was back to college before it showed up. (A few years later — by accident — one of my grad school profs was Bob Barton, the main inventor of this marvelous design.) I was able to get back in school for the fall of 1963, and the AF let people out up to 100 days early to not miss a term. I was able to get a good part time job programming for the National Center for Atmospheric Research (NCAR) in Boulder, that would pay my tuition and room and board for the rest of my undergrad years at the U of Colorado. *** One of the wonderful “meta things” done on the 1401 was Val Schorre’s 1964 “Meta II” system at UCLA, a compiler-compiler that would run in an 8K 1401. I wish I had known about this when I was a 1401er, but its invention happened after I went back to school. It will be very worth any computerists’ time to read this short but loaded with information paper from 1964. Meta II was a very clever way to use recursive procedures to parse grammars that were complex enough to describe programming languages and to emit translated code on the fly. The paper includes Meta II in itself (all in just a few lines), two complete examples of algebraic type languages, and a complete description of the virtual computer you need to make to run the examples. If I’m not mistaken, I recall that Val’s 1401 at UCLA also only had 8K characters. Doug Engelbart’s “Mother of all Demos” was programmed in a language — MOL — that was generated by a variant of Meta II (called “Tree Meta’). [2] A typical “scientific” computer ca 1962 was the IBM 7090. I’ll quote from the Wikipedia article: The first 7090 installation was in December 1959.[1] In 1960, a typical system sold for $2.9 million (equivalent to $23 million in 2023) or could be rented for $63,500 a month (equivalent to $501,000 in 2023). The 7090 uses a 36-bit word length, with an address space of 32,768 words (15-bit addresses).[2] It operates with a basic memory cycle of 2.18 μs, using the IBM 7302 Core Storage core memory technology from the IBM 7030 (Stretch) project. I never wound up having to program the 7090/4 series. But it could be fun here to compare it with its biggest competitor at that time: the Control Data 3600, designed by Seymour Cray. A typical system sold for $1,200,000 ($9.5M today — so less than half the IBM 7094), was a 48 bit machine (and could have a max memory of 256K words), and was a bit faster. I wrote quite a few programs for this at the National Center for Atmospheric Reseach in Boulder, starting in 1963. IBM called Control Data “Brand X” (possibly because it was so good they didn’t want to even mention the name). This was developed into a whole line of 3000 series machines, and CDC made a lot of money during this time period. CDC 3000 series - Wikipedia Family of mainframe computers The CDC 3000 series ("thirty-six hundred" or "thirty-one hundred") are a family of mainframe computers from Control Data Corporation (CDC). The first member, the CDC 3600 , was a 48-bit system introduced in 1963. The same basic design led to the cut-down CDC 3400 of 1964, and then the 24-bit CDC 3300 , 3200 and 3100 introduced between 1964 and 1965. [ 1 ] The 3000 series replaced the earlier CDC 1604 and CDC 924 systems. [ 2 ] The line was a great success and became CDC's cash cow through the 1960s. [ 3 ] The series significantly outsold the much faster and more expensive machines in the CDC 6000 series , but the performance of the 3000's relative to other vendors quickly eroded. The line was phased out of production in the early 1970s in favour of new members of the 6000 series, and then the CDC Cyber series, initially based on the 6600 design but spanning a wide range of performance. Model Word size Release date Price Frequency Memory max MIPS Picture CDC 3600 48-bit June 1963 $1,200,000 [ 4 ] 714 kHz (1.4μs cycle) [ 5 ] 1.536 megabyte (262144 x 48 bits) [ 4 ] 0.700 MIPS CDC 3400 48-bit November 1964 $680,000 [ 5 ] 667 kHz (1.5μs cycle) [ 5 ] 192 kilobytes (32767 x 48 bits) 0.700 MIPS CDC 3300 24-bit December 1965 $220,000 [ 5 ] 800 kHz (1.25μs cycle) [ 5 ] 768 kilobytes (262144 x 24 bits) 0.800 MIPS CDC 3200 24-bit May 1964 [ 5 ] $200,000 [ 5 ] 800 kHz (1.25μs cycle) [ 5 ] 96 kilobytes (32767 x 24 bits) 0.800 MIPS CDC 3100 24-bit February 1965 [ 5 ] $120,000 [ 5 ] 571 kHz (1.75μs cycle) [ 5 ] 96 kilobytes (32767 x 24 bits) 0.500 MIPS The upper 3000 series uses a 48-bit word size. The first 3000 machine [ 3 ] to be produced was the CDC 3600 ; first delivered in June 1963. First deliveries of the CDC 3400 and CDC 3800 were in December 1965. These machines were designed for scientific computing applications; they were the upgrade path for users of the CDC 1604 machines. However these machines were overshadowed by the upcoming 60-bit CDC 6000 series machines when the CDC 6600 was introduced in December 1964 and delivered in 1965. [ 6 ] Some high-end computer labs purchased these machines as stopgaps, while waiting for delivery of their 6600 machine. (CDC had indicated that the 6600 machines would use the same assembly language .) The lower 3000 series use a 24-bit word size. They are based on the earlier CDC 924 - a 24-bit version of the (48-bit) CDC 1604 . The first lower 3000 to be released was the CDC 3200 (May 1964), followed by the smaller CDC 3100 [ 2 ] (February 1965), and the CDC 3300 (December 1965). The final machine in the series, the CDC 3500 , was released in March 1967 and uses integrated circuits instead of discrete components. The 3300 and 3500 have optional relocation capabilities, floating-point arithmetic , and BDP (Business + Data Processing) instructions. These machines were targeted towards business and commercial computing. Control Data Corporation 's CDC 3150 was described as a "batch https://en.wikipedia.org/wiki/CDC_3000_series



What happened to the "parc copies" created by Xerox at PARC? Have they been preserved or recycled?
I wrote an answer to an equivalent question: Was anything meaningful lost for good when Xerox threw out their old PARC data disks?



What is the origin of the name "The Internet"? Was it chosen as a play on words with "interconnected networks"? Were there any other names considered before "The Internet" was chosen?
JCR Licklider — an experimental psychologist and (ca 1959/60s) fan of interactive computing — was given a lot of money by the DoD to set up a long range computer research community. One of his first memos looked like this: There is a good story (I’ve told it elsewhere in Quora) about why he called it this. The first pass at this was a large experiment in packet-switching: the ARPAnet. This was instigated by a successor of “Lick” — Bob Taylor — and made to happen by Larry Roberts. What should be done next was pretty unanimous, and was started by Larry Roberts. It was generally called “Inter-networking”. You can guess what this was shortened to eventually. Xerox Parc actually had a working inter-networking system that spanned the US before the official Internet — it was called “PUP” — for “Parc Universal Packets” — and Parc researchers were part of the larger Internet design committee whose main force was Vint Cerf (but which was a truly amazing group effort of top designers who wanted to work with each other). I would give Steve Crocker the main credit for the way the networking research community was organized — and so would Vint (this came up when we had dinner a few weeks ago). Looking at some of the other answers has prompted me to say a bit more here. The ARPA community did not regard the ARPAnet — which sent its first packets in 1969 — as an “internetworking” scheme. But, as mentioned above, the desire/plan to develop internetworking was already in the air. Both Bob Kahn and Vint Cerf worked on the ARPAnet, and in the early 70s started to produce proposal versions of what was to become TCP/IP. Thanks to Steve Crocker and the way the network design process was set up within ARPA, there are a series of “Requests for Comment” documents that provide a wonderful history of the ideas. (“Request For Comment” was thought to be less aggressive and more friendly than “Requests For Proposals” that were the norm in DoD.) The Ethernet was done at Parc ca 1973 to connect computing resources there, and very quickly some of the Altos were programmed to be what we would call “routers” today, and were used to internetwork Ethernets, the ARPAnet, etc. The protocols were called “PUP” (as mentioned above). There is also a somewhat funny story told by others and myself about how the first wireless use of the ARPAnet (using an old Ford bread truck full of gear) was used to transmit reports to Washington DC from one of Palo Alto’s outdoor beer gardens. This was soon used to connect to Don Davies’ packet network at the National Physical Labs in the UK. Here’s a talk I did that summarises a fair amount of the work back then:



How would Alan Kay go about designing a systems design course? What contents should be covered, and what knowledge or skills should one gain? Which exemplary projects/papers should be included to facilitate understanding?
In the true fashion of the 1960s-thru-1970s ARPA-IPTO research community (which included Parc), I would try to find the best people who could do this, and help find funds to support them.

I would try to help form a joint vision large and comprehensive enough to excite and guide.

Compared to what I think is needed, there would only be a few ideas that I could help with (and I think most of the central people we’d get as PIs would already have them).

One big notion and set of questions would be how to go about helping all of us — especially the students — internalize a much better “systems consciousness” (i.e. better “feelings” and “guesses” about systems, etc.).

I started to “think systems” from getting a degree in molecular biology, and first being fascinated by what was called “Embryology” back then (now “Morphogenesis”), then Biochemistry, and then when Watson’s great book “Molecular Biology of the Gene” appeared in 1965. At the time I was working my way through college as a programmer of super computers of the day for the National Center For Weather research — I didn’t know anything important about computing or computing systems, and there was some, but little, overlap with the weather models and computer systems, and systems in general.

Today, there are a variety of ways to use computers to get started on “thinking systems” — especially in both cognitive and visceral ways. I don’t think I know all of them at this point. There are “massively parallel simulators” (like StarLogo, etc.) in which it is easy to set up simple conditions that generate complex non-linear behaviors.

The Apple “Vivarium” Project was aimed at helping children learn to make and view the world via “systems glasses”.

There is Bret Victor’s Dynamicland which creates a wondeful system’s world in which one is embedded and can influence in easy and highly influential ways. I would definitely try to create early experiences and projects of many kinds using this kind of media.

One of many threads running though these ideas would try to help learners transition from normal commonsense (and wrong and weak) notions of static stabilities to the large idea that any stabilities are actually *dynamic stabilities*, and thus processes of some kind are necessary to keep desirable propertes within tolerances, etc.

And then there is *scaling* …

And then there is …

Etc.

Basically (I think) there are many things that would be good to do and learn before computer systems themselves become the object of study. Since much of the early ideas would also be reified using computer systems, being able to later “pop the hatches” of these could be a great start, etc.

There would be much more to this — and far beyond the scope of Quora (and my personal scope). I should have added at the beginning that I would normally think about this for months before proffering ideas — but the fun (and pitfall) of Quora is to see what can be done as a single stream of response.

It would be a great “grand challenge” project!



What is Alan Kay's perspective on the obsession with learning memory management and other "low-level" aspects when it comes to programming education?
I react to this question in the way I react to questions that are essentially about “how much of a subset of something important can we isolate as separate teachable subject before we destroy the connection to the “something important”?” For example, to what extent can you isolate “reading and writing” and even “literature” from *ideas*? It is not unlikely that most educational systems in the world do just such a separation — and, if so, I don’t agree with it at all. Programming is often quite isolated from systems design (and from systems themselves) because it is often taught in isolation as “data structures and procedures and algorithms”. I think this is quite a bad approach. (And systems are not the only larger ideas that need to be understood and heeded e.g. integrity and “before the ‘fact’ engineering” *) The question before us falls into the really interesting category of “runnable math” — meaning that we’d like to think of most programming and systems design in terms of their “relational requirements” rather than what is often needed to convert these into runnable and usable systems. I’m old enough to have been brought up in a mathematical, scientific, and engineering world in which part of the years of learning were spent on really understanding and being able to use fundamental principles and materials both when needed and to ground any higher levels of understanding. My prejudice is definitely that this is still a very good — imperative — idea. It is still being done in the established fields of maths, sciences, and engineering. However, there have been explosions of knowledge in many fields — including computing — so it is definitely worth pondering how to allocate our learning time to trying to understand much larger fields. I have written a few Quora answers praising the Nissan and Schocken “from nothing to an interesting program” approach. https://www.amazon.co.uk/Elements-Computing-Systems-Building-Principles/dp/0262640686 These days I think I would try to work with them to take a second pass at this wonderful work they’ve done. (This stuff can’t be praised too highly, but I think an even larger perspective and a fresh pass would yield great dividends.) For example, the approach to the HW could be made a fair amount simpler without losing an rigor, and the goals for the SW part could be greatly expanded without requiring more than a bit more total bulk of code. That would be a really interesting project, and also a good way of answering the slightly different and larger question that could have been asked here. I.e. say “Yes” via radically changing what it means to “learn to program”. * “before the ‘fact’ engineering” is a phrase coined by Margaret Hamilton, who was head of Apollo Guidance Computer mission SW (everything thing except for the OS). The ‘fact’ referred to any kind of errors, especially crashes. The idea was that the engineering’s main goal was to prevent ‘facts’ from happening during a mission. The evidence is that there was never a SW failure over all the Apollo missions, despite the enormous limitations of both the HW and SW media they were required to use. Apollo Guidance Computer - Wikipedia Guidance and navigation computer used in Apollo spacecraft Apollo Guidance Computer Apollo Guidance Computer and DSKY Invented by Charles Stark Draper Laboratory Manufacturer Raytheon Introduced August 1966 ; 58 years ago ( 1966-08 ) Discontinued July 1975 ; 49 years ago ( 1975-07 ) Type Processor Discrete silicon integrated circuit (IC) chips ( RTL based) Frequency 2.048 MHz Memory Ports DSKY, IMU, Hand Controller, Rendezvous Radar (LM), Landing Radar (LM), Telemetry Receiver, Engine Command, Reaction Control System Power consumption 55 W [ 2 ] : 120 Language AGC Assembly Language Weight 70 lb (32 kg) Dimensions 24 in × 12.5 in × 6.5 in (61 cm × 32 cm × 17 cm) The Apollo Guidance Computer ( AGC ) was a digital computer produced for the Apollo program that was installed on board each Apollo command module (CM) and Apollo Lunar Module (LM). The AGC provided computation and electronic interfaces for guidance, navigation, and control of the spacecraft. [ 3 ] The AGC was the first computer based on silicon integrated circuits . [ citation needed ] The computer's performance was comparable to the first generation of home computers from the late 1970s, such as the Apple II , TRS-80 , and Commodore PET . [ 4 ] The AGC has a 16-bit word length, with 15 data bits and one parity bit . Most of the software on the AGC is stored in a special read-only memory known as core rope memory , fashioned by weaving wires through and around magnetic cores , though a small amount of read/write core memory is available. Astronauts communicated with the AGC using a numeric display and keyboard called the DSKY (for "display and keyboard", pronounced "DIS-kee"). The AGC and its DSKY user interface were developed in the early 1960s for the Apollo program by the MIT Instrumentation Laboratory and first flew in 1966. [ 5 ] Astronauts manually flew Project Gemini with control sticks , but computers flew most of Project Apollo except briefly during lunar landings. [ 6 ] Each Moon flight carried two AGCs, one each in the command module and the Apollo Lunar Module , with the exception of Apollo 7 which was an Earth orbit mission and Apollo 8 which did not need a lunar module for its lunar orbit mission. The AGC in the command module was the center of its guidance, navigation and control (GNC) system. The AGC in the lunar module ran its Apollo PGNCS (primary guidance, navigation and control system), with the acronym pronounced as pings . The display and keyboard (DSKY) interface of the Apollo Guidance Computer mounted on the control panel of the command module, with the flight director attitude indicator (FDAI) above Partial list of numeric codes for verbs and nouns in the Apollo Guidance Computer, printed for quick reference on a side panel Each lunar mission had two additional computers: The Launch Vehicle Digital Computer (LVDC) on the Saturn V booster instrumentation ring the Abort Guidance System (AGS, pronounced ags ) of the lunar module, to be used in the event of failure of https://en.wikipedia.org/wiki/Apollo_Guidance_Computer



What does Alan Kay think of Ted Nelsons criticism of Xerox PARC? (Taking away the "right to program" by hiding everything behind the "PUI" and making computers into paper simulators with single-collumn text)
Ted has done a lot for personal computing, but occasionally gets things wrong. I got invited to give a talk for his 70th or 75th birthday, and decided to use a 1978 Xerox Parc Smalltalk system to show what we really did, and to praise him at the same time. The talk starts around 2:15 if the URL doesn’t take you directly there:



What do you think is the future of software engineering? Could advancements in technology eventually render it obsolete, or will these advancements serve as valuable tools that enhance the work of software engineers rather than replacing them?
Here’s part of a slide I used in a talk in 2021 expressing a kind of “Hippocratic Oath” within Engineering: The first two have been around for thousands of years. The ones for airplanes and software more recently. It’s worth noting that in every era there have been people — sometimes even engineers — who are willing to bypass these for various forms of gain and/or convenience. There is also the problem that only parts of “Software Engineering” resemble “Established Engineering” — much of what is called “SE” is considerably more ad hoc than the term demands. An historical perspective on Engineering will reveal many changes in what pro-level engineers in each era do, and what kinds of tools they learn and use. However, the “Do no harm” ethic has been quite constant over time. We also have to take note of the deep human trait of *normalizing* what is generally going on in each present time. For example, it is big news when a plane crashes and kills several hundred on board, but in the US in 2022 there was an average of 116 traffic deaths a day from vehicle crashes alone. Much worse, there is an average of 1315 deaths per day from smoking. This is not only *not* front page news, it is not even reported in most news media. This means that a deep danger in the future of engineering would be a “normalization creep” downwards to levels of harm not considered acceptable today. We can see hints of this in many parts of computing, especially “social” media and so-called “AI”. These are quite dangerous and harmful in their current form, but most of the population, including the engineers involved, have normalized that causing harm in this and other ways is quite acceptable. (And, of course, we can see many other examples of technologies invented and produced via engineering that are gradually turning the planet uninhabitable.) Another deep human trait is to try to avoid paying the full cost of most things, and especially to try to amortize costs not just to the future, but to future generations. For example, most infrastructure objects — such as bridges — are costed for constructing them, but not to replace them or do major maintenance 50 years or so hence. The software industries have taken up this unfortunate behavior all too whole-heartedly. We also have to note that the level of expression that is used to create software has not changed much since the 80s — and that this is in great contrast to the progression of levels of language that happened in the late 50s to the late 70s. Meanwhile, “Moore’s Law” at the components level continued exponentially, and without languages/development environments, approaches, etc. keeping pace (and not even keeping pace with the development and use of tools in the more established engineering disciplines — ironically using computers for CAD and SIM in ways that the computing field itself shunned). My guess is that the two main culprits were (a) rampant commercialization starting in the 80s, and (b) the lack of “level of skills” and ways to determine these that were analogous to the medical and established engineering professions. When Facebook took itself off the Internet for a day a few years ago (including its own offices), I started poking around to see what led to this. I found that Facebook essentially does “unit tests” via thousands of configuration checking programs whenever a change is made. There was not a test that would say “you must not partition the Internet on purpose”, etc. Note that trying to write thousands of individual laws that together capture the larger intent of a process doesn’t scale, and in the end is a terrible approach. This is similar to the Boeing 737 Max autopilot — which also had thousands of “laws” and responses. However, they didn’t have what John McCarthy called “commonsense” knowledge that most humans use to bridge between the specific and the general. Facebook code didn’t have any sense that the larger goal was to be on the Internet 24/7, and the autopilot didn’t know that it had people to protect and that diving towards the ground to correct a stall at low altitude is not to be done. What is quite sad here is that Moore’s Law did come through with the kind of scaling needed to handle these larger issues. But the ideals and perspective for what “real AI” (and “real programming in the 21st century) need got lost when a subset of the “real AI” problem (fuzzy useful correlations with little meaning) replaced “real AI”. ——— added June 19 2024 ——— Here’s something recently, very dangerous, and to this point: Firewall system problem blamed for outage in state’s 911 system, officials say - The Boston Globe The company that operates the system determined that the outage was not the result of a cyberattack or hack, but it remained unclear what caused the firewall to stop calls from reaching dispatch centers. https://www.bostonglobe.com/2024/06/19/metro/state-911-outage-firewall/?s_campaign=breakingnews:newsletter Yikes! I reckon that we’ve now lost almost 40 years of needed development of “higher level expression with integrity of meaning”. Doug Lenat’s CYC system had many of the right ideas, but needed to be redone a few times to reflect learning curve effects on how the architecture should work. Here’s one way to think about what is needed. It has analogies to classical engineering, but is of much larger scope. It also has analogies to “societal engineering” but constitutes a very large step from where we are. The center of what is needed is a set of processes that can represent most of the important meanings and goals in our century, and to be able to do it in ways that the validations of meanings can be understandable to we humans. Engineering has done this with various kinds of standards, including physical ones that represent the meanings of core concepts. Some societies have done this with carefully written relatively small constitutions, against which specific laws and actions can be tested. (Note that there are too many degrees of freedom for “case law”, so determining “legality” almost certainly has to be done dynamically.) To my mind — maybe it is now just the mind of an old fogey — this returns us to many of the ideals and visions of the early 60s. This time around, there are enough resources to do a much more complete job at “Software Engineering with Meaning and Safety”. (It won’t look much like what is called Software Engineering today, but that is much of the point here!)



I’m currently a teenager and I started looking at philosophy. I want to start taking a dive into it but I don’t know where to start. Any certain topics or books that I should look into?
A good and somewhat tricky question …

First, what does *philosophy* mean to you?

I think of it as the kind of deep thinking that one has to learn to do when science, etc., no longer serves — but you will have your own slant on this. The underlying notions of science will still help thinking about issues that don’t lend themselves to empirical experiments.

The contrast between maths and science can also help provide some grounding to the use of logic and representations in language. Einstein pointed out to fellow physicists in 1921:

“As far as the laws of mathematics refer to reality, they are not certain;
and as far as they are certain, they do not refer to reality.”

The larger meaning of this is that reasoning in language can express “certainty” but this is only about the logical entailment of the conclusion, not about “the world outside our heads”.

Another path to be negotiated is the one between learning what others have thought in the past (and are now thinking) and the central point here of learning to think your own thoughts in deeper and more fruitful ways. You will need to build up a lot of both.

One way to do this is to note down *questions* as you read the ideas of others (this is better in the long run than e.g. simply underlining and learning the ideas of others).

It will help you (a lot) to write down your current *burning questions*

I like “carpet bombing reading”, but sometimes a single “slim volume” can be a good way to get started. A good one — by Thomas Nagel — is “What Does It All Mean?”. You will find Nagel’s approach to getting into philosophy similar to what I’ve been advocating — I particularly like the introduction of this book: to me, it is the essence of how to start thinking about “difficult stuff”.

A few generations ago, Bertrand Russell wrote quite a few books for the general public about ideas, and presented them in a genial good natured but not trivial way. His book “The problems of philosophy” could be a good start.

In any case, the first cousin of philosophy is bullshit. This is both useful and a snare. The trick is to use bullshit to speculate, but then not to get caught in your own waste products! (e.g. by believing the bullshit). As with the invention of science, finding other ways to deal with ideas other than simply “believing” them is the key to making real progress.

Good luck, and have fun!



Can you provide some examples of a "thinking machine" that existed before computers were invented?
LexisNexis defines “machine” as

A machine is any system structure that can be used to complete a task by utilizing energy.

Lewis Mumford used the term “megamachines” as part of his approach for understanding and analysing societies — and pointed out that most societies were in fact “megamachines”.

“Artificial” — according to the Oxford English Dictionary — means:

made or produced by human beings rather than occurring naturally, especially as a copy of something natural.

With all this in mind, let me propose *modern science* as not only a “thinking machine” and an “artifical intelligence”, but as the most powerful and successful manifestations of these ideas so far, and in history.

I think this is not only quite lost on most of today’s computerists who work on so-called “AI” (which is even very far from the original definitions and larger understandings of the original founders).

For example, one of the keystones of “real science” is not only that *internal deductions and inductions are insufficient for doing science* but that the whole enterprise — including empirical observations and studies — are extremely difficult in general, and for the human mind in particular.

This is partly because — as Francis Bacon pointed out 400+ years ago — the human race is not set up for thinking clearly (a) genetically (b) culturally (c) linguistically (d) academically. “Scientia” back then meant “gathering of knowledge”, but Bacon pointed out that the results of the four “idols” above that humans “worshipped” led to the “gathering of distorted and quite wrong ‘knowledge’ “, and that what would very well help matters would be to invent a “new science”: new methods and new organizations that could partly get around our biased brain/minds, cultures, languages, and teaching.

What we call “real science” today is the “new science” that Bacon called for.

There are too many facets of “real science” for the size of a Quora answer, but let me point out that “real science” requires a society of scientists who are willing not just to explore in new ways but to test out the ideas in new and tougher ways (since most humans have “confirmation bias” which makes it difficult for the formulators of theories to test them deeply enough (so other scientists often do a better job of testing, etc.)).

(Note that funders often have considerable confirmation bias, etc.)

If any attempts are made to “do AI on a computer”, they should take into account what “real science” has found it necessary to do to generate necessary levels of confidence in the results. Without this we have a combination of confabulation with credulity — often associated with belief systems, such as religion — that has been really dangerous over history, and especially dangerous today.



What would be the place of AI in Doug Engelbart's vision of 'Augmenting Human Intellect'? How can we reconsider things within current AI 'developments'?
It’s very worthwhile to read Doug’s original plan in 1962. They could only work on a subset of this, and ca 1970 they wound up being identified with the subset (this often happens). Find this version of the plan on Engelbart’s website (hint: type “Engelbart” into Google). Below is the summary of the large scale plan. You can see in the red box a mention of various technologies they intended to use — and in the large scale plan itself, you can read how they planned to use “AI” (you need to remember that many terms — such as “AI” — had a very different meaning in 1962 (62 years ago!) and they certainly were not thinking of such limited and counterproductive technologies called “AI” today (such as ML, LLM, etc.)). In talks that people have posted to YouTube, I’ve explained some of the big ideas from this deep group of researchers. One big distinction in the 1962 time period is that they thought of “machine intelligence” as being a kind of complimentary set of thinking tools that could be “symbiotic” to how humans were able to think. They were not at all thinking about something like a slave or a major domo, but something more like a research assistant or a “Memex” (the latter was a big influence on Doug’s thinking). In the very late 60s the “official AI researchers” started to think that something like “intelligent Greek slaves” were needed for the “Romans” (Americans), and became rivals to Doug’s notion of elevating human thinking rather than just elevating power. This was a bad idea then … and it’s a bad idea today. For completeness, here are the references for this 1962 plan. Also, Ivan Sutherland and his work on Sketchpad are mentioned in this document, but there was no reference because Ivan wrote his thesis in Jan 1963.



How do I design an effective orientation program?
I don’t understand this question — could you expand in a comment? I’m not sure I can answer it, but right now can’t tell.



Why are object-oriented classes named classes? What is the etymology of object- oriented classes?
The second version of Simula (Simula-67) used the term “Class” for the descriptions that could be instantiated into “objects” which carried out the descriptions. Some of the background for this term very likely came from its use in mathematics.

The history has some points of interest. For example, the first version of Simula (Simula I) used the term “Activity” for the descriptions that could be instantiated into “Processes” which carried out the descriptions. Ivan Sutherland’s Sketchpad system predated Simula, and used the term “Master” for the descriptions that could be instantiated into “instances” which carried out the descriptions.

Other terms for “that description which can be instantiated” have been used over the years. For example, note that the “Processes” in multi-processing/time-sharing OSs are actually instances of a special kind of restricted virtual machine which the OSs supports and protects.

It is also worth noting that in both Simulas the description (class/activity) is not an object. A language in which every description is an actual object — e.g. Smalltalk — turned out to be a very useful idea.



If you had only 6 months to make a product using code, how would you do it? This question is inspired by Peter Thiel's approach to time management.
As posed, this question doesn’t have enough information to attempt an answer.



How can I convince myself that “real OOP” is a better way to build systems? What’s the reason other than Alan Kay’s “god-given vision” — Is there any math behind these ideas or are they just a bunch of “good-looking” ideas with "cool" rhetoric?
“Interesting” question, and “way to phrase it”. First, I doubt there was any “god” behind my “vision”. And why would “math” make you feel any better if you are asking about “better ways to build systems”? (That is the province of engineering, which only part of the time can use math to help, and sometimes has to get by “workable heuristics”.) And then, a key question is whether my almost 60 year old vision holds today after factors of a few millions of Moore’s Law? I always start off by doubting “things held to be true” in order to help thinking. And I certainly think that what I thought in 1966 should be suspect in 2024. The basic idea was that a really simple way to think about systems — that was already in use — was the analytic idea of identifying subparts and the communications between them. My little idea was that this could be modeled as “little complete computers” on a “network” communicating by “messages”, and this could be “mathematically neat and complete” if every entity (including the parts, network and messages) was in the form of “little complete computers”. The ancillary observation was that this would take a lot of software and hardware engineering to be made practical enough to work. This is what I called “OOP” back then. Trying to look at today, I think the ways of trying to understand systems by decomposition and build them by synthesizing organizations of parts are still the main ones (and are still problematical — but likely to still be needed and useful). However, I think the practical tools for understanding and making systems have not kept up with the immense scalings that have happened over the last 60 years or so. (Something worth pondering is that a qualitative scaling should also provide more computing power for better ways to intercommunicate, and thus better ways to “send and receive messages” — and this means much better ways to decompose problems and model better solutions. Depending on what you think “Real OOP” means might help your first question. If you think it means “the way we (Parc) e.g. built systems in the 70s”, then only a few of those principles still hold, and better ways are needed. If you think what “Real OOP” should mean (in this day and age), then I think you need to help invent the practical forms of *what is actually needed* today. I think it will be hard to actually understand any complex thing without *modularization” of some kind (and this will require techniques for “intermodule communication and coordination”). It’s up to you to decide whether what I thought about 60 years ago has any bearing on this today, and whether it was a kind of “applied algebraics and logic to practical problems” (along the lines of how John McCarthy thought about things).



It has been said that Windows is Bill Gates way of helping Seagate to sell ever larger hard disk drives. If that is true, then I claim that Smalltalk is Alan Kay's way of helping Intel sell ever more computationally capacious microprocessor chips?
First, let me copy the OP’s answer to his own question:

Bill Buckley:

The problem with this post, which I placed, has much to do with Quora’s limit on question size. So, I will try again. Dr. Kay, is it unreasonable for me to have said in my Graduate Seminar course (for the MS Computer Science), “It has been said that Windows is Bill Gates way of helping Seagate to sell ever larger hard disk drives. If that is true, then I claim that Smalltalk is Alan Kay’s way of helping Intel to sell ever more computationally capacious mircroprocessor chips?”

The top question is complex linguistically. The first sentence looks to be a “proverbial metaphor”, but the next phrase is “If that is true …”.

It isn’t “true”, so the rest of the question — a deduction — doesn’t follow as the writer would wish (of course, implications don’t require their preconditions, but normal usage is “if and only if” and these do).

Now to address Bill Buckley’s comment.

I would offer that “yes, it was unreasonable for you to say that”. The largest reason is that I never was interested in helping Intel do anything but start to create “emulation processors” for dealing with a variety of very high level languages (e.g. much higher in expressive power and safety than C) that both existed and needed to be invented.

I viewed Smalltalk as “something good in its timeframe and scaling”, to be superceded as soon as possible.

The opposite of my aims actually happened: (a) Intel stayed with extremely poor architectures and tried to make them faster, but not better, (b) Smalltalk did not get materially/qualitatively reinvented, and is still around today in close to original form.



What is the Chronology of Early OOP? (see self-answer for details)
There’s “early” and “Really Early”. “Really Early” mostly happened in the 60s (with a little continuity back into the 50s). My best recollections are in a history I wrote that was requested in the early 90s by the ACM “The Early History Of Smalltalk”. A version of this (without the Appendices) can be found on Bret Victor’s site: The Early History Of Smalltalk The notion of data structures containing pointers to procedures goes way back. I mention one example in this history that was in use when I started programming in 1961. Also, data structures with multiple fields were commonly called “objects” (procedure pointers or not) — so I made a bad choice in 1966 calling this new area “object oriented programming” (if conveys a poor image of the idea). If I were to do another pass through this history, I probably would have said more about Doug Ross (whom Ivan Sutherland mentions in his 1963 thesis). Doug was an early programmer on Whirlwind in the 50s and one of the first to write digital graphical simulations (bouncing balls, etc.) that were common on analog computers (these were partly to show that Whirlwind was really fast — it was one of the earliest — maybe the first — bit parallel computers). Two big ideas that were being thought about early on in different areas of computing were (a) “instantiation” and (b) that “meaning” had to include dynamics. For example in the data base world there was the notion of unit records of a particular arrangement of fields (initially on punched cards). One path was eventually to the Data Division of COBOL and what led up to it (this predated scientific computing by many years e.g. Algol 60 only had numbers, strings and arrays — “records” were in the future). WIth all due respect to Doug Ross, it was Ivan Sutherland ca 1961–2 who thought about instantiation of dynamic “objects” — this for the purpose of creating what is arguably the first real interactive graphics language and system: Sketchpad. Just as interesting, and a tad earlier, we have Bob Barton describing the architecture of the Burroughs B5000 — which also was the first to have its OS completely written in a High Level Language (initially an extension of Algol 58, then Algol 60 (called ESPOL): A New Approach To The Functional Design Of A Digital Computer. In this case the *hardware* was more higher level than any of the languages of the day — it could be thought of as a “Simula I” machine but before Simula. It included both instantiation and “meanings as including dynamics”, etc. Instantiation from a “Master” (Sketchpad term) or an “Activity” (Simula I) term (both of which are now called “Class” (from the later Simula 67), does not require inheritance chains to be extremely useful. The “Master” establishes a *category* that defines generically how the instances should behave and appear. Sketchpad allowed Masters to be made from any *composition* of instances, and this was very powerful. As the Smalltalk history explains, a lot of early ideas about “processes” were combined into a single definitional idea for a new way to think about “objects”. Some of these additional ideas included the kinds of instantiation into “virtual machines” that multi-process and time-sharing OSs were starting to do — this is a great way to start thinking about the ideas, but the overheads in having processes be “large” had to be eliminated. Pure math — in “algebras” etc — uses ideas that are like “inheritance” to get both power and logical coherency — so the notion of “inheritance” appeared early (e.g. in Simula 67). I think I explained in the Smalltalk history that the Lispish late-binding approach taken by Smalltalk-72 allowed many different inheritance schemes to be explored (including the very useful “slot inheritance” adopted in some of the AI systems of the day (e.g. Pat Winston’s thesis). The original plan for the Alto computer at Xerox Parc was to replace it with a much faster one in a few years to take advantage of Moore’s Law, etc. However, Xerox did not want to pay for many new machines (they were already complaining about the 100s of Altos we were making). This put a huge squeeze on researchers (like us) who were depending on HW capacity to stay very late bound. The need for faster and larger on the same machine inspired Dan Ingalls to design Smalltalk-76, which was more of a compromise between the Lisp and Algol worlds, but was a brilliant design and not much flexibility was lost (and it was almost 200 times faster than Smalltalk-72). Part of the design was the decision to use (and be really careful with) single inheritance for the system (but to still try experiments in *what should Masters look like?* This worked out well because we were primarily trying to invent a wide-spectrum usable form of “personal computing” rather than a programming language (and this helped all around). Smalltalk worked really well at the scales we had back then. But it was clear that e.g. “inheritance” was going to be tough to scale, and a workable multiple inheritance needed to be more than ad hoc ideas. For example, MIT’s Flavors were a perfect double-edged sword and very difficult to handle (and much larger in code than the entire Smalltalk system at Parc). Basically: scaling really called for a “knowledge based epistemological design” that would require a lot more than just combinations of inheritance and compositions and mixins, etc. We considered this necessary, but outside the scope of our research.



How can anyone who is intelligent believe in god?
David Moore wrote a good answer: Profile photo for David Moore David Moore · 1y How can anyone who is intelligent believe in god? One of the other answers features this little gem of a meme: It’s quite ironic, because (like so many memes) when one gives it a little thought one realises how silly it is. In a sense, all experiences are hallucinations. In no way are the operations of our neurons the same things as the energies that stimulate them. For example, we all know that we are bathed in ultraviolet light when we stand outside on a sunny day, but our retinal and cerebral physiology is such that we don’t see it. Ultraviolet light is invisible to us unless it is translated into the visible spectrum by some sort of manipulation. Fluorescence provides an excellent example of such ‘translation into our terms’. Similarly, the vast majority of what we understand about the world is made intelligible by simplified presentations of realities that we do not grasp due to our inherent limitations. Intelligent people understand this, because part of intelligent thought involves assessment and adjustment for such limitations. This is why we use microscopy, for example, to create false images of actualities that would otherwise be inaccessible to us. Understanding the distortions induced by doing so is critical to the truthbearing function of that optical operation. So, do you believe in atoms? If so, then the reason you do is because you believe the operations of the neurons of people using equipment like that in the diagram above to create pictures that present grossly distorted simplifications of reality somehow represent that reality correctly. Even though you (hopefully) simultaneously understand that the correspondence theory of truth on which such Picture Theory rests actually falls apart at the quantum level as Wheeler’s delayed-choice experiment demonstrates in (real?) time. Thus, just as we are inclined to believe scientists when they describe the phenomena they study, or colourblind people when they describe their senses, so too are some intelligent people inclined to believe that the experiences of some other people throughout history represent genuine insights into the nature of a reality in which we are a small and restricted part. Reproducibility is important in science, but many events do not recur. Your birth, for instance, is one. The fact that it cannot be recreated in experimental circumstances is not necessarily good reason to disbelieve your parents when they tell you where you came from. They may be lying, but whether we judge them to be doing so depends not on whether they can produce documents or photographs (all of which can be faked) but rather whether their lives bear ongoing testimony to their truthfulness. It’s one thing to die for something someone convinced you of. It’s another thing to have a number of people go to martyrdom for something they know they invented. Exhortations from such people to embrace truth and abhor lies would be beyond ironic. Once we realize that our biological brain/minds are (a) set up to remember and believe, and (b) all of this activity is internal, we can also realize that (c) what we think is “reality” is actually our own internal theatre playing back combinations of our beliefs and sometimes tiny signals from outside us. This means that we are always “dreaming” (and we hope our waking dreams are better maps of what’s going on than those while we are sleeping). Using these terms we can see why David Moore uses the term “hallucination” — and we should also be able to see that the best science can do is to make better “maps” — “representations” — of what more careful investigations can find out. Because scientists are humans, their brain/minds also *believe* — and good scientists will try to include the active idea of “this is a belief” as a label on their beliers. The term “god” in the question is vague: Wikipedia declines a simple definition such as “an entity superior to human beings in many — if not all — ways”. Instead, the article loads the term with many historical perspectives, etc. Long ago I majored in Molecular Biology — and still find it endlessly fascinating. One conjecture that MolBio gives rise to is that it is entirely possible — using just the biology of this planet — for there to have been or be “an entity superior to human beings in many — if not all — ways”. I think most intelligent and knowledgeable people would be able to believe in the above sentence. It is about possibilities related to well known science and weak, but reasonable definition of the term “god”. I don’t think there is enough evidence to actually *locate* such an enitity, *or to actually believe in its existence*. I think this would/should preclude belief in actualities for stronger definitions of “god”. However, one of the interesting pitfalls of science is that when we are on the trail of a conjecture, it is hard to not “sort of believe in it”. And, one of the largest stumbling blocks in trying to think is “confirmation bias” — and we scientists have this problem along with everyone else. I think “conjecture” is the twilight area for most thinkers. This is where our “waking dreams” have the most trouble (and fun). The theatrical nature of our biological mind is most of the time — a great gift — and sometimes leads us astray. Frank Thomas, the famous Disney animator, once told me that they were worried that the audience for “Snow White” would not be afraid for her when threatened by the huntsman (she was just a series of drawings). But there was no need to worry, because — in theatre — the audience can get completely emotionally involved in wooden puppets, and cry when one is “killed”. Here’s a good one. Suppose we believe we are an atheist — and have supportive reasons into which we’ve put a lot of effort. But, how tough should we be as a member of the audience in a great play where belief in a god is the center of the driving idea? Can we give ourselves over to the play — for the sake of the play — and then come back to our main set of beliefs when the show is done? The next night, in the very same building — also with great words being spoken and great music being played — we are now in a political rally. I don’t think we should give ourselves over to it, just because it is set up like a theatrical performance. I think we should really be skeptical and try to avoid simple believing — even — especially! — when we think we like the candidate. The real question is where to put our story worlds. I don’t think we should try to do away with them — so much meaning is wrapped up in them. But, the advent of science shows that we can also have mental worlds — still kind of story-like, still with some believing going on — that are much clearer depictions of “what’s out there?”. Shouldn’t we be teaching children how to live in and use their various mental worlds, rather than being trapped in them?



How do we program a universal bootstrapper that can run any meta-circular evaluator?
As stated, this feels like a “trick question”, because any Turing complete mechanism can be programmed to run any meta-circular evaluator. I feel that I’m missing something here. Am I? Please tell me more …

For example, the LISP meta-circular evaluator assumes that a LISP evaluator already exists. One question could be: how do you bootstrap a LISP evaluator from a Turing complete mechanism that doesn’t already have a LISP evaluator? That can be fun to think about especially if you’d like the result to look as much like a meta-circular evaluator as possible. Most of the Smalltalk systems did this quite nicely as well.

Val Schorre’s Meta II in itself was bootstrapped by writing it out in high level form and hand running it once to get the code for the VM to run (take a look at the first page of his Meta II paper). Alex Warth did a really nice job of making a succession of systems via getting an initial bootstrap going.

Etc.

——— Added 27 April 2024 ——— from the questioner’s comment below ———

Harri K. Hiltunen:

Remove the programmer. The universal bootstrapper program should work without human help. This is about communicating with alien computer programs without any programmers available. Earth sends Lincos followed by a human-like intelligent agent representative, other planets send other self-interpreters with their representatives, and there are billions of possible Lincos-equivalent self-interpreters being sent around the galaxy. The universal bootstrapper can run all of them without knowing what’s coming. The human is being removed from the bootstrapping for the purpose of, for example, making interstellar probes that can start communicating with their destination planets when in range, decades before arrival, without needing to waste decades relaying the alien communications back to Earth to be interpreted and converted into a software update to be sent back to the probe. This is also useful in the internet where ancient and newly generated programs are communicating with each other, trying to resolve broken or missing dependencies. If all programs have a universal bootstrapper available, they can prove to themselves that the offered services really satisfy the contract. No software could ever break in such an environment.

——— New Answer To Appear In Stages ———

I was not understanding the original question (as I guessed above) — I was puzzled because Harri always writes thoughtful comments.

This is a quite interesting question that I’ll have to think about for a bit. However, when Doug Lenat first did his thesis “AM” and then “Eurisko” — back in the 70s — some of what Harri is asking about did come up (and in the context of LINCOS). One question in two parts was (a) could a computer program like AM or Eurisko decipher the LINCOS sequence? And, just as interesting, (b) could a human being decipher the LINCOS sequence?

I have vague recollections that the (b) part was tried — and also —tried with some of the better successors of LINCOS (including the Stanford one I’m thinking about but can’t find with Google).

The really outstanding part of AM was the several hundred formal “heuristics for ‘interesting’ and following up ‘interesting’. And for Eurisko, was the inclusion of the heuristics in a form that Eurisko could not just use but do experiments on and improve as well. Doug wrote quite a few well rewarded papers on these two systems, and they are still very interesting today.

A really interesting intermediate computer system would one that could bootstrap *just one* good example of a LINCOS type sequence. Again, one would need to do what Doug did about “cleansing the heuristics” (for AM this was done by Doug getting both his friends and enemies to go through the heuristics enough times to “bless them” that they didn’t contain hidden math relationships of the kind that AM was supposed to be able to discover.

Now I will try to find the ellusive followup work, and ponder.

——— Added 27 April 2024 ——— First Steps ———

“Perspective/point of view/Context is worth 80 IQ points”

One of my favorite context stories was how Paul MacCready — in a few months — won the Kramer Prize for man-powered flight. He was driving across the desert in the 70s with his family, and thinking about his brother in law’s debt of $100,000 dollars (which he had taken on to make his wife happy). That morning he had seen that the British Pound was worth $2.50 and a stray thought got him to realize that the £40,000 Kramer Prize for man-powered flight was worth exactly $100,000 — and he said to himself, if I could win this prize, I could pay off this debt.

He had a degree in AeroEngineering, had been the world soaring champion a few times, and knew everyone who was working on this problem. But the prize was decades old and no one had been able to do it. He said to himself “These are really good people — why?” And then he said “The problem is that we don’t understand the problem”.

He told his gang when he got back to Simi Valley — “our goal is not to win this prize, but to invent a simple air frame that can survive 10–12 crashes a day”. This is because his expert friends would make an elaborately designed airframe, do one or two flights, crash it and then spend another year building the next one.

Six weeks after his drive in the desert, his group had made more flights and had more crashes that all of the other engineers in history.

Six months after the drive in the desert, the knowledge gained by the mainy flights and crashes created an air frame — The Gossamer Condor — that won the Kramer Prize. A few months later, an improved air frame — The Gossamer Albatross — won the big Kramer prize of £100,000 pounds for the first man-powered flight over the 23 mile waters of the English channel to France.

Bottom Line for this section: what are the smaller tests that need to be devised to get a much better POV on what the actual problem might be?

——— Added 27 April 2024 ——— The Cuneiform Tablets of 2015 by Long Nguyen & Alan Kay

This paper has several useful accounts of history, and an example of a possible preparation that could bring a program today back to an informative as well as a working life 1000 years from now.

We started by describing the Domesday book of 1086 AD. One of our comments: “It may be that our descendants shall know more of England in 1086 AD than of England in 1986 AD.”*

*This is because paper lasts longer than any digital media we have today — the Domesday book still exists in almost perfect form — and because we still can read the Latin in which the Domesday book was written — and this is because many important ideas were discussed early on in Greek and then Latin.

We eventually describe a bootstrapping scheme that involves a file of bits in which the first part is a simple bit-map (one bit/pixel) image of a page with a brief description — probably should have been in Latin rather than English (but we thought that a few English authors would be remembered 1000 years hence) — of a VM that any programmer could get working “in a few hours hack in an afternoon”. This would run the rest of the bit-map, which would be something like one of the PARC completely contained Smalltalk systems. This would have a working development system, and — since the simple VM might not be fast enough in the future (hard to believe) — the contained development system would also assist the programmer to make a much faster kernel if needed.

The actual “content-content” of the file would be objects manipulated and given meaning by the whole system.

1, If we take the “MacCready Sweet Spot Principle” into account, then our first attempt at Harri’s question probably should be to first make a bit sequence that can be given to human computerists who would then be able to make the intial VM *without* any advice in any recognizable language*. They could use any kind of programs (or none) to help figure this out from the bits give to them.

*Note: we know that humans can use pictorial languages for describing things (but we don’t know this about aliens). Should we try to do the bootstrap above without using any images?

It would likely use many of the techniques and ideas in LINCOS and “Astrolinguistics” to get from numbers to relations to transformations. This would be interesting to do and see what happens. Perhaps there might be a modern Henry Kramer to put up a prize? Will there be a “Paul MacCready” who will see the best ways to do all the experiments needed?

An important side note at this point is that there would still likely be a very wide gap to jump from a really difficult problem for certain smart humans to making something similar and much wider for actual aliens. If one reads the literature, quite a few very smart people have been thinking about the “communicating with aliens” problem for many years (centuries), and many of the most important issues and barriers have long been put forth.

2. I think a good parallel project would be to see what a computer program — like an “AM” — could do with messages written in LINCOS or its successors. In this case those who write the message would not be those who write the computer program to see if it can figure out — more or less from scratch — what is attempted to be conveyed?

Note that the social nature of human language (and the likelihood of its own bootstrap via cultural convention and “negotiation of even simple meanings in discourse”) make it a very poor vehicle for attempts to communicate “meaning without feedback” with faraway aliens.

——— Added 28 April ——— “CosmicOS” by Paul Fitzpatrick

I’m still just finding stuff from years ago that I already knew about, but have “vagued off” since. It is worth looking at the work of Paul Fitzpatrick — then at MIT — who took a shot at a real bootstrap from scratch to telling the aliens how to make an interpreter for a LISP like language.

There is an older page here:

CosmicOS - Paul Fitzpatrick
https://people.csail.mit.edu/paulfitz/cosmicos.shtml
And a newer one here that seems to contain the latest entire message.

a next-generation Contact message
https://cosmicos.github.io/
Here is an explanation of the project (at bottom of page):

GitHub - paulfitz/cosmicos: Sending the lambda calculus into deep space
Sending the lambda calculus into deep space. Contribute to paulfitz/cosmicos development by creating an account on GitHub.
https://github.com/paulfitz/cosmicos
Here is Paul Fitzpatrick’s blog (go down to the end/beginning and work up to find another explanation of CosmicOS)

Paul Fitzpatrick
English to SQL translation is getting better! Can you guess which of these queries to find the actor appearing in the most movies (in a fictional dataset) was written by a human, and which by machine? Well ok yes you probably can, but the point is they are both pretty good! In early 2019, when I was looking for published models for text to SQL translation, I had the most joy with one called SQLova ( Hacker News discussion from that time ). SQLova was trained on the WikiSQL dataset, a collection of natural language questions and SQL queries for tables drawn from Wikipedia. Part of what made SQLova work well was using BERT to transform its input. BERT is a model released in 2018 for generic NLP tasks, trained on BooksCorpus and English Wikipedia, that led to a wave of state-of-the-art improvements in all sorts of language problems as people bolted it on to their models. That wave continues to this day, and BERT’s descendants get better and better. SQLova was limited to queries on a single table, which is not the most interesting use to make of relational databases. In 2018 the Spider dataset had been released, with questions and queries involving multiple tables, so I was looking forward to models showing up trained on that. IRNet come out with a pretrained model in late 2019, and I started playing with it. It was great to see joins and more complex clauses coming into play. But a downside of work benchmarked on the Spider dataset was that the benchmark didn’t include what is called “value” prediction (such as figuring out parameters in a query), and research code developed with Spider in mind often skipped that. This makes sense from a research perspective, but generates incomplete queries. I noticed a network called ValueNet released recently. Building on IRNet, it adds on some machinery for guessing values, and generally makes more of an effort to produce fully executable queries. I appreciate that. Let’s try it out! As a concrete example, let’s use the Sakila database, as introduced on the jook website . This is a (fictional) database about movie rentals from physical stores back when that was a thing. It has 16 tables, with a rich relational structure: On the jook website , they give an example of a (human-written) query that “finds the actor with most films”: Let’s see what ValueNet can do: (I wrote movies rather than films because I’ve lived in the United States too long, but it works either way). The SQL looks plausible! It picked the right tables, relations, and aggregations. But the answer is just an ID. Let’s be more explicit that we want a name: Gina! Looks like the result matches what the human got. ValueNet gave us a name, but in a begrudging way, just the first name. Let’s get more specific again and ask for a full name: Neat, it gave us all the name parts in the table. It didn’t know how to construct a full name but who does really . To match the human query, let’s also ask how many films this Gina person acted in: Th
https://paulfitz.github.io/
I also found that there indeed (apparently) is a Kramer-like Prize, to wit: “the Russian billionaire Yuri Milner launched a campaign called the Breakthrough Initiatives, a $100-million dollar donation” (source Atlantic Magazine ca 2016).

I am surprised and disappointed not to find more (a) about what I used to know, and (b) what has been done in the decades since. I’m guessing this is a problem with Google, but still …

——— Added 29 April 2024 ——— after pondering ———

Paul Fitzpatrick chose to create a “single epic story” as his bootstrap. This is fun to read, etc.

However, my brain/mind is more or less back in the state it was when Long Nguyen and I — prompted by the recent “Christmas Project” initiated by Dan Ingalls* of getting the bits of “Notetaker Smalltalk” from a file that almost wound up on the trash heap going again — started thinking about 1000 year bootstraps.

Here we are thinking about “1000 light year bootstraps”.

Here is a ponder: the tradeoffs between McCarthy’s meta-circular definition and a Translator/VM definition, for example in Meta II by Val Schorre. The former only uses one language for both its own meta-language and its definition. Fun to read and figure out, but lacks clues to how to get it actually running on a real computer. The latter uses two languages: one to define the operators of an e.g. simple polish postfix byte-code VM interpreter (like the hardware of the B5000), and the second to create a compiler of the target language into a program using the VM’s operators.

The paper above that Long and I wrote — “The Cuneiform Tablets of 2015” (TCTO2015) — pointed out that such a simple machine could be described that any skilled programmer could implement in an afternoon — and that this would be enough to get the rest of the file actually running and participating in helping the aliens understand it.

I am possibly in the very same rut here, but this seems like the way to go for this project, and it should result a much simpler writing and testing. Basically, get a simple machine emulated, get the rest of the system going by itself, and make it so that the rest of the system is not just interactive but can negotiate meaning.

This would result in the “big file” containing something like the beginning of LINCOS — or more likely something like Fitzpatrick’s CosmicOS — but just enough to define a Turing complete dog simple machine such as the one in TCTO2015. Then have the rest of the system contain the next bootstrap steps and the image of the “negotiable agent”.

This also reduces the burden on testing and debugging the home grown AIs to see if human AIs can create the bootstrap VM from the bits without cheating. (This would have to follow some of the paths already trod by Doug Lenat and AM and Eurisko.)

I think this would result in a really fun and worthwhile “MacCready” phase for the larger issues.

Participants in the “Notetaker Smalltalk” project 2013-14: Dan Ingalls & Vanessa Freudenberg (leads), Ted Kaehler, Yoshiki Ohshima, Long Nyguen, Alan Kay



What do You think of Smallscript (https://www.codeproject.com/Articles/5373567/Smallscript-A-Smalltalk-Inspired-Scripting-Languag) ?
Have fun with it!

And see what you could do to allow systems to be made that are distributed over the Internet.

Also, these days, I think I would take a shot at various granularities of “pseudotime”, etc. (most things in sentence two above will need it anyway …)

Cheers



How does Alan Kay perceive the recent reconstruction of the Antikythera mechanism in relation to his "Cuneiform tablets" paper?
As with many other scientists, I’ve followed the analysis and reconstructions of the Antikythera mechanism for many years — and have been lucky to have had a number of extended conversations with some of the prinicipal investigators. And, likewise with many other scientists, I’m very interested in “early thoughts” — their substance, shape, precursors, contexts, etc. — and how to tease something substantial from any kind of past record — whether far past, or recent, or yesterday. My longtime great friend, colleague, and all around genius, Dan Ingalls, and I got very interested in how much “meaning and intent” one could include in a piece of software — rather than is the practice — even today — of having “meaning and intent” only be in the minds of the programmers, and sometimes in a few pieces of documentation or comments. For example, in the 60s and 70s, moving a piece of SW or a programming language, or a data base etc. a few 1000 miles, almost — and sometimes — required moving a programmer along with it. This was especially critical if the receiving end had a different machine, operating system, etc. And, the famous “Lick” had pointed out in 1963, that future scaling and networks meant that a main problem in the future would be “communicating with aliens” (meaning it would be similar to bridging gaps between many quite different contexts and beliefs). Earlier, Hans Freudenthal got interested in actually trying to “communicate with aliens” and had worked out a language and scheme — called LINCOS — to bootstrap such communication. There are a few even more interesting proposals that have been done since. Lincos language - Wikipedia Constructed language Lincos (an abbreviation of the Latin phrase lingua cosmica ) is a constructed language first described in 1960 by Dr. Hans Freudenthal in his book Lincos: Design of a Language for Cosmic Intercourse, Part 1 . It is a language designed to be understandable by any possible intelligent extraterrestrial life form, for use in interstellar radio transmissions. [ 1 ] Freudenthal considered that such a language should be easily understood by beings not acquainted with any Earthling syntax or language. Lincos was designed to be capable of encapsulating "the whole bulk of our knowledge". [ 2 ] The Lincos " dictionary " is intended to be transmitted first before any additional messages. It teaches natural numbers by a series of repeated pulses, separated by pauses. It then teaches >, <, =, +, -, by examples such as . . . . . > . . . (an extended pause is shown around > in this example so as to suggest to an alien that > is a new separate symbol; otherwise, an alien might think that the whole pattern is a new symbol of unknown meaning). In introducing =, unary notation is shown for numbers: . = 1, . . = 2, and so on. This progresses to multiplication, division, variables, and constants, then propositional logic , set theory , and first-order logic . The dictionary tries to introduce questions by leaving mathematical expressions unsolved (e.g., ? x x + 101 = 11). [ citation needed ] The next section of the Lincos dictionary introduces a word for second , "Sec", by playing pulses of various lengths, followed by Sec, and the number of seconds, "until the receiver may be expected to remark that the numbers... are proportional to the durations", [ 3 ] thus teaching both that Sec is a unit of time, and exactly how long it is. It then introduces means for measuring durations, referring to moments in time, and talking about past and future events. [ citation needed ] Freudenthal's third section is perhaps the most complex, and attempts to convey the concepts and language necessary to describe behavior and conversation between individuals. It uses examples to introduce actors speaking to each other, asking questions, disapproving, quoting other people, knowing and wanting things, promising, and playing. The first steps (having already introduced sets of numbers and questions) are to introduce some new symbols (distinctive patterns of pulses), say that they are NOT numbers, and transmit sequences showing two of these new symbols separated by the word "Inq" (inquiry), followed by a question about an equation, then the symbols reversed, followed by the answer (example below). It is thus expected that, after many repetitions, the recipient will determine that these new symbols are entities asking and answering the questions, rather than some other context for the questions. [ citation needed ] Finally, the fourth section describes the concepts and language relating to mass , space , and motion . This last section goes so far as to describe physical f https://en.wikipedia.org/wiki/Lincos_language “Bootstrapping” SW was a favorite topic in the ARPA research community — whose main aim was to go far beyond incremental improvements of the present day. Bootstrapping LISP is a really fun topic and endeavour, and the Engelbart folks really got into bootstrapping in the mid to late 60s. Another super fun example was Val Schorre’s “Meta II” paper, which included instructions how to bootstrap it, plus many examples of non-trivial programming languages made with it. The origin of Smalltalk at Xerox Parc came from a “hallway discussion” about “tiny most powerful kernels” for languages, and I thought that if you took what was understood about systems in 1972 and “inhaled the fumes of LISP and Meta II” the right way, you could get something that was even more (quite a bit more) expressive in the same tiny size. I went away on a trip, and when I came back Dan had implemented my little example as a proof of concept — and it was extremely fun to play with. When the Alto started working in April 1973, Dan and Ted Kaehler immediately reimplimented it, and in just a few weeks, we had something to experiment with that had real expressive power. Our group had an extreme learning curve, and this was aided and abetted by the quick reformulations of the whole system that could be done via its meta-kernel. —— jump cut to 10 years ago —— An almost destroyed Alto disk pack had a suspicious file on it that looked like it might be “Notetaker Smalltalk” (or “Smalltalk-78”). As a “Christmas Vacation project” Dan, Vanessa, Ted, Yoshiki, myself, and a few others, decided to see if it could be brought back to life. This was possible because most of the system — which included its operating system — was written in itself and the file contained a complete image “snapshot” of the system at a point in time. This is described (in itself) in a video I did for Ted Nelson’s birthday: And in some detail in a paper — Reviving Smalltalk-78 — we (Dan and Vanessa mostly) wrote. This got us thinking about “what should a file look like to do the same bootstrapping by programmers a 1000 years in the future?”. Long Nguyen and I did a number of investigations about ways to do this. We did not go the entire LINCOS route, but decided to see if we could describe in a simple one page bit-map image a tiny VM that could bring up the entire system on a supercomputer of the future, and then that system could help the future programmers implement a much more efficient kernel. So the overlap between the Antikythera mechanism and the “Cuneiform Tablets Of the Future” is pretty small: it is mainly about making reconstructions in the future of “lost” interesting past mechanisms much easier.



Has Alan Kay started any activist movements to realize his vision of personal computing? What were the hurdles? The negative impact of capitalism and corporations, etc. What one can do as a political act?
This is not my nature.

But Doug Engelbart had some feel for this — and still, wound up not getting through to the larger organizations of society.

I think part of the problem is that in our current day (and probably most past days), any real inventions — even with great power — that required quite a bit of new learning, are bypassed by most (who tend to wait until there is something, no matter how tiny, that they can sell).

So the “mouse” succeeded but the best of his ideas were not only ignored, but not learned enough to even be ignored.

This used to piss Doug off. He use to say “The mouse is just a button on the car radio — we invented a whole car!”

Eeecks!



What does Alan Kay think about collaboration as it applies to Xerox PARC? What about the structure of PARC, and the people who were there contributed to such amazing collaborations and outcomes?
There are parts of the answer to this question in some of my other answers about Xerox Parc. (I would put in links here if Quora were easier to search — maybe some readers will help.)

The best book I’ve read about this research community is “The Dream Machine” by Mitchell Waldrop (it is in print). The book looks at the larger story of ARPA’s “Information Processing Techniques Office” (IPTO) via its founder JCR Licklider (universally known as “Lick”, a wise experimental psychologist who got interested in the potential of interactive computing as a potential “symbiotic intellectual partner”. This book is a must because Parc was in most respects an offshoot and integral part of the IPTO research community — its researchers were mostly recruited from IPTO and they brought its way of doing things. It was perhaps a bit “more like IPTO than IPTO” because Bob Taylor (who had been a director of IPTO in the 60s) pushed specifically for some of the best processes he had seen.

These included “arguing a lot, but only to make progress, not to win”, hiring processes and day to day interactions that minimized rivalries and maximized possibilities for cooperation, etc. My only real complaint about “The Dream Machine” and most of what has been written about Parc is the omission of the amount of cooperation in the whole ARPA community, and *why?*.

Some of this “zeitgeist” is traceable to the many activities around MIT during WWII, and especially the massive radar development effort in Building 20 (the “RadLab”), that was catalyzed by “various gifts” by the British and by look ahead visionaries.

Jerome Wiesner was a student while in the “RadLab”, later got a PhD at UMich, was head of the research lab of electronics at MIT, President Kennedy’s science advisor, then President of MIT, formed the Media Lab with Nicholas Negroponte, etc. etc. He was a “smart solid citizen” type (as Dave Evans would say). He had numerous experiences in the RadLab, and was very interested in why it worked, given the normally highly opinionated egos and top talents during that time (many Nobel Prizes were later awarded to RadLab members).

Jerry said that the original group decided at the beginning that the only thing that counted was “progress in the development of all the different kinds of radar system needed”, and that “egos had to be checked at the door”. This apparently partly led to the notion of “arguing for progress rather than to win”, and this was enforced as part of the social mores of the lab. Another part of the social glue of the RadLab was the fun of making gadgets that all shared, and this developed into “the fun of making gadgets together”, where the complementary nature of the top talents was recognized and synergized. This led to a special kind of “cooperative competition”. (This apparently also happened in its own way at Los Alamos.)

Jerome Wiesner - Wikipedia
American electrical engineer, science policy adviser, and university president Jerome Wiesner 13th President of the Massachusetts Institute of Technology In office 1971–1980 Preceded by Howard Johnson Succeeded by Paul Gray Director of the Office of Science and Technology In office January 20, 1961 – January 24, 1964 President John F. Kennedy Lyndon B. Johnson Preceded by George Kistiakowsky Succeeded by Donald Hornig Personal details Born Jerome Bert Wiesner ( 1915-05-30 ) May 30, 1915 Detroit , Michigan , U.S. Died October 21, 1994 (1994-10-21) (aged 79) Watertown, Massachusetts , U.S. Spouse Laya Wainger Education University of Michigan ( BS , MS , PhD ) Awards IEEE Founders Medal (1977) Vannevar Bush Award (1992) Scientific career Fields Electrical engineering Institutions Massachusetts Institute of Technology Los Alamos Laboratory Thesis Pre-ignition phenomena in gas switching tubes and related rectifier burnout problems (1950) Jerome Bert Wiesner (May 30, 1915 – October 21, 1994) was a professor of electrical engineering , chosen by President John F. Kennedy as chairman of his Science Advisory Committee (PSAC). Educated at the University of Michigan , Wiesner was associate director of the university's radio broadcasting service and provided electronic and acoustical assistance to the National Music Camp at Interlochen, Michigan . During World War II , he worked on microwave radar development at the MIT Radiation Laboratory . He worked briefly after the war at the Los Alamos National Laboratory , then returned to MIT's Research Laboratory of Electronics from 1946 to 1961. After serving as Kennedy's science advisor, he returned to MIT, becoming its president from 1971 to 1980. He was an outspoken critic of crewed exploration of outer space , believing instead in automated space probes. He challenged NASA 's choice of developing the Apollo Lunar Module as a means to achieving Kennedy's goal of landing men on the Moon. At Kennedy's direction, he investigated Rachel Carson 's criticism of the use of the pesticide DDT , and issued a report in support of her claims. He was an advocate for arms control , and a critic of anti-ballistic missile defense systems. While MIT president, he was put on President Richard M. Nixon 's extended "enemies list" . Early life and education [ edit ] Wiesner was born in Detroit, Michigan, the son of Jewish immigrants from Silesia [ 1 ] and raised in Dearborn. He attended Fordson High School. He received a Bachelor of Science (BS) degree in electrical engineering and mathematics in 1937, and a Master of Science (MS) degree in 1938, at the University of Michigan . He received a Doctor of Philosophy (PhD) in electrical engineering from UM in 1950. [ 2 ] Interested in radio broadcasting and acoustics, he was associate director of UM's radio broadcasting service. He also participated in studies of acoustics, and assisted in developing electronic techniques, at the National Music Camp at Interlochen, Michigan . [ 3 ] In 19
https://en.wikipedia.org/wiki/Jerome_Wiesner
Another aspect of this to consider is that there was very little competition for funding — WWII — and especially Roosevelt’s science advisor former MIT professor of EE: Vannevar Bush — made sure there was a lot of funding available for many important projects. Years later this was not lost on ARPA and Lick (who realized that funding whole departments in universities made a lot of sense, and would soften the usual university political competitions for space and students, etc.).

In any case, by the time Parc was set up — and by one of the former ARPA IPTO directors — there were plenty of young IPTO researchers in the various ARPA projects who had been brought up in the “ARPA Dream” of “interactive computer intellectual amplifiers for all, pervasively networked worldwide”, and were used to the “ARPA way” of doing things, and especially how the young researchers were able to cooperate (and that ARPA IPTO funded their development and cooperations).

So the big vision was already in place, and so were the researchers, and so were most of the processes. The catalyst was Bob Taylor — who knew, and had funded, most of the researchers in the community — and he was a critical factor in convincing us that it would be OK to “do research in a company like Xerox”. *Parc would not have happened without Taylor*

There was also a lucky happening in the earliest days of Parc. The group around Butler Lampson — one of the most brilliant of the young researchers in the 60s — had been at Berkeley, and then set up a company to make an advanced time-sharing system (the BCC 500). A business downturn made it impossible for them to continue at just the time Taylor was setting up Parc. Bob knew them and was able to convince the whole company to go back to being researchers and all join Parc. So, in one swoop — Jan 1971 — Parc got a critical mass of about 12 super-top former ARPA IPTO people “who could do anything”.

I was consulting for Taylor at the time, and was about to go to CMU to work on “personal computing”, but this got me to call Pittsburgh to tell them that “things had suddenly gotten much too interesting in Palo Alto”. The combination of Butler Lampson and Bob Taylor was a tremendous draw for further recruiting.

Butler Lampson - Wikipedia
American computer scientist Butler W. Lampson (born December 23, 1943) is an American computer scientist best known for his contributions to the development and implementation of distributed personal computing . Education and early life [ edit ] After graduating from the Lawrenceville School (where in 2009 he was awarded the Aldo Leopold Award, also known as the Lawrenceville Medal, Lawrenceville's highest award to alumni), Lampson received an A.B. in physics ( magna cum laude with highest honors in the discipline) from Harvard University in 1964 and a PhD in electrical engineering and computer science from the University of California, Berkeley in 1967. Career and research [ edit ] Professional Developers Conference 2009 Technical Leaders Panel During the 1960s, Lampson and others were part of Project GENIE at UC Berkeley. In 1965, several Project GENIE members, specifically Lampson and Peter Deutsch , developed the Berkeley Timesharing System for Scientific Data Systems ' SDS 940 computer. After completing his doctorate, Lampson stayed on at UC Berkeley as an assistant professor (1967–1970) and associate professor (1970–1971) of computer science. For a period of time, he concurrently served as director of system development for the Berkeley Computer Corporation (1969–1971). In 1971, Lampson became one of the founding members of Xerox PARC , where he worked in the Computer Science Laboratory (CSL) as a principal scientist (1971–1975) and senior research fellow (1975–1983). His now-famous vision of a personal computer was captured in the 1972 memo entitled "Why Alto?". [ 1 ] In 1973, the Xerox Alto , with its three-button mouse and full-page-sized monitor , was born. [ 2 ] It is now considered to be the first actual personal computer in terms of what has become the "canonical" GUI mode of operation. All the subsequent computers built at Xerox PARC except for the "Dolphin" (used in the Xerox 1100 LISP machine) and the "Dorado" (used in the Xerox 1132 LISP machine) followed a general blueprint called "Wildflower", written by Lampson, and this included the D-Series Machines: the "Dandelion" (used in the Xerox Star and Xerox 1108 LISP machine), "Dandetiger" (used in the Xerox 1109 LISP machine), "Daybreak" ( Xerox 6085 ), and "Dicentra" (used internally to control various specialized hardware devices). At PARC, Lampson helped work on many other revolutionary technologies, such as laser printer design; two-phase commit protocols ; Bravo , the first WYSIWYG text formatting program; and Ethernet , the first high-speed local area network (LAN). He designed several influential programming languages such as Euclid . Following the acrimonious resignation of Xerox PARC CSL manager Bob Taylor in 1983, Lampson and Chuck Thacker followed Taylor colleague to Digital Equipment Corporation 's Systems Research Center . There, he was a senior consulting engineer (1984–1986), corporate consulting engineer (1986–1993) and senior corporate consulting engineer (1993–19
https://en.wikipedia.org/wiki/Butler_Lampson
I have written a Quora answer about Butler (somewhere). One of many big deals here was that he was super-brilliant, highly opinionated, could argue rings around anyone, very devoted to his ideas and projects *and* was also very helpful to projects that were not particularly in line with his opinions (like the ones I was running). This was crucial to Parc’s success, in that it could sustain multiple projects with similar goals but different methods *and* cooperate whilst doing so. (This was not perfect, but was generally done very well as far as I was concerned.)

I give most of the ultimate credit to the combination of Bob and Butler and how they complemented each other’s roles.

——— Added 22 April 2024 ———

I forgot to add a different longer range slant by some of the same pioneer players. For example, Lick and Bob Taylor wrote an influential paper in 1968 “The Computer As A Communication Device” that is well worth reading. Here is one quote from it:

“Take any problem worthy of the name, and you find only a few people who can contribute effectively to its solution. Those people must be brought into close intellectual partnership so that their ideas can come into contact with one another. But bring these people together physically in one place to form a team, and you have trouble, for the most creative people are often not the best team players, and there are not enough top positions in a single organization to keep them all happy. Let them go their separate ways, and each creates his own empire, large or small, and devotes more time to the role of emperor than to the role of problem solver. The principals still get together at meetings. They still visit one another. But the time scale of their communication stretches out, and the correlations among mental models degenerate between meetings so that it may take a year to do a week's communicating. There has to be some way of facilitating communication among people without bringing them together in one place."

They had both funded Engelbart, and this is a restatement of Doug’s central idea and purpose — and in fact was central to what Parc’s vision was all about. So — looking at this from Taylor’s point of view — given that the technology to do the above was not yet invented, a main problem with Parc was going to be (a) the absolute need for “the few people who can contribute effectively” to be brought together, and (b) to stave off the likely sources of “trouble” from those “who are not likely to be team players”.



How does Alan Kay view the connection between the relational model and "real OOP"? How would you avoid “impadence mismatch” problem with RDBMS? How was with Smalltalk Systems?
This deserves a longer answer than this.

One interesting history is that of “unit records”, eventually being kept as a deck of “Hollerith” and then “IBM” cards. The deck would most usually use one card for each record, and would use the same fields — a sequence of characters — on the cards for the same kind of data. The punched card accounting machines used “plugboards” to program operations on the deck (such as sorting, tabulating, etc.). A deck was thought of as a kind of “table” with fields running left to right and records from top to bottom.

Typical fields would be “Employee Number”, “Name”, “Salary”, “Date of Birth”, etc.

Fields one would *not* find would include e.g. “Age”. So answering a question like “Age of Employee X” was a vastly different question than e.g. “Name of Employee X”.

Eventually, the cards were put to tape — initially one for one with the cards — and eventually with much longer records and more fields.

A very high percentage of data on IBM cards, was processed by IBM machines and then computers.

IBM had an opportunity in the early 60s to move from this limited model of the world to one that included “processes as things to be retrieved”. For example, there was no actual barrier that prevented IBM from including runnable code within a record itself, or part of a “data file” on a tape.

This would allow dealing with more complex kinds of things one might want to save, find, ask questions about, etc. For example, at this point “Age” could readily be a field that directed a computer to fetch code to run that would look at a Date Of Birth and today’s date, etc. Such structures existed in the early 60s, especially in several labs at MIT(where IBM also had a large research lab).

It is worthwhile to try to understand both the corporate mind and the mind of the markets it served to understand why IBM and other companies persisted in recapitulating the old card image system, even though it actually wasn’t a very good way to model the complex world we live in.

Similarly, one can see this tendency to “automate the obsolete (but familiar)” in the use of terminal emulators, which themselves were emulations of punched cards and key punch eras.

A really interesting side part of the story is that “transactions” did need to be introduced, so relational data bases were now living in “pseudo-time” (a very good idea).

Part of the early motivations for objects came from the desire to deal with modeling and querying in the simplest uniform way (the “Age” query is a simple example that breaks the oversimplification of rudimentary data bases).

—— Added 5 May 2024——

The other two answers made me realize that I didn’t address the Smalltalk of the question, and the other two answerers thought I did.

What I would do would be to design a “knowledge” system with a semantics that includes as many relationships including temporal ones as possible. This would try to work both from models and from the kinds of questions and operations that might be desired (so: from the end-user’s point of view).

This would be very much in keeping with some of the classic work in “old time AI and Expert Systems”. An interesting example of a large version of this was Doug Lenat’s CYC system.

About Smalltalk: at Parc we created a new kind of Smalltalk and implemented from scratch every two years (72, 74, 76, 78, 80).

Something weird that happened when the first version was released to the public — Smalltalk-80 — is that — despite this version having all the tools to make very different kinds of languages (and Smalltalks) — everyone “on the outside” just took ST-80 as a “language” rather than as a “metalanguage for making the kinds of systems you want and need”. This completely missed what Smalltalk and Smalltalk programming was really all about.

So: start with the knowledge system design you want and need, then use useful metalanguages to implement a test version, then use usefulmetalanguages to make a solid releasable version.



What does Alan Kay think about Ray Kurzweil’s concepts of Singularity and The Law of Accelerating Returns?
These ideas have been around for some time, and are due to many people (see the wikipedia article for a short summary). Accelerating change - Wikipedia Perceived increase in the rate of technological change throughout history In futures studies and the history of technology , accelerating change is the observed exponential nature of the rate of technological change in recent history, which may suggest faster and more profound change in the future and may or may not be accompanied by equally profound social and cultural change. In 1910, during the town planning conference of London, Daniel Burnham noted, "But it is not merely in the number of facts or sorts of knowledge that progress lies: it is still more in the geometric ratio of sophistication, in the geometric widening of the sphere of knowledge, which every year is taking in a larger percentage of people as time goes on." [ 1 ] And later on, "It is the argument with which I began, that a mighty change having come about in fifty years, and our pace of development having immensely accelerated, our sons and grandsons are going to demand and get results that would stagger us." [ 1 ] In 1938, Buckminster Fuller introduced the word ephemeralization to describe the trends of "doing more with less" in chemistry, health and other areas of industrial development . [ 2 ] In 1946, Fuller published a chart of the discoveries of the chemical elements over time to highlight the development of accelerating acceleration in human knowledge acquisition. [ 3 ] In 1958, Stanislaw Ulam wrote in reference to a conversation with John von Neumann : One conversation centered on the ever accelerating progress of technology and changes in the mode of human life, which gives the appearance of approaching some essential singularity in the history of the race beyond which human affairs, as we know them, could not continue. [ 4 ] Moravec's Mind Children [ edit ] In a series of published articles from 1974 to 1979, and then in his 1988 book Mind Children , computer scientist and futurist Hans Moravec generalizes Moore's law to make predictions about the future of artificial life . Moore's law describes an exponential growth pattern in the complexity of integrated semiconductor circuits. Moravec extends this to include technologies from long before the integrated circuit to future forms of technology. Moravec outlines a timeline and a scenario [ 5 ] [ 6 ] in which robots will evolve into a new series of artificial species, starting around 2030–2040. [ 7 ] In Robot: Mere Machine to Transcendent Mind , published in 1998, Moravec further considers the implications of evolving robot intelligence , generalizing Moore's law to technologies predating the integrated circuit , and also plotting the exponentially increasing computational power of the brains of animals in evolutionary history. Extrapolating these trends, he speculates about a coming "mind fire" of rapidly expanding superintelligence similar to the explosion of intelligence predicted by Vinge. James Burke's Connections [ edit ] In his TV series Connections (1978)—and sequels Connections² (1994) and Connections³ (1997)— https://en.wikipedia.org/wiki/Accelerating_change A graph I sometimes use in talks to help put more context on these claims looks like this: The key idea here is that — at some point in the past — we started improving “power” via invented techniques and methods, and much of this is in accord with compound growth laws. However, the “wisdom” (*green*) needed to use the power has not followed the same exponentials (and one could argue that the slight linear increase I show here is more of a wish than an actuality). The *red* is a time and state when whatever wisdom is around cannot prevent a disastrous use of the powers. Vi Hart has a very good line for what we need: “We must ensure that human wisdom exceeds human power”. Humanity has done a terrible job of even recognizing this problem, much less trying to deal with it.



What is the most shocked you have been when waking up?
My wife and I were renting a flat in London — where she was working on a book. I woke up in the middle of the night — for “old men’s reasons” — and decided to see how much Hilary Clinton had won by. I don’t think I’ve ever been more surprised upon waking than I was to find that Trump was going to win the election. I realized that I was vastly out of touch — out of undertanding — with tens of millions of my fellow American citizens. It wasn’t the “unusualness” of Trump, but the idea that millions who had gone to school in the US had not learned enough of the real issues — or how to think about them — to see the monumental mistake they were making. This also helped me put in context the also recent “Brexit” results here in the UK — and I think with similar failures of public education here (this is just a surmise by a fairly ignorant visitor). An American politician — Tip O’Neal — once said: “All politics is local”. The extent to which he was right was — and is — shocking. —— Added 24 April 2024 —— I should have added that what jumped into my mind after the shock was a comment made by the commentator H.L. Mencken in 1920: “As democracy is perfected, the office of president represents, more and more closely, the inner soul of the people. On some great and glorious day the plain folks of the land will reach their heart's desire at last and the White House will be adorned by a downright moron.” ― H.L. Mencken, On Politics: A Carnival of Buncombe



What is Alan Kay's advice for smart people who don't yet know what they're interested in working on? How deep should people explore topics like maths and science before deciding it isn't for them?
The advice given by “The Valark” below is pretty good. Part of the idea is that for the most part each person is interesting and idiosyncratic enough to need “custom advice”, which is often best obtained through many experiences of their own. Einstein said “Love is a better teacher than Duty”, and this means in part that what a guy named Alan Kay might have to say is likely to be irrelevant.

Added April 15th 2024 — it occured to me that I should have included what I personally wound up doing. I have put this at the end to not mess up the original sequence

But we should look at the second — more general — sentence of this question. To me, it is in the category of “How far should I try reading and writing before deciding it isn’t for me?” My reaction to this is “These are too important to decide to ignore for any reason: please press on to gain enough fluency to be part of the literate world”.

I feel the same way about mathematics and science, and will explain why in a minute. First please read this answer where I mostly quoted Kurt Vonnegut about art:

Quora
https://www.quora.com/qemail/track_click?aoid=MEOH3bjMmtq&aoty=16&aty=4&cp=0&ct=1712701034538708&et=103&force_notif_url&id=477646c9adb440d3bb1833e83e31cca4&notif_url=%2FDo-you-agree-with-Alan-Kays-comment-that-a-computer-science-degree-teaches-students-how-to-write-bad-code%2Fanswer%2FAlan-Kay-11&request_id=7&src=1&st=1712701034540906&stories=68412595569&uid=aJ9ciL9ZPHa&v=0
To the list of arts he asked the children to pick one, and do something just for the hell of it. He could have added maths and science — but he would have to give a little hint because most children or adults are not as aware of these as they are about drawing a picture or writing a little poem.

This is because the way “maths” and “science” are presented in school could hardly be farther from their actual nature. For example, writing a little poem or drawing a little picture are rarely (I hope!) presented to children as a means to some end, but learning math and/or science is all about ends rather than how the experience works as fun and internal mental vitamins.

A nice Marvin Minsky quote is: “You don’t understand something if you only understand it one way”. This raises the ante on how we use the idea of understanding, and it is another way to approach the paths that Vonnegut urges for the children — a lot of his urging is to explore what it means to make something yourself (and in a principled way). Picasso had a nice line for this: “Learn the rules like a pro so you can break them like an artist”.

We often have to be reminded about the largest and most important difference between maths and science: that our ideas and beliefs and reasoning about them (maths) happen strictly between our ears, and thus can be consistent but not at all in accord with our external situation or what we should be doing — whereas science is done with the knowledge that it is being thought about by our limited brains and so we need to put in a lot more work to draw any kind of conclusions that might have something to do with “What’s out there?”.

Einstein, in a talk in 1921, felt he had to remind his audience of physicists about this: he felt they were taking their mathematical conclusions far too literally about “What’s out there?”. He said to them:

“As far as the laws of mathematics refer to reality, they are not certain;
and as far as they are certain, they do not refer to reality.”

If we are able to see that what he means by the “laws of mathematics” refers to all human reasoning, then we can see why getting fluent in maths and science is critical for general daily like (as with reading and writing). He is talking about the need for a special new kind of sanity for humanity: one that both allows internal reasoning and then requires it to be “negotiated” with the actual universe we live in (whether within our families, towns, countries, etc).

Just before he told them the above, he pointed out why we need mathematics (better more careful thinking and relating):

“it is mathematics that offers the exact natural sciences a certain measure of security which, without mathematics, they could not attain.”

Again, please realize this says “It is clear thinking and reasoning that offers our dealings with the world a security which, without clear thinking and reasoning, they could not attain”.

Then couple this with “As far as our clear thinking and reasoning refer to reality, they are not certain, and as far as they are certain, they do not refer to reality”, and you will have a major idea in your grasp, that all humanity needs to understand and practice.

Maths and science provide many more ways to view our situations, and the science part also protects our powerful conclusions from becoming like religious beliefs — they must remain tentative with respect to “reality”.

“Mapping” is a part of “maths and science”, and also a good metaphor for the larger scope of their pursuits. In human history we have had maps constructed from beliefs — e.g showing a location for the Garden Of Eden, and many maps that attempt to depict what travelors will find. Both are inaccurate. The best of the latter types can be very helpful. None of them depict actual “Truth” — in they end they merely represent beliefs. The depiction of a map is its “math part” and its relationship to what is found when visited is its “science part”. I think we can see why getting this clear and straight is critical to human life and society.

In other words, “science and math” are not just about dealing with the physical world. Enlarged to their real definitions and methods, they are really about dealing with the defective maps we humans create to try to understand everything around us. Getting better at this is not just improving how we operate, but it is critical if we want to have a workable strongly participative society.

The great educator of children — Maria Montesorri — pointed out that the main business of early childhood education is to have the children experience in their environment the strongest ideas of our civilization — to have their outlook become the best we humans can do. To do this we need to heed the ideas of all the above thinkers — and more — to help others to gain a much richer notion of where and who they are.

Added April 15th 2024 — here’s what I wound up doing with regard to the first question. This is not advice, just history. I had no plan or future goals, but was intensely curious and had learned to read fluently several years before school.

School was not a good fit, but I kept going back all the way through college, because there was more chance of getting a good steer than by just being an autodidact, and this even after serving in the US Air Force. I wound up doing way too many things, but the critical thresholds in school helped these be non-trivial.

So: pure math, molecular biology, anthopology, English (especially theatre), jazz, pit playing, classical organ, drawing and painting, and paying for all with the trade I learned in the Air Force: programming. This resulted in a burnout, but also graduation from U of Colo.

The burnout made me not want to go to grad school in either math or biology, and I didn’t want “a job” I bummed around for a few months — with what turned out to be anxiety attacks (I don’t recommend these) — until I had the thought: “I can program but I don’t know anything about computing: I could spend a year somewhere getting a masters and cool off”.

I liked the altitude and climate in Boulder, so went to the library to look for places that (a) had a Masters in computing, and (b) were above 4000 feet in altitude. There was just one in 1966: the U of Utah, and they had just started an “ARPA research project”. I had never heard of either. This turned out to be truly great luck, because it was just right for my peculiarities. I later found out that I was accepted because Dave Evans didn’t look at transcripts, just resumes — he said that transcripts are almost never good predictors of success as research grad students.

A fun footnote is that over the next few decades I wound up using every single thing I had learned when I didn’t know what I was doing, but was just being super curious. This could possibly be the next best thing to specific advice when you don’t know what to do: keep learning what you are interested in — you will have so many possiblities when you stumble into a good situation.



Did pipe organs develop at all during the classical period?
“Classical period” in history (e.g. the ancient Greeks) or “classical period” in music (e.g. Haydn, Mozart, etc.)? The answer of Alan Piersonn to “Did the Greeks create the first pipe organ?” gives the bare facts for the ancient Greeks. If the question meant that, then “Yes” is the answer. Technically, during the Alexandrian Greeks period (which happened many decades after the “Periclean Greek Golden Age” — but was not at all inferior in intellectual powers and knowledge). For the second meaning of the question — the time period according to Wikipedia of about 1750 (J.S. Bach’s death) to 1820 (almost the death of Beethoven & Schubert) — this is a bit ticklish to answer. My “more than a hobby” for most of my life has been as an organist, who has also helped make and design a few. I used to own a really nice 17th century style pipe organ for many years (it has since been donated to a US university). My personal short answer to the literal question would be “Hardly at all to no real change or progress during that time”. This might be too harsh, but — as far as design and conception — there was virtually no change, and one could argue that one of the main “golden ages” of organ conception and design had peaked around the end of the 17th century with what is sometimes called the “North German” or “Schnitger style” instruments. This style got started partly because of the Hanseatic League trading cities, and their wealth and somewhat competition fueled the development of large instruments with full pedals and a wide variety of sounds. However, a fun part of organ history is that — in France, starting arguably 20 or so years after the classical period, and probably fueled by the orchestral sounds of “Romantic Music” — there appeared a remarkable young organ builder by the name of Arisitide Cavaille-Coll who had his own — and very differnent — ideas about organ sound and how to build them. Cavaille-Coll built his first instrument — St Denis in 1843 — at a young age. It was quite large (he must have been an impressive young man to have been funded) and it was built behind the older case shown here. From the start he had a particular kind of sound in mind — that in part involved the kind of blending of voices and effects (such as crescendos and dimenuendos) that orchestras were starting to do, and which included several inventions of new pipe sounds (such as his special conception of “harmonic flutes”, etc.) that solved some old problems and allowed new ideas about tone. These organs were also possible because the 19th century revolutions in mechanical technique also included an invention by an Englishman — Barker — which was essentially a “pneumatic amplifier”, sensitive and powerful enough to be inserted between the keys the organist played and the valves in the chests that admitted air to the pipes. This permitted very large and much louder instruments to be built, and yet to be played musically and delicately. This was the start of a revolution in organ building in the 19th century. Cavalle-Coll’s company made most of the large organs in France (and many of the small ones), about 600 during his lifetime. An important point to end this too short answer is that while C-C revolutionized organ sound and building, his instruments did not obsolete the earlier 17th century instruments (though they did for awhile in the minds of the avant-garde). Today we organists think of these historical instrument types as two parallel peaks — two golden ages — in the long long history of the instrument (which indeed does go back to the ancient Greeks B.C.). Today there are many wonderful examples of original instruments from both the Golden Ages that are regularly played.



If Alan Kay could travel in time, back to the inception of computing, but could only take a notebook as a knowledge base from today, which insights would he pick/ignore, how would he approach recreating computing, and which year would he choose?
I don’t know how to answer this question directly. But let’s consider Doug Engelbart, who in effect, did something very much like this. In 1962 he wrote an extensive research proposal (for funding from the Air Force) with the title: Using the phrasing of the question, here is his “notebook as a knowledge base from today” brought back from the future he was able to generate in his mind. In this “notebook” he included most of the important things that were needed and would be needed. Virtually none of them were actually doable in 1962, and the spectactular famous “Mother of all demos” he and his group created by 1968 and showed 2500 people in San Francisco, was just a bare subset of the notebook. A much smaller subset (along with distortions) got implemented over many years by industry, and the center of most of Engelbart’s important ideas is lacking in today’s technologies. My points here are (a) bringing “notebooks back from the future” are what the visionaries have always done, (b) almost no one will listen to anything that is qualitatively different from the norms of the day. What has been learned over and over is that humans are generally resistent to ideas and tools that do not seem to benefit them directly and quickly. And — unless there is a war or equivalent enormous obvious threat — society will not fund anything that isn’t simply derivative from the “normal present” — no matter what the ROI has been in the past.



How can I get in contact with Alan Kay for an interview for my high school project? I want to interview him about the history behind the development of the GUI and his work at PARC
Please tell me more about your high school project …

In any case, a good place to start is to read enough of what is available to gather enough context to ask good questions and to see how the answers fit with history.

Here’s a relatively short tribute I wrote about the larger research community I was involved with. It has a fairly extensive bibliography. The Power of the Context

Some of the work I and my group did at Parc is in a history requested by the ACM “The Early History Of Smalltalk”

A longer white paper I wrote for the MacArthur Foundation in the UK is about “How?” really big efforts have been set up and succeeded.

A book about the larger context that is pretty good is “The Dream Machine” by Mitchel Waldrop.

———— I’m still hoping for a comment from the student who asked the question …



If Alan Kay were starting a computing research group today, how would he think about where to locate it?
Interesting query. I have only had extensive experience with one style of “research grouping” — the ARPA/Parc style was very successful — but it is quite possible that there are other styles that would also work very well.

The central idea in most groups, and especially research groups, is to have the group be far more capable than the most able single members. One way to do this is to find ways to have diverse top people be able to synergize their abilities. This is not easy, but has been done quite a few times.

Another issue is scale. My experience is that the size of a good research group (up to 10 or even 20) is not enough to create the larger “research environment” needed (which needs to have several top research groups). Parc was (possibly) the smallest “research environment” that provided what research groups needed for themselves, and what produced the larger richness that the whole environment needed.

IMO the above (still today) needs to have everyone co-located.

Another level of scale is “research community” — for example, the ARPA-IPTO research community — which can span the larger field of interest (still within the larger visions). In the 60s this was about 15–17 large “ARPA Projects”, mostly at universities and some at defense related companies. Grad students and interns were the main messaging actors, with a variety of larger meetings each year of healthy subsets of the community (for example, each year there was a week long “ARPA grad students” meeting of the “top two” students from each project).

Everyone could see that some of the advanced technologies the community was inventing would be of great use to the community: personal computing, pervasive networking, next versions of the Engelbart system, etc. An interesting side note here is that the Engelbart system had a version of collaboration that was more advance than exists today, and that worked very well for distributed meetings. However, their preferred method was to have all in a meeting be in the same room *and* to use their advanced system for collaboration.

The co-location needs stemmed from the ARPA-Parc practise of “bull-shitting around”. A lot of time was spent in ways that would horrify most managers today: lots of beer, lots of hiking, biking, tennis, skiing, etc. An important part of this is that the researchers were basically fanatics about their projects, so lots of hours were spent on these, but just when each day was at the discretion of the researchers. The “bull-shitting around” also included a lot of “semi-serious” speculations about “life, the universe, and everything else”. This often included spur of the moment insights about the actual research work.

I don’t know how to explain the previous paragraph any better. This doesn’t work nearly as well at the smaller “research group” size as it does at the “research environment” size.

One bottom line: although many good things can be done with VR/AR, the latter is not nearly enough. More recent experience shows that Bret Victor’s “Dynamicland” approach — which was not technically possible in the 70s at Parc — would have really boosted the work.

Now we get to the crux of the question. A huge problem today is that the inflation of real estate prices far beyond regular inflation has changed the dynamic and constraints for all researchers, and especially young researchers. For example, I was a year out of my PhD when I joined Parc and my little house in Palo Alto within biking distance was about twice my modest yearly salary. Since then regular inflation has perhaps been a factor of 5–6, whereas the house I had back then has inflated by at least a factor of 50 (meaning that real estate has added an additional factor of 10).

If we add in the intangibles of “being near a major university”, etc., then we have severe problems with desirable locations and funding that didn’t exist back in the 60s and 70s.

Most universities (and companies) were slow to see that their future staffing partly depended on them being in the real estate business (Stanford was an exception).

I will add more to this after seeing a few comments.



If earning a lot of money is my target, what major should I choose? My interest was in computer science, but people are saying to me that don't do it because AI is going to replace it. What should I do?
I don’t think I can give you a satisfactory answer for a number of reasons. The first and largest one is that I’ve never had making money as a target.

I have always thought that “making money” is much too small a goal for human beings. At best, it could be “something to be noticed occasionally whilst doing something you love”. Along with that, “quality of effort” is almost always a better way to judge yourself than “quality of results”. A high former often brings a high latter, whereas just a desire for a high latter can be nerveracking and wreck what you are trying to do.

That great philosopher — Linda Ronstadt — once said: “All you can do is follow your instincts, and refine your talents to support your instincts”.

The second reason is that it is hard to deal with the topic of “AI” in the large these days without many more definitions and side conditions — and especially with (current) “AI” and programming, let alone computer science.

“AI” used to mean what is called today “Artifical General Intelligence”. The original term was co-opted — as sometimes happens when people want to claim progress by relabeling what they are doing as being an original harder art form. (This happened for “object-oriented” as well.)

A larger problem is to try to replace something that affects human life and society with something easier to use but is much more dangerous. Examples include systems that work some of the time but have deadly flaws that require real understanding of the embedded context to understand. The working some of the time allows demos and sales, but the flaws can and do literally kill. Examples include devices such as the Boeing 737 Max Autopilot which could sometimes fly a plane, but didn’t know it was flying a plane — and with passengers — so would sometimes do un-aeronautical things — like trying to fix a stall by diving, but at too low an altitude, etc.

Something to ponder is that some of the prevailing ways of thinking in business are trying to use “ANI”s for convenience and to save overheads (the “N” stands for “Non”). So they are already destablizing what have already become many not-stable-enough systems. People are starting to use “ANI”s to generate “plausible bullshit” in the manner satirized in Jonathan Swift’s Gulliver’s Travels where the philosophers on their floating island have a machine that generates random sentences which are searched for “ones that might mean something”.

This is hitting the nail on the head from just about 300 years ago (1726). How could Swift do this so well so long ago? Because he knew just how ignorant, stupid, greedy, and gullible most humans are and always have been. This means that satire in almost every age will ring true to later ages.

Philosophically, “Real AI” should be possible. It is difficult to make any power safe enough to be used, and — so far — it has been impossible to educate humans well enough to use any of their powers wisely enough.

Vi Hart has a great line for this: “We must ensure human wisdom exceeds human power”.

This seems like an excellent answer when someone asks “what should I work on?”



How does Alan Kay view the use of artificial intelligence to guide diagnosis and treatment in clinical medicine? I've been invited to write a "Con" review re: AI in hypertension (someone else to provide "Pro") & I wish to learn Alan Kay's thoughts.
*Actual* “artificial intelligence” could be of great use. This term today has been watered down to mean something quite different than when it was coined more than 60 years ago.

The central issue here is how to vet the processes from phenomena to causal conclusions.

For example, I have a bad kind of cancer (musosal melanoma stage 4) which is being held at bay by immunotherapy after an operation. I have tri-monthly full body PET scans and head MRIs that watch for indirect activity that might indicate that tumors are trying to form. I have heard that the minimal resolution is about a square millimeter (which might be about 100,000 cells) and this little white spot could be missed, even by expert humans.

The current “ML” techniques — when they work — are essentially “curve fitters” and transformations from thousands of parameters into a few categories. This should be very useful in medical scans, etc. But “correlation doesn’t mean causality”.

The Large Language Models are mostly syntactic and there is little in the techniques to actually create trustworthy models of the world — especially at the level needed in medicine.

Because we humans have many cognitive biases — a dangerous one in medicine is “confirmation bias” — it seems to me that using an LLM to produce what looks to be a diagnosis will be a very bad idea with the current state of the art (and this includes cases where the diagnosis is accurate (it’s not accurate for the reasons medicine needs)).

A better approach would be to make hybrids of MLs for “noticing” (Kahneman’s “System 1”) and actual *cognitive models* of medical knowledge to assess what is being noticed (Kahneman’s System 2 for “Slow real thinking”). This would allow important things to be noticed and then critiqued (without “hallucinations”) for a human doctor to then peruse.

I also have hypertension and only know enough to realize how complex the actual physical systems and measurements on them are for any useful model to be made on a patient. For example, some interaction with my cancer drugs and my long term use of hypertension meds cause precipitous drops in the BP, and I not only would faint every once in a while, but my left leg would “go away” for a while. While this was going on, there was no guess that the problem was my BP dropping until I fainted while in a hospital and they found how low it was.

Doctors will appreciate that we still don’t know why it happened, and what finally worked was to just cut my former BP medicine in half. Many such mysteries still exist in the complex feedback mechanisms in e.g. the endocrine and immune systems. (We need much better models!)

Right now, the responsible response of medicine to what is called “AI” today, should always strive for safety, reliability, stability, scalability, explainability, etc. before using it as a shortcut (which will often be dangerous and against the spirit of the Hippocratic Oath).

To everyone worrying about my cancer — please don’t. I’m in my middle 80s, and was expecting that this could be an “eventful decade”. Before 2013 it most likely would have killed me in months, but the introduction of “immunotherapy” has made a difference for a percentage of those with this type of cancer. There can be side-effects from the operation and the boosting of one’s immune system (and I’ve gotten a few of them). I think many diseases are tragedies when children get them, but are part and parcel of old age, and its inevitable end-game. I am very thankful that I did not get either dementia or arthritis — and so can still play music — compared to these, the cancer is much to be preferred).



Was there ever a serious attempt to a "multiple inheritance" Smalltalk cc: Alan Kay, Dan Ingalls? If there was, why didn't it work out?
Bob Taylor — who set up Parc computing research — was one of the directors of ARPA-IPTO (and, when there, among many of his good deeds were to heavily support Doug Engelbart, Dave Evans, and to get the ARPAnet started). As I’ve written elsewhere, IPTO had “visions, not goals” and “funded people, not projects”. So if anyone asked Bob what they should do, Bob would just say “Follow your instincts”.

My instincts were about most aspects of personal computing and systems, and they were especially magnified after meeting Seymour Papert in 1968 and seeing what he was doing with children and why LOGO had been developed to help. This led to the idea of a future “Dynabook” for “children of all ages” that would be “personal computing” in the form of a tablet and stylus computer of the future.

Influenced by Sketchpad and Simula I in 1966, I got very interested in the notion that “every SW idea could be represented as a network of intercommunicating virtual computers”, and — after meeting Papert — that “computing is more like a medium than a tool” — and if so — like reading and writing — ways must be found to not only include children, but to start early with children (as R & W does).

This led to many early goals. For example, programming should not just be for making things, but to make *beautiful things*, and learning to program in a good children’s language would mean leaning many important ideas about larger things: such as processes, *meaning*, systems, *thinking*, *math and science*, *meta things*, etc.

For example, a neat goal would be to invent a language in which “simple things were simple, and complex things were possible”. This was already the case with LOGO (which combined semantics from LISP, and some of its appearence from JOSS). I could see that the “everything is a virtual computer” idea could be “LOGOfied” and this could lead to a language that could (a) work for children, (b) all of it down to the metal could be written in itself (c) the kids could explore every level safely (d) the structure of the system itself could be illumimating … Etc.

By far the best and most interesting examples of these larger things intertwined with computing were to be found in the ARPA research community, and especially in its “real AI” research community.

One of the earliest big ideas in computing took quite a while to appear in a full-fledged form — this was *general instantiation” (this is curious, because the desire to use things over again and to make similar things came early).

Some of the earliest uses of this idea in a strong way happened with the 1962 Sketchpad and the first Simula (ca 1964) — and we should not forget COBOL and its need to deal with record formats and instances of them. However, Algol 60 and most other languages didn’t have it, and the languages that could express it — like LISP — “didn’t know they could”. Interestingly, the advent of time-shared multiprocessing used instantiation to make multiple virtual machine processes at the operating system level.

Nygaard and Dahl, the inventors of Simula I (which appeared in the US in 1966), realized that they could get parallel virtual machines if they broke the Algol block mechanism loose from the stack discipline storage allocation scheme. This was a huge leap, and provided a way to think about programming such structures that the magical Sketchpad had shown a few years earlier. The result was a kind of hybrid, but mathematically minded observers could readily see that everything in Algol could be replaced and defined by this single idea — and this would simplify everything greatly (except that a lot of new software engineering would need to be done to make this practical).

My earliest thoughts about a children’s language along these lines happened after the Dynabook idea and were very much catalyzed by (a) how LISP was set up, (b) how LOGO had been derived from LISP, and (c) what was going on in *AI* with regard to modeling situations and reasoning about them. I called the first designs “SLOGO” (for “Simulation LOGO”, and because I had a strong feeling that the first versions would really be “s l o w”).

Now let’s fast forward a few years to Xerox Parc, and a chance “hallway meeting” with Dan Ingalls and Ted Kaehler, and a discussion about “the most powerful language in the world”. Because I had studied LISP and was aware of McCarthy’s half page (page 13 in the LISP 1.5 manual) definition, and had realized that you could get full objects by looking at what McCarthy did a bit “sideways”, I asserted that you could get the “powerful language” in about the same amount of definition. This led to about two weeks of work to come up with a similar definition that was even simpler and included the new ideas. A month later I came back from a trip and found that Dan had implemented the definition, and we now had a language system we could program in, and extend. This was the first working version of Smalltalk.

Working with Dan has been one of the great pleasures of my life (and we are still very close friends and colleagues). Dan has been a genius at many things, but to pick just one standout ability: he just naturally converges on making working extensible systems that others can use (these are far more than demos, and they have led to many great and useful ideas).

I loved instantiation, but did not like way Simula 67 did inheritance. For example, there was no protection of “meaning of species” when one subclassed. For another, the subclassing could isolate a trait that seemed as it if should be more of a parameter than a subclass. And so forth. Multiple inheritance could fix a few of these, but with more problems. For example, what if each inheritance chain had a method with the same name (e.g. “print”) what meaning should the system try to come up with if “print” was sent to such an object? Maybe sequential application? But there were many cases where collision of methods were much more non-linear and confusing. (This was ugly, and certainly nothing to put in front of a child.)

However, the “LISPiness” of the first Smalltalk, allowed many kinds of late binding to be done, and that allowed us to do many experiments with various forms of “inheritance”, including: delegation, “slot inheritance”, “exemplars” instead of classes, etc.

Here is another Quora answer of mine about interitance:

What does Alan Kay think about inheritance in object-oriented programming?
https://www.quora.com/What-does-Alan-Kay-think-about-inheritance-in-object-oriented-programming
One thing we realized is that “the cleverness you can come up with whilst programming, is rarely matched by the understanding of people looking at the code afterwards” — so “if something can be done, it doesn’t mean it should be done”. This especially applies to *meta*, even if *meta* is available.

It was amazing that Xerox funded the many Altos, but they balked at funding a next gen machine just a few years later. This meant that researchers like us had to now optimize research code in order to make progress. “Once you put on your optimization hat, it is hard to think research thoughts”.

This led to a crisis, and once again Dan saved us by making a kind of Smalltalk (Smalltalk-76) that pinned more things down early, but preserved much of the original flavor. This was much faster and larger, but more care had to be taken with experiments. One decision was to try “single inheritance with lots of design”. As done by Dan, this was a bit of a miracle (which he had to reconstitue every few years from the mess that the rest of us started to create).

One of the things that turned a possible disaster into a success was that the larger goals of the group were not primarily about programming language design, but about *personal computing for novices who could grow”. This meant that most of the experiments we did — successful or not — were not incorporated into whatever was the Smalltalk of the day.

My bottom line on all of this is that inheritance is a good idea if there is a carefully worked out set of guards and principles — a kind of “algebra”/”constitution” — to preserve meanings, deal with non-linearities, etc. In other words, something that could lead to a comprehensive and powerful enough and compact enough ontology that would make things more clear (as opposed to just “more powerful”).

A final note here: it’s not just making a powerful system that should be the main goal, it’s “making a reformulatable system that should be the main goal”. In other words, if things are going well, there will be a learning curve that one will want to incorporate gracefully in the system. In a new discipline with a learning curve “Power is fleeting, change is required”.



Now that machines can generate and "understand" language, what would Alan Kay argue that are a set of first principles from which to rethink the human-machine interface?
One of the ways to spot many LLMs is to note that they often don’t handle “gender anaphora” well (meaning that a sentence about a man will be followed by a sentence also about that subject but using “she” instead of “he”). This is really ugly and bad (especially considering that this was done much better before ML).

I don’t see any evidence of actual “understanding” either of language, and especially of the world and situation that we live in.

Here’s one way to think about “What is actually needed?” (WIAN). Consider actual humans trying to live together.

They are somewhat clever, can use language a little, are not terribly knowledgeable, think their beliefs are actually “reality”, etc.

I don’t think a society with just “traditional/genetic traits” is going to work very well without something like a “constitution” (whose first version might be simply an agreed on set of customs for the societal culture). These customs will include “rights and wrongs”, ways to negotiate, etc.

In other words, once you have a system, it is really beneficial to set up tests and feedback loops to deal with safety, stability, scaling, etc. Soon there is a realization that “laws” are not nearly enough — what is needed is a “constitution” which is a philosophical framework laying out the desired norms of the system, ways to deal with instabilities, and how specific laws can be derived from the constitution. I.e. the constitution is a kind of *language and operating system kernel* to be a basis for many things in the present and new challenges in the future.

As an old fogey, what I think I’m seeing today is *cleverness without enough context* in the age of the industrial revolution.

What I want in a computing system I can work with, is something that is much more trustworthy and understandable than most humans and societal systems. I certainly don’t want to work with something that is extremely dangerously *below* expectations and wishes even for humans.

I think that too many people are hooked on “doing something/anything” rather than taking the time and resources to find what is actually needed to do, and then to do it “well and carefully”.



How do I write a program to evaluate the arithmetic statement: 𝑥 = (𝐴 + 𝐵∗ 𝐶) / (𝐷 - 𝐸 ∗ 𝐹 + 𝐺 ∗ 𝐻) a. Using a general register computer with three address instructions. b. Using a general register computer with two address instructions?
This sounds like a homework problem. Should you really be asking such questions on Quora?



What was the first PC, Kenbak that's according to AC M and Steve Wozniac or LINK, according to Alan C. Kay?
First, we have to decide on the term “personal computer”. I have defined it as something one can easily have in their house, made in some quantity, that has at least textual I/O, and real programs can be written on it. A really early one would be the Bendix G-15 in the 50s, done by the even earlier computer pioneer Harry Husky. Thanks to Peter David Hill for reminding me of the Librascope/Royal McBee LGP-30 (1956), and to Scott Carter for pointing out that *Librascope* was the main factor in the creation of this machine (see comments). This was also the computer that Margaret Hamilton first used to learn programming. If we want to add a real display, paged swappable memory, file storage, and quatities of 1,000 to 2,000, then there is Wes Clark’s LINC in 1962. Mary Allen Wilkes and her home LINC (she designed and programmed the OS) I think of the LINC as the first real “personal computer”. Here’s another “earlier than Kenbak” personal computer: the Datapoint 2200, first sold 1970. It was actually a “programmable terminal”, that was strong enough to be used stand-alone. If we remove the necessity for interaction (which I consider critical to the definition), then we can look at just “computers you could have at home”. One of the very first in this limited capacity would be Ed Berkeley’s “SIMON” from the early 50s (the magazine below is dated Oct 1950): One of the first to write real programs for this was Ivan Sutherland when he was about 11 or 12. Note that this exhibit is (a) from the Computer History Museum, and (b) is earlier than the Kenbak (actually by quite a few years) — I helped build one around 1954. So I don’t understand why the question above claims that the ACM thinks that the Kenbak is earlier (this feels made up to me). In any case, both the G-15, the LGP-30, and the LINC very much predate the Kenbak and they both are much more the real thing IMO.



How can anyone take math and physics seriously enough to devote a career to them, (overwhelming evidence that the material universe obeys mathematics) without positing that abstract entities have an independent existence outside the human mind?
Many of the answers to related questions are worth reading and pondering. For example, see what James Fullwood has to say. Here is what Einstein had to say early in a talk to physicists in a lecture at the Prussian Academy of Sciences in 1921. First, he noted that: “But there is another reason for the high repute of mathematics: it is mathematics that offers the exact natural sciences a certain measure of security which, without mathematics, they could not attain.” But, then he reminds them of one of the most important epistemological ideas/principles of science: “As far as the laws of mathematics refer to reality, they are not certain; and as far as they are certain, they do not refer to reality.” There are many important ways to contemplate this. One is the analogy — even overlap — of maths to maps — and that “the map is not the territory” [1] — but a really good map can be of critical help when exploring and explaining a territory. Another — larger — notion to ponder is that pure maths can be made “certain” because they are creations of the human mind in which every part — including representations and inference — is grounded in given definitions. There is no outside world. This is part of what goes on “between the ears” of humans. In other words, we can make maps of imaginary places that obey all the logical rules of maps of actual places e.g. Tolkein’s “Middle Earth” map obeys map inference operations such as “if the SHIRE is north of FANGORN, and ERED LUIN is north of the SHIRE, then FANGORN is south of ERED LUIN” etc. We have never found anything on our planet that resembles this map, but with the map alone, we can’t say whether the map has a referent or not. (It seems not.) This is why the Greeks (and Thomas Aquinas, etc.) got in trouble by thinking that “perfect reasoning” was enough to explain our situation. (We can only make up “perfect stories”, with regard to consistency, etc.) We do real science by trying to find relationships — especially causal ones — from limited and noisy phenomena. Much of mathematics started in attempts to symbolically make models that tried to mirror phenomena, and especially to be good enough to predict some phenomena, and — often — to find interesting differences between the predictions and the phenomena. We should be able to see that we are now “in a kind of negotiation” between “what’s out there?” and the kinds of worlds we can build “between our ears”. This is very different from mathematics all by itself (because “proving something” in the math of science, means very little without seeing if the phenomenological world seems to match up). This is one way to look at the meaning of Einstein’s second quote. I once had a nice long chat with Steven Weinberg about this and that. One part touched on his book “Dreams of a Final Theory: The Search for the Fundamental Laws of Nature”. I pointed out that one of many curiosities of this book was that in one chapter he would say that “the math is not the universe (the physics)”, and in the next chapter would act as though “the math is the universe”. He said, “Yes, this is a huge problem for most particle/quantum physicists: we don’t have anything but the math to think with, and because our brains want to believe things, we start believing the math”. (He liked Einstein’s second — and wonderfully explanatory and cautionary — quote as a way to get recentered on what we are actually doing when we are trying to do science with the help of math.) P.S. There is no “overwhelming evidence that the material universe obeys math”. It is actually quite the opposite. The universe is whatever it is, and we make up language and notions to try to create models that can mimic some of it. It’s quite wonderful that we can do some of it, and this is why some people devote their life (it’s a lot more than a “career”) to science. [1] Alfred Korzybski “Science and Sanity”



What question(s) might Alan Kay like to answer that he has not yet been asked (publicly)?
Thanks for asking, but if it worked that way, I would be continuously writing essays, etc.

My odd motivations include the desire/urge to answer questions *plus* the fun (and necessity) of getting questions from “outside”. Quora supplies these, along with the need for brief writing (which I’m trying to improve).



How did ARPA/IPTO & Xerox PARC decide who worked on what? Were there conflicts over who got to do what?
An excellent and interesting question.

The computer research part of Parc (two labs out of four) mostly were staffed from the ARPA-IPTO community and considered themselves part of it — and, most especially, considered that they were there to carry out the “ARPA Dream”, which was initially formulated by JCR Licklider (“Lick”) as (my phrasing):

1. Computers are destined to become interactive intellectual amplifiers for everyone universally networked worldwide

Here are more ARPA-IPTO principles of the 60s (my phrasings):

2. Visions not goals — “Visions: Cosmic and Romantic”

3. Fund people not projects

4. Only fund the very best people

5. Fund Problem Finding — not just Problem Solving

6. No peer review

7. An important part of the research results are new and better researchers.

8. Really Advance Something Very Important

9. Design and build in the future and bring back to the present

10. “Living Lab” – make enough of the inventions so that many can use them.

11. Milestones not deadlines

12. It’s “baseball, not golf”

… and so forth.

The tricky part to describe here is that the researchers themselves decided on what to work on. This meant that how the researchers were chosen was the most critical process. As Patrick Scaglia of HP Labs (another great research manager) liked to say “Hiring is 110%!” (This is the “fund people, not projects” in item 3 above.)

One way to think about this is that “Artists are people who can’t not do Art”. They tried to choose these kinds of people, who wanted to do their Art in the context of the Vision, and “who looked like they could do something”.

Bob Taylor — who set up the computing research at Parc — had been the 3rd director of ARPA-IPTO and had codified Lick’s intuitive principles. He never chose a goal, but as he said “My job is to create an environment such that when my lone wolves need to cooperate, they will”. When asked his job description he said “I’m a kind of impressario”.

This meant there was no party line for project goals, and this allowed more than one kind of project to address parts of the vision. So there were multiple language projects, multiple display and UI projects, etc.

Why was there almost no rivalry?

Basically a combination of mutual respect, no competing for funding, etc. At Parc Taylor decided to eliminate rivalries from the start by requiring that *all* the researchers be *really enthusiastic* about the next hire. I.e. a single negative vote and a prospective researcher was not hired. This made the hiring process slow and laborious, but the new person was welcomed with open arms by all — and this made a tremendous difference.

The only main rule at Parc was that everyone had to attend the weekly meeting — called “Dealer” — where Chinese food was consumed, all things were discussed, talks were given, pledges were made and unmade, argumentation was encouraged (not to win, but to *illuminate* — more below). These meetings were on Wednesdays, started at noon, and went on until all outstanding issues were taken up, etc. Excellent minutes were taken by volunteer “scribes”. Etc.

Another fun notion came about because Xerox allocated people not by funding but by “headcount” (called “slots”). This meant that when a precious “slot” appeared, the tendency was to recruit another star. And this meant that we were always “tech support light”.

This was discussed quite a bit — at some point it was realized that *we* all had been “tech support” earlier in our careers and still liked many parts of it. This led to the idea that Parc researchers could spend up to about 40% of their time “being hands” for other projects, as recreation. The nice thing about this was that the change from being “creative” to being “support” was very restful. For example, I really liked to solder, so I could help with soldering needs on other projects part of each day. One could make a “public pledge to help” in a Dealer, and at some point, one would “unpledge” when one felt there was no extra time available.

ARPA-IPTO liked to “argue for illumination”, and was very argumentative. The level of this at Parc shocked visiting Xerox executives. Taylor had refined this to handle situations when the arguers had forgotten about “illumination” and started to try to win. He would call out “Type 2!” This meant that now each arguer had to explain the other person’s point until the other person agreed, and vice versa. By the time this was done, the heat of the moment had subsided, and clarity was again the goal.

By my reckoning, ARPA-IPTO and especially Parc, had many of the absolute top minds in computing. The process made it not just easy to cooperate, but also created enormous synergies. This included for example, people with different notions of what a new kind of programming language should be like actually helping each other to make their respective notions work.

Resource limitations could be a source of conflict — but Taylor kept Parc small so that he could get super-researchers within the budget. This worked extremely well (socially), even when Xerox would crimp the budget when they went through difficulties. Towards the end of the first 10 years, this gradually deteriorated.

After ARPA became DARPA ca 1970, it no longer operated as it had (and this was one of the reasons Parc came into existence). After Parc (ca 1982) there were no large enlightened funders left. The individual researchers carried on in various places, but the large “communal” critical mass never happened again. The way I think about it is that though I still had a research group of top people, I could no longer walk down the hall to get Butler Lampson to tell me why my “idea X” wouldn’t work, and then start thinking about why it *really would work* (Butler was great at both arguing and being honest when the argument changed.)

One fanciful way to think of the change is that the funders no longer wanted artists to create beautiful murals, now they just wanted painters to paint the walls a certain color.

This leads to *Principle 0*:

0. The goodness of the results correlates most strongly with the goodness of the funders



How do you think the rise of AI personal computers will impact the PC industry?
Anthropology was one of my minors in college, and the head of the department had a Macaque monkey as a pet, and would occasionally bring it to class. When we asked him about his experience at home, he said “It’s like having a dozen cats with hands”.

The current technologies seem terrible to me, and I don’t think they are generally helping except where they can be closely vetted: to e.g. help find white patches on an Xray that shouldn’t be there.

This is a little bit like an industrial revolution version of mass produced Trumps, but able to traverse the world via the Internet.

I advise reading the psychologist Gary Marcus, who has been commenting and chronicling the myriad failings and dangers.

https://garymarcus.substack.com/



To what extent did The Inner Game of Tennis shift PARC’s point of view?
It had a huge influence on my point of view — I saw the Harry Reasoner show with Tim on it and read the book ca 1975 — but I don’t think that it affected anyone else at Parc while I was there.

However, a few more words are in order. TIGOT indirectly affected some of the most important work at Parc, especially in the UI and Learning areas. One of Tim Gallwey’s nicest observations — “The problem with most learning is that the parts of the brain/mind that have to do the learning, don’t understand English!” — resonated very strongly — and hooked up nicely with Jerome Bruner’s ideas about “multiple ways of knowing” and “multiple minds” between our ears. This led to the “multi-modal” GUI and the slogan “Doing with Images makes Symbols”.

Another idea we really liked was Tim’s tactic of finding an activity for the learner that would simultaneously distract the non-learning part of the mind that was trying to control things while drawing more focus and attention to what the learning parts of the mind needed. I believe that we could have done a lot more with this idea than we did.

The question says “PARC’s point of view”, and I don’t think this was shifted, but it was certainly the case that Tim’s work greatly influenced both me and my research group (the Learning Research Group).

I should also mention that Tim Gallwey and TIGOT were huge influences on the research groups I set up after Parc, especially for Apple and later at Disney. I hired Tim as a consultant and advisor, and his ideas were important parts of the larger worldview of this research.



Is there any hope of a hardware company making something like bartons computer hardware now?
The central idea and issue here is that exponentially massive scaling changes the design issues and design rules every few years.

Bob Barton and his colleagues at Burroughs responded to the first really high level “tidy” languages — e.g. Algol-58, and then Algol-60 — with a “systems response” to their challenges, in which part of the result was a new environment and meeting place for hardware and software. This was amazing for its time (1960–62), and still enlightening and thought provoking today.

However, by the mid to late 60s, it was possible to use newer knowledge to come up with both better notions of programming languages *and* better notions of where languages and hardware could now meet. For example, dynamically loadable microcode into a faster than RAM buffer was possible in the late 60s, and allowed better designs all around (Parc took advantage of both what Barton did, and the newer components, to come up with stronger combinations, etc.)

And Bob himself helped invent — secretively — such architectures for Burroughs during this time (we didn’t know exactly what he was working on, but it turned out to be what he later called a “configurable” computer (the “1000 series” of designs)). Burroughs tried to do a lot of things with this approach (and it’s interesting to contrast it with the much simpler/parsimonious work of Chuck Thacker at Parc on the Alto, and later machines).

The exponential nature of Moore’s Law makes each HW “era” just a few years for each particular opportunity for getting substantial leverage from “brilliant design”.

One of the “rubs” here is that there are large learning curves required, and historically, only a fraction of researchers did the work (and wanted to do the work) required to “re-wire” their instincts and points of view. Part of the lure to this small percentage is the analogies to “truth and beauty”. The main goals of “industry” are quite different, and are aimed at “pools of customers” (and also “pools of workers”) — and how to deal with “legacies”.

There have been 60 years of Moore’s Law since the early 60s, and — conservatively — this means about a factor of 40 doublings (20 doublings is about a factor of a million), distributed over increases of computing power, capacities, and lower price. The universal nature of the microcomponents means that very conservative next level designs will still speed up and cost less. In practice, this has meant an almost freeze in actual progress in programming language designs, etc., over the last 40 years.

So the questions I would ask first would mainly have to do with “how should we be programming today?”, and “what should computer systems look like today?”.

By the way, the big deal with Bob Barton, was that he was always asking these questions, and he was always extremely dissatisfied with *everything* in computing, including his own brilliant work (which he could clearly see was “mainly good for its brief time”, and which needed to be superseded often). Most of us at Xerox Parc felt quite the same about what we accomplished in the 70s.

P.S.1. It’s worth noting the lag between “serious graphics” and GPUs. We can also note the appearance of special HW for doing low res vector/tensor processing for ML (and here we could “ask questions about the worth of the questions …”.

P.S.2. Bob was a mathematician and “systems guy”, so his instincts were aimed at places in desired HW-SW systems where key needs were creating bottlenecks (and even neglect), and where “math” (relationship insights) could be rendered in HW to make a big difference. I urge interested readers to really take a look at the “Barton architecture” (he gives Ben Dent credit for helping with the ideas) that was used in the B5000 — and to especially look beyond the use of a stack, postfix code, etc. to see how protection, processes, and virtual memory were done. His classic very short paper (4 pages of “math”) from 1961 is cryptic, but is worth grokking (alongside a more conventional account of the B5000).



Do you believe it is ever a good idea to use a GOTO in a C/C++ program, why?
First, check out “Zahn’s Device” (Wikipedia calls this “Zahn’s Construct”). This will take you back to this ancient controversy, and some interesting work arounds.

I started as a machine code programmer in the early 60s, and gotos (JMPs) were very much used.

It’s worth looking at the voluminous machine code listings of the Apollo Guidance Computer, and reflect that they *never* had an actual software bug in the history of the moon shots (the famous glitch on the moon landing was a procedure error by an astronaut, and the software actually was able to fail-safe the error in real-time).

The issues about whether to use them in higher level languages that had more structured ways to change flows of control, revolved not around the goto itself, but about whether at any given point in a program a programmer could have an accurate picture of the current state of the variables. Most gotos were written so as not to create mysteries at the other end of things — in other words, the goto was not so much of the problem as the “where from” questions in the midst of code.

The many kinds of race conditions often led to really careful programmers not assigning to the same variable more than once in a loop, etc.

The biggest problem with “taking care” and then letting time pass, is that the exact reasons why something is OK get forgotten, and changes often “don’t take the same care”.

Then there is the observation of Tony Hoare (Turing Laureate) that “Debugging is harder than programming, so don’t use up all your cleverness programming”.

When programming is looked at longitudinally, it looks (and is) often really bad. One reason for high level languages is to deal with “the programmer problem” by walling off dangerous stuff wherever possible (this will include not just control, but storage management, etc.). This should nowadays exclude most sequential “munge-variable” programming, and dangerous control constructs such as “goto”.



When was the first computer invented? Who invented the Internet?
I’ve answered the first question a few times on Quora. By just saying “computer”, we can look far back in the past to find mechanisms invented by humans to help them reason and calculate. Two very old ones (B.C.) are the abacus — not just in China, but in Greece and Rome — and the amazing Antikythera mechanism which seems to be a very sophisticated calculator for the apparent movements of the sun, moon, and planets. If we were to look for “programmable computer”, and give credit for early ideas, we have to mention Babbage’s “Analytical Engine” which was intended to be programmed using the techniques of the Jacquard Loom which had sequences of punched cards to program patterns woven by looms. If we include credit for actually making a device that worked, then we could pick John Atanasoff and Cliff Berry (1939) or Eckert and Mauchly with ENIAC (ca 1945). Personally, I don’t think either one of these machines is programmable enough. For “semi-programmable” computers, I prefer Colossus by Max Newman (and especially) Tommy Flowers at Bletchley Park. However, my pick is Konrad Zuse who — working by himself — made several programmable digital computers (the Z3 in 1941) and even invented “the earliest” higher level programming language — “Plankalkül” — The Computer History Museum Fellow citation sums it up: "for his invention of the first program-controlled, electromechanical, digital computer and the first high-level programming language, Plankalkül." The Internet — as with so many of our modern computer technologies — was done in the context of a research community — funded by ARPA, with Parc funded by Xerox. The way these highly productive communities work is that ideas are freely shared, there is a lot of process, designs, testing, etc. and there are usually a few “special sparkplugs” amongst the general high talents that make a difference. Awards often miss some who should be included, so we should look further than e.g. the NAE Draper Prizes for the Internet and for Personal Computing. The Draper Prize for the Internet was awarded to Vint Cerf, Bob Kahn, Len Kleinrock, and Larry Roberts. To this, we could add names such as Paul Baran, Dave Davies, some of the ARPAnet pioneers, and several of the Parc networking researchers who also contributed to the Internet (e.g. Bob Metcalfe, Butler Lampson, etc.). My favorite “special sparkplug” from this esteemed group is Vint Cerf, who both (a) was the co-creator of the TCP/IP protocols, and (b) ran what was perhaps the most productive of the group efforts to nail things down into workable forms.



Isn't artificial intelligence just a fancy name for sophisticated programming running in fast enough computers?
This is a pretty good question — especially because you are trying to separate labels from actual meanings.

I think homing in on a good definition of “intelligence” will help you answer your question for yourself — especially if you can come up with a good characterization of a goal for “artificial intelligence”.

I was once in a meeting in the 60s that included heavy duty military officers and a few AI researchers (I was neither: just “a systems guy”). One of the points one of the generals made was that they really had very little use for a computer simulation of normal human intelligence (because they could draft and train soldiers for these needs and tasks). What they really wanted — he said — was “superhuman intelligence”.

Another point of view is that we might be able to use the term “intelligence” when a machine can be made to do something that a human would need intelligence for. A very simple example used long ago by John McCarthy (who coined the term AI) was that of a house thermostat. It could sense the temperature and take actions to turn on or off the heat to try to keep the ambient temperature within a certain range.

I don’t think that is enough. If a machine can beat a human at chess, is it intelligent? I would argue that this is like comparing a race by a human and an automobile (it doesn’t make sense because it is a category error).

The Boeing 737 Max has been in the news frequently over the last few years — most famously for having an autopilot that would e.g. trying to correct for a stall condition by diving the plane to get more air flowing over the wings (pilots know this is a good general idea — but pilots also know (a) this is a terrible idea if the plane is only a few hundred feet off the ground, and (b) there are human beings on board who will be killed thereby). In this case the autopilot is quite stupid enough to be cheated at cards by the thermostat! Boeing allowed this to happen twice (meaning their executives are all too close in IQ and ethics to the autopilot).

Years ago I suggested that the term AI be replaced by “flexible competence” — this has the advantage that each part of the term has more meaning and can be measured in more meaningful ways. Note, that could even apply this to engineering artifacts like drawbridges: they are competent in two distinct ways, and employ flexibility to accomplish this (they could be the new “thermostat” in a new metaphor).

I think the bottom line for any kind of engineering can be taken from the Hippocratic Oath in Medicine (itself a kind of engineering science): “Above all do no harm”. The bridge must not collapse, the plane must not crash, the building must not fall, computer programs must be safe, and be able to prove they are safe, etc.

I think the previous paragraph is the best answer I can come up with for your question.



What was the best computer system you worked on (or with) and why? For "best" and "computer system" pick your own definitions. 🙂
Thank you Mark!

For me, questions like this are in the same category as “What is the best book you have read?”, “What is the best food you have eaten?”, “What is the best musical instrument (or even “best guitar” or “best pipe organ”) you have played?”, or “best performance you’ve been involved in?”, and so forth.

I will try an elliptical answer first, and then see if the same logic can be applied to computing experiences.

(Leaving out a lot of backstory detail here) long ago, there was to be a production of “The Fantasticks” when I was in USAF in San Antonio, and they couldn’t find a harpist (after needing a concert pianist — which they did have — it is the next key instrument in the score). I volunteered to take a shot at it (and the wonderful pianist thought it was both funny and game, and helped). Our show ran for 56 performances (and closed only because the theatre had already booked another show).

As we entered the last week of this, there were literally tears all around — cast and musicians — from the thought that in a few days we would not be able to continue. This can be hard to explain, but it is well known in many art forms: it’s a combination of basic material being so good that the more you contact it the more it amazes and gives back — and — that each encounter is different: the audiences are different, and the performers are being changed by the experience and life in general, so each show is different (and also it is often the case that memories don’t just come up with *one* performance that stood out, but *many* — because *real art* has so many many aspects to give and to love).

Similarly, one’s memory of general performances doesn’t just single out one material — i.e. “The Fantasticks” is a kind of theatrical miracle, but it is far from the only theatrical miracle.

This obtains for books, musicians and composers, etc. I think it’s possible to invent a category named “wonderful”, but for me it is always populated by many examples. (So: Bach and a Schnitger type organ, Franck and a Cavaille-Coll type organ, *and* all the way to e.g. Mark Knopfler and his special musical vision via a special way to play a Strat — “wonderful” is a *big* idea!).

One final preliminary here concerns the human beings who are the central movers for art — it is hard to describe them other than “they are people who can’t not do art”. This makes this group very interesting, memorable, and (often) confounding.

Now let’s look at computing, and the questions about “best” and “computing system” and “why?”.

To me “computing system” is in the category that includes paper, pencils, pens, charcoal, watercolor, canvas, oils, and so forth. It is something that can be shaped into a manifestation of an idea. Note that the ideas in art almost always transcend the materials, and part of the artists’ processes are to find a manifestation of the idea/impulse in the materials at hand. I think most readers can see that rather often it is the limitations of the materials that really help the artist to shape.

So: too many “wonderful things” over the years to be able to mention.

To me, the big deal and reward is to be able to “do the work” by being able to follow one’s nose towards the “perfume”. There is nothing like it. “The center of ‘why?’ is Art”

For me, this was most fruitful in settings that were rather like theatre communities that had just enough funding to create what was needed and sought after. I’ve mentioned the ARPA/Parc research communities as a wonderful and long rich period. But, with the right people as colleagues, lots also got done in less felicitous environments. “Artists have to do art” etc.

I particularly enjoyed the aftermath of the “Disney Etoy Period” when what we had created for 8–10 year old children — and worked so well for them — was also comprehensive enough to do my talks to adults and show them examples that the kids could do that were greeted by both gasps and applause (for not being easy or even a reasonable goal in the “professional systems”). This system was possible via one of the many great teams I’ve worked with.

That was one of many “Fantasticks” level eras for me.



Which personal computer was the first to use an operating system not based on Unix?
As I define “Personal Computer” Wes Clarke’s LINC from the early 60s qualifies as one of the first (see “The LINC Was Early And Small” by Wes in ACM’s “History of Personal Workstations”).

It had a display, a keyboard, virtual memory, etc. — and an interactive OS — which was called: LAP6.

One of the central figures in most aspects of this remarkable tiny machine — especially the software system — was Mary Allen Wilkes, a truly amazing computerist and programmer — “smarter than anybody” — and perhaps the first to have a personal computer in her home).

Here is her abstract of this system for her 1970 CACM paper on this OS.

LAP6 is an on-line system running on a 2048-word LINC which provides full facilities for text editing, automatic filing and file maintenance, and program preparation and assembly. It focuses on the preparation and editing of continuously displayed 23,040-character text strings (manuscripts) which can be positioned anywhere by the user and edited by simply adding and deleting lines as though working directly on an elastic scroll. Other features are available through a uniform command set which itself can be augmented by the user.

The machine, although small, aids program design by providing display scope and premarked randomly addressable LINC tapes as standard items, in an environment similar to that of a sophisticated terminal. The tapes are logically similar to a disk. Priority was given to the design of efficient tape algorithms to minimize the limitations of the small memory. Techniques developed for handling scroll editing, filing, and the layered system structure are outlined.

LAP6 is used by about 2000 people in 11 countries. Its design was strongly influenced by performance criteria established in interviews held with the LINC users themselves during the specification period.

Also, the Xerox PARC Alto was a personal computer, and predated 8-bit micros and most of Unix — and had a number of OSs, including with GUIs.

Numerous 8-bit micros also had OSs long before Unix.



What are some good biographies that every aspiring programmer should read?
Sadly, as with histories of our field/profession, most are not very good.

One I liked a lot — and I knew the author over many years as well — was “Computing in the Middle Ages” by Severo Ornstein, who was one of the main people on the LINC (the first personal computer), the ARPAnet at BBN, the laser printers and the Dorado at Parc. He is still kicking at age 93, thank goodness!

The best (only really good one) history of ARPA-IPTO and Parc is “The Dream Machine” by Mitchell Waldrop. We all thought it was amazingly accurate in so many respects.

The ACM’s Histories (Workstations, Programming Languages, etc. are pretty good).

“A Mind At Play” is about Claude Shannon.

Good histories of machines include:

“The Whirlwind Project” — “The Apollo Guidance Computer” — SAGE Q7 (not the actual title).

There are a number of good histories about “how to pull off new big things”. For example:

There are several good histories of Radar, especially the one about Tizard, and another about Loomis (“Tuxedo Park”).

General Groves wrote a wonderful matter of fact book about the Manhattan Project “Now It Can Be Told”.

There are several great histories about building the Empire State Building.

Mark Nobles asked in a comment below: “When will you finish your autobiography?”

My reply was:

I haven’t started one, and am still actually living a life of *doing things I love* … so I have a feeling I will never write one.

When Quora asked for a one-word “tombstone” description, I replied “Curious”.

When conferences ask for a “short bio”, I usually reply: “No one owes more to his research community than I do”. This sums it up for me.

However, the ACM (via Jean Sammet) did suspend my regular activities by “strongly encouraging” me to write “The Early History Of Smalltalk” for their HOPL II conference and history series (circa early to mid-90s).

One of the requirements for the Kyoto Prize (2004) was that I had to write and give a “Commemorative Lecture” that had to include some autobiographical elements. https://tinlizzie.org/VPRIPapers/m2004002_center.pdf

I wrote a tribute to the ARPA/Parc community in 2004 when Butler Lampson, Chuck Thacker, Bob Taylor and I received the NAE Draper Prize for “Networked Personal Computing”. https://tinlizzie.org/VPRIPapers/m2004001_power.pdf

Recently — for a 50th year celebration of this and that — the University of Utah asked me to give a talk about what I had done in my 2.5 year long doctoral program — this is more of a history than a bio: “08 We Were So Lucky Alan Kay

*Biographies* are tricky — told by others they are generally quite inaccurate and often miss the point. Unfortunately, much the same obtains for *autobiographies*! Still the latter are the best if the authors have found ways to take multiple perspectives on themselves.



Discuss fully the history of computer and the operating systems. Why are Gul-based operating systems more popular than command line interface (CLI) based operating system?
This sounds like a combination of a command and a homework assignment (a would be command). Please try me again with the first sentence omitted.



What profound insights did Seymour Cray have that allowed him to design faster computers than others could at the time? Do they consist of proper cooling and signal sync, which, frankly, seem to be methods that computer scientists should have known?
I had both the privilege and experience of working with Seymour Cray in 1965 on his CDC 6600 at his Chippewa Falls lab (when he was still with Control Data). I very strongly suggest that you first read the Wikipedia article on Seymour Cray to get a small but useful gist of his life and works. Then I suggest you read the Quora answer I wrote about “Idiosyncrasies of Old Computers”. I will probably repeat some material from this.

First, it was because of Cray’s design abilities that there was a company called “Control Data” and that its products were the only competition that IBM was worried about: Cray’s 1604 was perhaps the first completely transisterized computer — and it was very good — conflicts with Sperry Rand about this machine led to the formation of CDC. The CDC 3600 was a fierce competitor for the top end IBM scientific computers (like the 7090 and then 7094), and — having spent some time programming it at the National Center For Atmospheric Research in Boulder (NCAR) — I thought it was superior.

There was also an “arms race” for super computers — due to both the cold war and, in NCAR’s case, concern about the weather systems of the world, and in the early 60s, this was joined by a concern about global warming — the first good enough research was done in the late 50s by Charles Kealing, and definitive results announced by NSF in 1963 (so most of the world wasted most of 60 years to prepare).

The early buyers for SCs were Los Alamos, Livermore Labs, Sandia, NCAR, etc. IBM had a habit of quashing competitors by announcing a new machine to oppose them (whether or not IBM was going to eventually deliver, or even work on the machine — this eventually wound them in court in a large anti-trust case, which they lost). One of the IBM announcements was about the “Stretch” (the IBM 7030) which was worked on, but IBM had a very hard time actually building this pipelined architecture.

Meanwhile, CDC promised “something wonderful” in terms of both speed and price when they announced the 6600. All the big buyers bought one before any were made and running.

Because of their past experience with CDC, the big buyers expected (a) that the eventual machine *would* really be as fast as claimed, and (b) that the software *would be terrible*.

CDC had set up a large SW team for the 6600 in California (very far from CDC’s headquarters in the midwest), and soon a number of nicely written manuals showed up about the various assemblers (the 6600 had “essentially/logically” 12 processors of two different designs). These were in the form of 10 “peripheral processors” which were essentially “In/Out Units”, made flexible via programming, and a single 100ns cycle time CPU (but if carefully programmed one could make use of its multiple fetch and store units, multiple arithmetic units, etc). It had its own version of pipelining via a smart cache (called “The Stunt Box”).

One of the catches of computers in those days is that core memory was much slower than the very fast new silicon transistors — Cray got them from Fairchild — used for CPUs.

So main memory was organized as 32 banks with the lower 5 bits of the 6600’s addresses treated as bank addresses. One could start a new fetch on any bank every 100ns, but would have to wait 10 of these cycles (1 microsecond) for the actual data to show up. Part of the many games a programmer had to play with this machine was to keep as many banks cycling as possible (failure to do this would drop the effective CPU from 10 MIPS to 1 MIP! The Wikipedia article on the 6600 is not very good but will help a little (I don’t think it was written by anyone who was a programmer of this machine).

At NCAR the two head programmers — Paul Rotar and Dave Kitts — decided that CDC would flunk the software part of the 6600 development — so they decided to write their own version for NCAR’s use — and to conform to the manuals they had gotten from CDC.

I was a part-time “journeyman programmer” there working my way through U of Colorado (full time at NCAR in the summers). Despite this, Rotar and Kitts asked me to create “a fast 6600 simulator” that would run fast enough on the 3600 we had to allow them to do the systems software needed. This was a lot of fun to do, given the complexity of the 6600 HW system (it was opposite to most machines, the instructions were much simpler (basically like RISC instructions for the CPU) but the parallelisms and timings and coordinations were very tricky).

With a little help, I got this going in a few tense months and Rotar and Kitts happily used it for their work — I wound up doing some utility programming for them — and this got me to include a debugger into the simulator (the 6600 didn’t have interrupts — not such a bad idea, every programmer hated interrupt systems — but what was there in its place was quite inadequate.

Los Alamos and Livermore got serial numbers 1 and 2 (NCAR’s was serial 0 and was the test computer at Cray’s lab in Chippewa Falls Wisconsin).

Then the boom lowered. And Rotar and Kitts turned out to be righter than their wildest nightmares: there was *no* software from the official CDC software teams in California — they had only written manuals, and apparently had just goofed around while the hardware was being built.

It turned out that the other labs had experienced similar misgivings, and had also written some SW — and Seymour Cray himself had written an amazing stopgap multiprocess OS — in octal absolute, there was no assembler — in order to debug the machine.

Those who had actually *done something* got together and decided to send themselves to Cray’s Chippewa Labs to cook up something workable. I left school for 6 months for this (I got invited because Rotar and Kitts wanted a disassembler (to be able to deal with Cray’s octal code) and that I would also need to write a “6600 simulator on the 6600” to debug code (because of the lack of an interrupt system)).

Dijkstra had a nice phrase for the general programming in the early 60s: “logical brinksmanship”. And this could be applied to most of Cray’s (and his design partner, Jim Thornton’s) approach at every level to designing and building the 6600’s hardware.

I should say something about the setting. Cray was born in Chippewa Falls (population about 10,000 in the mid-60s) and he owned 40% of CDC stock — so when he wanted to escape the bureaucracy of the home base in Minneapolis to set up a lab across the woods from his home, they could only say yes. The lab was huge in size, with multiple large rooms (“bays”), but only had about 20 people (they could do everything). Each bay was like a separate building and only had one project in it (plus *all* the tools that project needed). In one bay was serial 0 of the 6600, in the next bay was what would eventually be serial 0 of the 7600, about half built already, etc.

Most of Cray’s worker bees were friendly and talkative. One of them did not talk, and wandered around in a lumberjack shirt. After two weeks there, I learned this was Seymour Cray! (who didn’t say anything until he really needed to say something …). Another image I have of him is him sitting in his office, which was completely bare except for a chair, desk, and a single piece of paper on the desk that he would be looking at. No one knew what was on the paper, or whether the paper changed over time …

What kind of mind could write a multi-process OS by literally punching octal into punched cards? (The legend is that it worked first time — and it certainly worked well when we were there.) And it had to work because the 6600 had no “blinkin’ lights” — it had a bootpanel of switches to get things started, but then lots had to run because the console was just two CRTs and some keys — called a “Gazork” because it looked like a mythical sea beast.

That kind of mind could conceive of and make a computer that just barely worked (and had many idiosyncrasies — not unlike an English sports car of the day (with its great set of tools in its boot), as contrasted with a Japanese car, which fooled GM because it didn’t break and didn’t need to fixed and tweaked)). There are two very different kinds of engineering required for these two different approaches!

For example, it is easy to get race conditions with HW, especially as it is made locally faster and faster. One way to deal with these are to have multiphase clocks that essentially perform a “wait until all signals have settled” before going on to the next logical stage. However, if you are willing to “go to the brink” you can use connections and wires as delay lines (and just make sure that everything is tweaked so that when the wires are looked at, they all have good data. This was accomplished in the small (on the HW plugin modules) and in the large, on the connections between the modules (many wires would have little tags on them saying “this wire must be 48.4” long when replaced — that’s a little over 4 ns of delay!). The engineers would tune the machine literally using nail clippers!

There were theoretically parallel adders and multipliers, but physically there was only one unit — the wires as delay lines trick was used to buffer the partial results of one logical unit while the other logical unit was cycling (this trick was later used to great effect on the Parc Alto by Chuck Thacker instead of putting a slower output latch after the ALU).

My boss at NCAR — the wonderful Glenn Lewis, who had gotten his PhD from von Neumann — was mild mannered, super smart, and terrific to work for and with. The only time I saw him lose it — and it was a total meltdown — was when he found out — after the delivery of our 6600 — that it had *no* parity bits or checking in the main memory. When confronted, Cray said he wanted to use every core in memory for “information bits”!

Here are some extracts from Paul Rotar’s history of early NCAR computing:

The 6600 was a distributed architecture (central scientific processor supported by ten very fast peripheral machines) and was actually a reduced instruction set (RISC) machine many years before such a term was invented.  It also turned out that the CDC 6600 had a number of serious design problems that would impact the user community. 
... 
Early in 1965 it became apparent that software development by CDC for its 6600 was not on a realistic schedule.  There were two systems: SIPROS (Simultaneous Processing Operating System) and 
COS (Chippewa Operating System).  A test of ten codes showed that only one would run on either system.  Thus, the CF began development of an operating system that would be ready to install on the machine when it arrived in late 1965.  This decision to develop operating software for the CDC 6600 set the character of the facility until the advent of the Cray era. 
... 
The CDC 6600 system was designed and implemented by the CF Systems Section consisting of Paul Rotar, Dave Kitts, Bob Working and Gene Schumacher (on loan from CDC).  *Systems staff* had written a simulator for the CDC 6600 which provided an assemble, load-and-go environment for testing the CDC 6600 operating software modules on the CDC 3600.  The development and use of these simulation systems allowed the CF to prepare systems software for future machines and to enhance the existing software without disrupting the users because the simulators ran in the users’ environment. 
** I was part of “systems staff”.

You can see that Paul Rotar did not think highly of the 6600. I (later) thought of it as more of a scientific experiment in computer hardware design rather than an actual workable machine. Though I was a “staff programmer” I really didn’t know much about computing (my majors at CU were pure math and molecular biology). In grad school, I found myself in a deep computing culture (the ARPA-IPTO research community) — which knew a lot about HW and SW — and was able to have more after the experience thoughts about the 6600.

It definitely addressed many of the major problems of designing and making a supercomputer. I.e (a) The memories are both too small and way too slow. (b) The faster you try to run, the more heat you develop and have to dissipate. (c) there are difficult timing issues at almost every level of scale (d) eventually you actually have to write and debug and run programs other than benchmarks: what is this like? And what are the tradeoffs in eventual speeds?

To address (d) first, there were no levels of program writing that were satisfactory. *Extremely careful and mind-boggling care* could result in fairly small, fairly fast programs. I wrote a few of these (data conversion as a streaming process was a good target for what could be done). However, the kinds of programs that were not only written in FORTRAN, but the scientists wanted to stay with HLLs — and were complete nighmares in the machine codes of the 6600 — ran *really s l o w* … For many years, the FORTRAN used on the 6600 ran slower overall than the FORTRAN on the 10 times slower 3600!

This was because unwinding a loop didn’t help much — you had to unwind the loop and then carve it on the diagonal (to get sections of iterations i, i+1, and i+2 as a chunk). It was years before compiler technologies evolved to even deal with such complex planning issues. The result was that what Cray wanted (the fastest by far computer in the world (which he got)) conflicted with what the scientists wanted (the fastest by far FORTRAN programs in the world (which they didn’t get at all)).

This conflict has happened many times over the history of computing — and still persists. There are still very disconnected worlds of computing, and the goals — and what optimization even means — are very often vastly different. This has generally been the case for “supercomputing” of most kinds over the decades since Stretch and the 6600.

It might help the readers of this already long answer to ponder other — better — ways to deal with issues (a), (b), and (c) in light of the “scientist’s goals” for having (and paying for) a supercomputer in the first place …

For example a lot of the desired programs were essentially “fluid dynamics” simulations — the fluids could be air, water, oils, biological organisms at the microlevels, fissioning plutonium, etc. Science understands most of these in terms of local interactions and intermodule interactions. For the atmosphere, one carves it up into “cells” (whose size depends on the computing capacity), and writes code as discrete differential relationships within each cell (there might be several thousand parameters) and also in terms of how cells relate to neighboring cells — there might be thousands to millions of cells. Similar techniques are used for each kind of fluid studied.

What architectures can you come up with that address *what is actually needed* (WIAN)?



Where do you think programs begin to be a "big ball of mud"? That is where do you think programs get large enough that a person cannot track all the interactions between the parts, even if the parts are coherent abstractions?
The great computer scientist — and first Turing Award winner — Alan Perlis, liked to say “One person’s constant is another person’s variable”.

For concepts, cognitive science has measured “cognitive load” going back to George Miller’s 7±2 simultaneous “chunks” (this was for sequences of alphanumerics — is more like 4±3 for larger ideas at once). Visually, we can “browse” about 100 images at once to locate a particular image (this is an important “constant” for UI designers). Beyond this, *search* becomes more and more important.

A typical 400 page book is about 20,000 lines (and usually organized much more than code is). 100,000 lines of code not looked at for a few years is much harder to grok than when dealt with in more recent memory.

Dan Ingalls’ first Smalltalk-76 system included its own OS, development and end-user tools, graphics system, etc. and was about 185 pages when printed out on 8.5″x11″ paper. This was about 10,000 lines of code (50 total classes) and was quite joyful to read through and work with.

Another way to look at the question is via questions of “modules at a time”. Even though we humans have real cognitive load issues, the above suggests that code put (and debugged) in a small number of simply communicating modules that have been debugged standalone can be handled reasonably well.

The catch is that the “code” here is often different in form than the codes used for the internals (this can be dealt with to a large extent by deriving the forms for the smaller modules of code from what is needed in the large — e.g. this is how the Smalltalk “language” was originally derived: the system itself is just conventions for communication and for message forms — one makes a “language” via coming up with great designs and expressive messages).

It’s sobering that what one sees on the small visual angle of a screen is usually about 50 lines of code in a window, and there is often little sense of the whole system and its possible interactions. This gets to be “mud” very quickly — and the weak module and expressive possibilities of most systems make it difficult to produce meaningful overviews.



How might Alan Kay elaborate on "You could think of it [Val Shorre's Meta II] as an active interface language on objects"?
Part of the context here is from the early thinking about objects (using Meta II as an example of a pattern matcher), and part is very relevant to today and its scalings.

Let’s suppose we’ve gotten far enough to see that “entities” that can act as whole computers communicating on a network can *semantically* represent anything that computers can do — and that no other concepts are needed.

We then need to think about “messages” between the objects. We should be able to see that after some levels of scaling that “messages that can command” are a bad idea — and this should lead us to conclude that this is also a bad idea on smaller scales: so messages shouldn’t be imperative.

Claude Wilbur in a comment below asked a good question about “imperatives” that I should have made more clear here. My reply was: What I meant was that messages should not be *commands* that have to be obeyed — messages should only be *requests* (that can be refused by the receivers), so *every entity is a “protected server”*.

We should also be able to see that scaling will start to make “names” less and less useful (for example, to have “names” that can be understood in a world of billions of possible destinations, is not a good approach).

This can lead to several useful ideas. One is to understand why human languages have “sentences” rather than just a “word” for every idea (we want to have fewer basic concepts than the ideas we want to express — so we want to be able to combine basic concepts to express new concepts, etc.

Another useful idea is that — because words tend to be very local — ultimately finding useful destinations for our messages will require flexible description matching.

Both of these ideas lead to possibly much simpler ways to get more done.

Early pattern matching systems which influenced my thinking were Schorre’s Meta II and Carl Hewitt’s PLANNER. For interested readers, I commend the study of Gelernter’s LINDA and what it could mean as the start of being able to deal with much larger scalings and meanings.

Note: a wonderful thing about programming languages in general is that a rather small set of primitives can produce a very large range of meanings. A problem with programming languages in general is that programs beyond a certain (small) size are rapidly opaque to humans, and thus lose their explanative usefulness.

But what if a “message language” was designed much much better? And the use of it was much less ad hoc than programming too often is?



Is it necessary to make a GUI version for a simple program?
In the spirit of “Context is worth 80 IQ points”, let’s try a different perspective. A UI is the meeting ground between processes, especially between humans and media (where the “process” is sometimes the description of another “process”). To me, then, the *appearance of a program itself* is its first layer of “graphical user interface”: i.e. the visible meeting ground between a reader and the intended meaning of the program. It is often the case that the writer of a program will be unable to read it some months and years in the future (i.e. programmers need “user friendly” also). The Turing laureate Tony Hoare once remarked that “Debugging is harder than programming, so don’t use up all your cleverness in writing the program!” (You will eventually get wiped out trying to debug it (and often in the future).) This doesn’t promote COBOL to a favored position because what its program texts describe are often quite opaque intentions. “English” that is unreadable is still “unreadable English”. It also doesn’t automatically damn a language like Pascal, C or an assembly code. We should see however that most line by line comments only say what the line is doing (and — oops — these sometimes don’t get updated when the code is). The reader needs to understand *intent* and *meaning*. “Bad imperative languages” can work because, although the actual text of the program can be opaque, it can be surrounded by good to great explanations, and even integrity tests. An extreme case is Don Knuth’s TEX book “B” in which the actual Pascal program is embedded in excellent “literate text”. My own view is that language designers should deeply ponder these issues, because the less readable is the program text, the more strain is put on the programmers to reveal their intentions — and humans — and programmers! — being what they are, the results will often be ugly and substandard. Doug Engelbart’s NLS system in the 60s was programmed mostly in a low-level form — basically a structured assembler with expressions called MOL — that tried to be a happy medium between the very slow SDS-940 and the higher level language that they would have liked to program in. They implemented their own Meta compiler-compiler to implement MOL, and they had the chops to make all their tools in a timely fashion, with net wins all around. A nice touch was to embed all of their code within the hypertext structurings of NLS itself, and its features of shared collaboration and explanation, and this provides a great programming tool along with all of its other benefits. (Hard to believe today’s generation both is mostly unaware of this *and* somehow is not able to carry off the tool design and building.) To go into a more primitive area, in the early 60s most code was in assembler, on cards, and some programmers found that *always* using macros allowed them to create a “higher level language of intentions”. Some, like the legendary Bruce Baumgart would reimplement his macro language on each new computer he had to work on so he could port useful old code. (This was very tidy, and his code was very readable at the macro level.) Note: a problem with macros is that it is almost always the case that the expansion of the macro at compile or assemble time means that its structure and meanings are lost when real-time debugging (the debugger doesn’t usually actually know about the larger structures in code — but a richer scheme could allow this … If used judiciously, an *actual extensible language* can really help. Important note: in the hands of poor designers, this can be even more of a disaster than a “regular language”. This is because — in the heat of battle and debugging — even good programmers often become poor extension designers, with a “Tower of Babel” result. This is because “extension is meta” and “meta usually has vaster implications”. OTOH, getting good at this should be a required stepping stone in the learning curves of programming, regardless of the level of language. (I should mention that we found out about this the hard way with Smalltalk-72, which was very extensible in syntax, semantics, and pragmatics. Dan Ingalls rescued this with a great design compromise in ST-76, which was almost as extensible and more readable in many ways than the earlier version, and was superior in most important ways. One of my greatest pleasures was reading the code of the entire ST-76 system when it first appeared. However, we were generally not as tidy as Dan was when he rewrote the system from scratch every few years — even with tools, we would often commit spur of the moment design blunders which would start to fog up the clarity of the system. Having a good debugger really helped, but our mostly highly successful experiences with this way of doing things also brought home some of the design flaws in Smalltalk itself, especially in capturing the essence of whole systems.) It seems to me — now that we are entering the year 2024 — the whole field should be doing much better here, shouldn’t it? The above was written more or less assuming the longstanding practice that code is a form (usually text) that stands apart from the eventually running program itself. But in the spirit of the last sentence, I think we should ask — in 2024 — if any program should be written in a standalone textual form. We are no longer using 026 keypunches — instead we already have interactive computers at our disposal (oddly, we seem to need to frequently ask ourselves, “how can my computer help me do what I want to do?”). For example, to make ourselves feel safe and virile, should we use our computers to simulate the 026 (perhaps with a few improvements) to produce files of simulated card images, which then have to be compiled and loaded (usually in an early bound form)? This seems completely divorced from the actual realities of large complex systems, which — besides functionality — need integrity, safety, dynamic relations to intentions, fixability and changeability in the future, etc. Recall that all of Facebook (including the company itself) took itself off the Internet for more than a day because its system didn’t know that it was supposed to be on the Internet! (This despite over 9000 unit tests for this and that!) IBM 026 card keypunch Simulated 026 punch cards (note: this editor was done in the *80s* long *after* more capable integrated IDEs and hyperlinking had been done in the 60s and 70s — note the comment (which is not trying to be funny!) — almost makes one think that a huge percentage of programmers might be both lazy and undereducated!) I.e. Moore’s Law did happen as predicted (and more so) for HW from the 60s on, but almost the opposite happened for SW — Yikes! Just to rub it in — here are some non-lazy and industrious computerists, who saw Adele’s Smalltalk book in the early 80s and decided that they wanted to do this, despite only having MSDOS and only character displays with no pointing device. But a character display is still a kind of raster …. so: Here is Digitalk Smalltalk Methods from 1983, done independently from Parc, and using techniques necessary on 8-bit micros and early IBM compatible HW. It includes a complete window based IDE and debugger (if you’d like a few more early heroes, how about George Bosworth and Jim Anderson?) — from Digitalk Methods manual found online. I.e. “We don’t need no stinkin’ bitmap!” to make an overlapping window interface, etc. One conclusion relating to the original question is that any reasonable IDE in 2024 should automatically provide a UI for any code written. There were a number of past examples from the 60s and 70s, and a white paper that attempted to be provocative written in 1984 with the title “Opening The Hood Of A Word Processor” …



Is the most important thing for programming "readable code"? If you had to name only one thing.
I wouldn’t name just one thing: this is a bit like asking “which leg of a three legged stool is the most important one?”

For example — in the end — (b) we are always creating systems and integrating with systems. So it is really “an important thing” that the language is very good at expressing systems, interrelationships, and runnable requirements.

If we are only picking three, there are lots of “(c)”s. For example (c) safety/trust/accountability could be thought of as a “compound leg”

I would still want a (d) that has to do with “creating and controlling world-lines and histories”. This would be required for a significant part of (c), so maybe that is where this hugely important, much neglected criterion could reside …



What is Alan Kay's alternative solution for the web which he mentioned in OOPSLA '97?
—— Some additions and edits made Dec 3 2023 ——

I will confess to no longer remembering exactly what I said 26 years ago …

I was shocked that the web people and the browser people had apparently taken no heed of much better visionary work in the past, which could have made a big difference in how things went and now are.

Three big examples were (a) Doug Engelbart’s NLS system (and even more important: Engelbart’s visions about collaborations and communications), (b) Apple’s Hypercard system which was both really good as it existed in the late 80s, and more importantly: showed a path for how the web could be matured to the benefit of all users, and (c) how Postscript solved important systems problems.

Instead the WWW went for simple text based markup docs, and the web browsers were generally even worse because they concentrated on consumption rather than authoring.

It’s worth noting that the option to *really share* was built into the OS of Engelbart’s system (and today, 55 years after his big demo, none of the standard operating systems in use have this as a basic feature.

Hypercard showed how an end-user authoring system could be learned and used by literally millions of end-users, and it provided a model for how web authoring could be done. This was initially completely ignored by the web people, and then gradually not done well (recall the actual early history of Java, and why “Javascript” is called “Javascript”.

John Warnock and Chuck Geshke were originally at Parc, where an interesting problem had arisen regarding editing on bit-map displays and using the new high resolution bit-mapped laser printer of Gary Starkweather:

What should a personal computer send to a laser printer to get arbitrary images printed?

The first solution — a data structure interpreted by the printer — broke down when it was realized that it would be almost impossible to deal with all the possible images, to update printers, etc., etc.

However, this brought up the idea of *sending a process* to a virtual machine of a programming language — on the printer — aimed at making images in a resolution free manner. The process could be run safely in an address space — and use the equivalent of capabilities for accessing outside references (like the pixel buffer for the printer, etc.). The printer interpreter would know the characteristics of the printer, the resolution, the media, etc. This solution would much better match up “who needs to know what”, and allow printers to be “colonized” by the simple VM interpreters. This later allowed Adobe to quickly populate printers with Postscript.

The *process* is thus a kind of object which contains both code and data and is safely encapsulated.

Note that this suggests — especially within an Internet — a model for dealing with a “web” and its user interfaces. I.e. send processes rather than text, and have the common denominator start with (a) an OS to deal with virtual machines that can run them safely, and (b) to map the outputs of the VMs to the user’s screen (the equivalent of the printer). There is a big opportunity (c) to think of the GUI for this as a way to help organize interactions, but also (d) to allow the processes to do their own UIs when this is a good idea.

Note that this also suggests that peripherals — such as printers — contain the code in a higher level language for their own drivers (and can send it to user software on the personal computers rather than choking up the local OSs with tons of driver SW), etc.

The above paragraphs are the kind of thinking that systems thinkers would do …

Another comment I might have made back then is that the initial visions were so small by the web and browser people that they thought the answer was “an app” rather than a real operating system environment that could handle what the Internet was becoming (Engelbart had a much better vision before there was even an ARPAnet in the 60s).

I recall that the web people eventually revealed that they did not know about Engelbart’s previous work.

From the “practical” point of view it is very difficult to start with a weak conception and then incrementally add on features without winding up with a kludge and unnecessary complications for all. This includes really important considerations such as safety and trust (they are hard to add to an initially non-safe and non-trustworthy system). Similarly, it is hard to scale a system not designed to be scaled.

Etc.

This is what made the success of the web and its browsers so shocking. I have a shudder every time I peruse a Wikipedia article about something dynamic (like a programming language, physical system, etc.) that has no dynamic model as part of the explanation (and Wikipedia is running on the end-user’s own powerful computer!).

What part of “computer” did the web and browser people not understand? (Note that the original web was done on a Next computer with a decent UI and OS — and the later browser was also done initially on computers with actual OSs and GUIs …). My guess is that they didn’t think about the future (because any thought would have revealed that what was required was actually *more* than personal computing already had, not less).

A no less shocking thing today is that (anecdotal) I’ve found that most people who claim to be computerists are stumped when they are asked to criticize e.g. the browers, or the standard OSs, etc. They have become normalized to “poor” and think it is “normal” and “OK”. To me this is not only sad but dangerous.



How is math translated into code? For example, ray tracing in computer graphics, how is the math and physics equation of that translated into a code?
The answer by Jim Lux will help. Here’s another slant on this. A lot of early computer programming was about taking mathematical formulas and writing code to compute them. One of the very first “higher level programming languages” (released in 1956) was FORTRAN, whose name was derived from FORmula TRANslation. It’s main ability was to allow recognizable arithmetic formulas more or less in algebraic form to be written directly in the code. (I have a Quora answer somewhere about FORTRAN. Thanks to Dane Clarke, here is the link: Profile photo for Alan Kay Alan Kay · 4y What was the historical significance of FORTRAN? The historical significance of FORTRAN is primarily due to the level of software engineering in the system, but there was also a “four minute mile” effect from the combination of “early” and “efficient”. An earlier significant system along the lines of “runable mathematics” was by Laning and Zierler — an interactive system called “George” ca 1952— on the Whirlwind I computer at MIT. The syntactic and semantic facilities provided by George exceeded those of FORTRAN, and it also was one of the very first interactive programming systems. (Apparently FORTRAN was already started at IBM by 1952.) Especially in the 50s and into the 60s, with tiny and slow hardware, hand optimization of code in assembler was one of the main jobs for programmers. The IBM group headed by John Backus determined to make compiled code compete with optimized assembly code in both speed and space. This led to long, many pass compile times — thought of as a luxury back then (programmers were cheap), but the results were quite good enough in many cases. The extent to which IBM carried this out was astonishing. For example, you could actually write and run FORTRAN on the 8KB 1401 at USAF Air Training Command (where I worked in the early 60s). I think (I hope someone will track this down) that the FORTRAN compiler would do over 100 passes — from tapes to tapes — to do a compile. This was somewhat frowned on by the brass at the time. FORTRAN’s (relatively) efficiently running code produced two distinct kinds of user groups: (a) scientific programmers and (b) job shop programmers who wanted to get certain programs running well enough and quickly. The FORmula TRANslation part was limited to just arithmetic expressions: everything else in the early FORTRANs was quite basic and at the level of control structures and simple macros in assembler. ) This effort was successful because John Baccus and his IBM team put an enormous amount of work into not just doing the translation, but in optimizing the code that was produced to compete successfully with hand written code. For a time, FORTRAN was actually regarded as “early AI”. Most subsequent programming languages — Algol, C, C++, Pascal, Python, etc. — have also implemented “formula translation” and similar optimizations. (Added Nov 24 2023) I realized that the short answer above is a bit cryptic for the level of the question. What FORTRAN brought to the table was to have a form in which simple algebraic formulas as used on paper could be put on punched cards in almost the same form. AREA = SQRT( S * (S - IA) * (S - IB) * (S - IC) ) Note that SQRT(…) is used instead of the radical sign, and that multiplication is specific (“*”). At the next level, most computing machines have an ADD instruction and usually a SUB tract instruction. Most will also have a MUL instruction (some will have a subroutine for this). And most will not have a SQRT instruction, and will do this via a subroutine. The task of the FORTRAN compiler is to translate the characters in the example line to a sequence of machine instructions that will perform the indicated computation. The “variables” will be modeled by words in memory (“core” memory in those days). Most machines back then didn’t have a stack, so the compiler would also allocate temporary variables to hold intermediate results. For example: LDA S SUB IC STA TEMP1 would compute the value of the last subexpression. The other subexpressions would be computed the same way and stored in TEMP2 and TEMP3. Then the machine code would do the multiplications LDA S MUL TEMP1 MUL TEMP2 MUL TEMP2 This will be the value of the parameter for the SQRT subroutine, which might be called thusly: JMPSUB SQRT STA AREA which will assume its parameter is in the “A” register. A convention on this machine might be to leave the result in the “A” register and return to the instruction word after the JMPSUB which will store the result in AREA. Note that this code could be optimized … Bottom line is that human conventions for “formulas” can be used in a program if there is a compiler that can translate the formulas into the machine code of the machine being used while preserving the meaning of the original goal. A very early famous (and famously simple) system for translating from grammars of programming languages to runnable code is “Meta II” by Val Shorre. The wikipedia article has an explanation of it and also has links to the original papers (read this one, it is a classic). META II - Wikipedia META II is a domain-specific programming language for writing compilers . It was created in 1963–1964 by Dewey Val Schorre at UCLA . META II uses what Schorre called syntax equations . Its operation is simply explained as: Each syntax equation is translated into a recursive subroutine which tests the input string for a particular phrase structure, and deletes it if found. [ 1 ] Meta II programs are compiled into an interpreted byte code language. VALGOL and SMALGOL compilers illustrating its capabilities were written in the META II language, [ 1 ] [ 2 ] VALGOL is a simple algebraic language designed for the purpose of illustrating META II. SMALGOL was a fairly large subset of ALGOL 60 . META II was first written in META I, [ 3 ] a hand-compiled version of META II. The history is unclear as to whether META I was a full implementation of META II or a required subset of the META II language required to compile the full META II compiler. In its documentation, META II is described as resembling BNF , which today is explained as a production grammar. META II is an analytical grammar. In the TREE-META document these languages were described as reductive grammars. For example, in BNF, an arithmetic expression may be defined as: < expr > := < term > | < expr > < addop > < term > BNF rules are today production rules describing how constituent parts may be assembled to form only valid language constructs. A parser does the opposite taking language constructs apart. META II is a stack-based functional parser programming language that includes output directive. In META II, the order of testing is specified by the equation. META II like other programming languages would overflow its stack attempting left recursion. META II uses a $ (zero or more) sequence operator. The expr parsing equation written in META II is a conditional expression evaluated left to right: expr = term $ ( '+' term . OUT (' ADD ') / '-' term . OUT (' SUB ')); Above the expr equation is defined by the expression to the right of the '='. Evaluating left to right from the '=', term is the first thing that must be tested. If term returns failure expr fails. If successful a term was recognized we then enter the indefinite $ zero or more loop were we first test for a '+' if that fails the alternative '-' is attempted and finally if a '-' were not recognized the loop terminates with expr returning success having recognized a single term. If a '+' or '-' were successful then term would be called. And if successful the loop would repeat. The expr equation can also be expressed using nested grouping as: expr = term $ (( '+' / '-' ) term ); The code production elements were left out to simplify the example. Due to the limited character set of early computers the character / was used as the alternative, or, operator. The $ , loop operator, is used to match zero or more of something: expr = term $ ( '+' term . OUT (' ADD ') / '-' term . OUT (' SUB ') ); The above can be expressed in English: An expr is https://en.wikipedia.org/wiki/META_II



What programming language does Alan Kay consider to be the most advanced?
I used to track programming languages as well as I could (but as Jean Sammet pointed out in the late 60s, “There already have been over 3000 PLs designed and made. How can we sort them?”).

The ACM chooses languages for its “History of Programming Languages” (HOPL) series, but only from languages that have been in use for about 10 years, have affected other languages, etc. There have been four HOPL collections of histories so far, all published as books.

I’ve given a few talks that have asked about why software and programming languages have not even kept pace with exponential Moore’s Law HW scalings, and consequent changes in requirements (e.g. security, safety, specifications, etc.).

The main events for me after I turned 80 three years ago, have been trying to avoid Covid here in London, and developing a bad cancer about two years ago (“It is what it is”, no comments needed).

But I’m pretty sure now that there could be one or two really interesting languages out there that I’m not aware of. The last time I really checked was a few years ago when I got asked to do 3 different “Software Engineering” talks at almost the same time. This got me to scurry around to contact colleagues to find new colleagues (especially in the tactical trenches) who would have more accurate perceptions of the current situation.

I found a number of top people I had not been aware of (for example, Gerard Holzmann of NASA) who had accomplished really interesting and important things. But I did not find anything of great new interest in the programming language arena. Most of the top SEs I talked to were doing projects bolted onto quite weak (and old) programming systems and requirements (for example NASA requires its SW for e.g. a Mars Rover to be done in C (!) despite that it will have to run for a year or more on Mars!). The result is that an enormous amount of computer science and engineering (and funding) is expended to take what is essentially a house of cards, and make it into a “real house”.

Universities used to be a very good place to birth new programming languages, but today’s funders seem to have little taste for funding these today — and worse, what I see (anecdotal as it may be) are many students getting trained in languages for legacy systems in business rather than being taught how to design and build new languages.

So:

The real question is not “most advanced?”, but “up to today’s actual needs?”
Is there a good list of requirements for “up to today’s needs?”
Are there even any first-try example languages that are aimed at “up to today’s needs?”
I find it hard to imagine that any reasonable candidate these days would not include larger facilities (such as active knowledge of important parts of the world that the results will touch (think Boeing 737 Max Autopilot, the Facebook and Google crashes, etc.)).

It should be clear that “something actually ‘expert’” is needed, but that the LLM ML approaches today are really dangerous without serious grounding in a cognitive knowledge-based system.

ADDENDA

<to appear>



What does Alan Kay think about programming and teaching programming with copilots and LLMs of today?
A little context and bias before I attempt to give an answer: I started programming as a job ca 1961 in the USAF. The programming style was “data munged by imperatives with loops” (sound familiar?). An interesting wrinkle was that the machines we used at that time (and most computers) did not have what would be called an “operating system” — but there were yearnings, and also interesting subsets of code that helped “operators” run “jobs”. In just a few years, as the result of many different forces, a number of different operating systems started to appear, and most of them had as one of their main goals to be able to run multiple processes, including processes written by different people for different reasons.

Big shifts in outlook started to happen, and many of them were ultimately about “trust” and “security” in “evermore complex systems of processes”. The history of this is really interesting — including that some of the best solutions were done very early in the game (some of these never got generally recognized by the run of the mill programming establishments). In order to get to the answer of the question, I’ll only urge readers to look into this “epistemological revolution that was generally missed by the field”.

Let me use *trust* as the central theme for this question and answer. *Trust* was certainly part of the criteria for a program before the days of multiprogramming, but it was basically whether a running single program could be trusted to do what it was supposed to do. Bugs were there, but only affected the program that had them — in rare cases, bugs could be propagated via “wrong data” written on tapes, etc., but in practice, almost all bugs were local, so the integrity of a program was quite a bit easier to ascertain and improve.

When the desire for multiprogramming happened, it was attempted on hardware that could not confine the different processes (sometimes an entire core memory image (usually less than 150KB) was rolled in and out for each job to and from a drum or even tape memory). This led to some hardware assists of various kinds, and attempts to have the cake and also eat it.

The bottom line today is that most computations are not as secure and trustworthy as they need to be (in part because some of the best solutions wound up being perceived as being “too expensive” and then abandoned).

Note that *Trust* becomes front and center as Moore’s Law advances, especially with advanced networking.

Note that *Trust* becomes IMO the *dominant issue* when all that has gone before is added to “NCANIP”s (non cognitive artificial non-intelligent processes* are allowed to run wild in forms that as Harari has pointed out “hack the language communications systems of our species”.

Note that “Trust” is one of the deepest issues — much larger than “just computing” — when not just communication, but actual education is one of the main needs and goals. We want to be able to know the degree of trust we can allow for what our own minds come up with, what we hear from others, what we read, our teachers, etc. A big deal with science is that it was partially invented by humans — after hundreds of thousands of years — by learning better methods than trusting one’s senses, or mere beliefs of one’s cultures.

A simple principle is that for most things that are automated and scaled, the *trust requirements* have to be vastly expanded and made vastly more strict.

Finally, to try to answer the question … (Summary: I don’t think it would be a good idea at all)

First, I don’t think the way programming is generally taught (as I’ve seen it in a number of major universities) is a good approach (and hasn’t been for about 60 years now). Qualitative changes of many kinds have obsoleted the general approaches, the languages, the operating systems, the infrastructures, *and* the ability to *trust* what can be found online — Etc etc.

Second, the genii has been out of the bottle for much of the Moore’s Law expansions, and this creates critical questions — as with most infrastructure issues — of how much effort should be expended to keep patching the problems vs actually addressing them with better more secure, more trusted, and *more system* approaches. (Putting typical businesses in charge of these tradeoffs has been an unmitigated disaster — but we can see that savvy and wisdom in government is not up to the task either — still, something like government will be required.)

(Note, it used to be part of the job of universities to not just teach/help-learn ideas, but to be both “keepers of the flame” and the vanguard for “reinventing better fire”. (Claim/guess: most so-called “computer science” departments in universities today do neither — they seem to now be trade-schools for what existing businesses/organizations have gotten themselves into trouble with.))

Third, I like to praise the (actual) Internet design because (a) it was really done well, and (b) I was not involved enough in it to claim any credit (thus, my praise should be more “trustworthy”!). A key part of their design was to *not allow direct sending of commands* — only bits could be sent. This means that (other) software inside each physical computer has the responsibility to *interpret* the bits, and the power to *do* (or not do) *some action* (note that this and other ideas of the Internet suggest ways to organize software processes, etc.).

Fourth, I do think that an expert system could be made to *help* teach programming much better than a book can, and better than most teachers of programming do now, so that most learners can *actually learn* much better than they do now.

By “learn” I mean that important processes and relationships are created between the learner’s ears by their own brain.

By “help” I mean that — especially when *changes in epistemological points of view from one’s own common sense are required* , it can make a huge difference to be near a “special human” whose personality is strong enough to make us rethink what we think we know. Our “trust” in this person is not necessarily that they are “right” all the time, but that “they are consistently interesting, and thus worth listening to”. Occasionally, we can have that kind of trust in an author, but it is often all too easy to be lazy about rethinking …

Fifth, I think ML and LLMs, etc., do have a part in “actual machine intelligence” in the future, but not nearly the most important parts. I don’t think the systems exhibited today are to be trusted at all (and note that using such a system to provide explanations for what another such system has concluded, is just piling BS on BS). That humans also do this all the time is “interesting”, “dangerous” etc., but it is also why trying to move from *superstition* (this is actually what “reasoning by correlation” amounts to) to more scientific methods is critical for anything like civilization to be created.

Finally, I think the big deal here is vastly larger than the vast disasters in computing systems at all levels. If we look at human anthropology, we see a species that treats solitary confinement and banishment from society as punishments — we are a society because we can cooperate and trust a little — but when are safely back in society, we start competing like mad (and cheating like mad) as though the society is there to be strip-mined. Scaling and technique has made this a “Spaceship Earth” issue that can no longer be put off and ignored and violated.

Robert Heinlein once warned: “The bull wears itself out on the cape and fails to see the sword”.



What is machine code? Is it really the lowest level of programming? If it is, how does this make sense as we still have assembly languages and high-level languages like C++ and Java?
This is a pretty good question. This answer is a bit more detailed than the others (but read the others, they have useful information). In “the olden days” of the early 60s when I was a journeyman programmer, we would often call the lowest level in which the machine could be coded “absolute code” (or “code absolute, or “octal absolute” or “binary absolute”). This is because Maurice Wilkes had invented “microcode” as a very good way to deal with the conflict between (a) what is called the “control problem” of a programmable computer, and (b) the desire for the machine to have a number of useful fast instructions, whose “control” would be “un-fun” in just logic gates. Some computers would have microcode and some not — all needed some form of control logic to route information (usually binary bits) around the machine from functional unit to functional unit. An early fun thing to learn about computers is that every part can be made from a single kind of building block — e.g. either a NAND or NOR gate. All other logical functions can be synthesized from these. Sometimes for convenience, three logical functions — AND, OR, and NOT (which can be made from NANDs or NORs) are used to help explanations. Because there is a lag in the operation of a gate — meaning *time* is also involved — it is easy to make a “clock”, which can then be used to help with the scheduling of events. For example, if we put a signal through a NOT gate the result will be inverted, and if we feed the result back to the input, we will get a train of digital pulses that are the “ticks” of the “clock”. The simplest things made from primitive gates are units that will add, compare, remember, “decode”, clocks, etc. These are analogous to a railway switching yard with trains that can move on tracks. The “control” part is usually made of humans in coordination, routing the trains by switching switches, etc. The “control” of a CPU is logic that will route the “trains of bits” around the switchyard for various purposes. In a long ago microcoded machine, a microcode “word” is a sequence of 1s and 0s that will open and close gates that will act as the routing switches. Usually the microcode words are looked at in sequence, and there can be branches, etc. The earliest versions would use something like diodes to represent the 1s and 0s, and the microcode would be wired in. This would be programmed by the manufacturer and would not be changeable in the field. A “diode matrix” ROM for holding microcode (you can see these were simple, fast, but painful to fix bugs/modify, etc.). What a regular programmer thought was “code absolute” would be the next level above this, and this would be fetched from main memory, held in an “instruction register” and carried out by the underlying microcode (plus some shortcuts of various kinds). (For example, the operation field in the instruction word would often lead to a particular microcode word and the microprogram would start executing there. What ever the level of coding, most programmers would not write their code in terms of raw bits, but would use a “symbolic assembler” to help. These would most often have a line of symbolic code for every configuration of bits. (I wrote a Quora answer that showed some of Margaret Hamilton’s Apollo assembly code — the listing also shows the actual bits.) However, it was realized early on that it would be *just wonderful* if you could dynamically load microcode! This would allow you to not just customize your machine, but to create new machines as needed. The catch was that the memory for the reloadable microcode had to be as fast as possible — faster than main memory (at least 5 or 6 times faster). This would allow the “inner machine” to cycle faster than the main memory could cycle (and thus it would look like HW rather than SW). A fun machine was the Packard-Bell 440, which had an interesting compromise: the lower thousand words of main memory were done in terms of much faster thin-film technology (vs cores for the rest of RAM). On this machine you would load in your most used subroutines into the fast memory, and they would act like microcode instructions, etc. Some machines had “diode boards” which allowed microcode to be switched out by hand. An interesting “beast” was the Control Data 6600 whose “regular code” was essentially microcode, and the poor programmer had to work at a very tough primitive level to match up the fast CPU with the slower (but banked) RAM scheme. The Xerox Parc machines all had reloadable microcode, and this was used heavily, and worked very well. The reason for all the different kinds of languages (above the “bits”) is to help our limited human minds grapple with the problems to be solved with the least amount of interference from complex or tricky features the machine might have. (The state of this art has hardly advanced in the last 40 or more years — and this is not a good thing at all, and results in much too complicated SW.) Added Sept 12 2023 — a story that might help. One of first classes in grad school in ‘66 was “Computer Architecture” by Dave Evans, who had actually done two Bendix computers while a VP there (the G-15, and G-20). On the first day he gave us the specs of an architecture and a problem that needed to be solved. Each of us had to simulate the architecture to make a running emulation of the computer, and then solve the problem by writing a program that the emulated architecture could run successfully to get the desired result. The only reasonable and reliable vehicle IMO was a FORTRAN that ran on the university’s Univac 1108 (which was a batch computer — so you had to submit cards and wait for a run to see what went wrong). Some of the other students actually used the 1108 assembly for this but this was just making more work for themselves. However, the double debugging problem here — both the emulator and then the emulated program running under it — motivated me to write an actual assembler (also in FORTRAN) for the emulated machine. This was an additional pain, but wound up paying dividends in getting the whole system to meet Dave Evans’ requirements without burning out too many brain cells.



How did the first computers look like and what could they do, compared to modern-day machines?
The question explicitly says “first computers” — so, for anyone with a taste for history, this should rule out answers for computers that happened after 1955 or so, and I will concentrate on “first programmable computers”. One nice part of the answer is that — with enough memory (e.g. many many magnetic tape drives) — the early computers could carry out literally every computation that today’s computers can. The results would take much (much!) longer to be formulated. This principle is due to Turing in the mid-30s. In *practical* terms, many (most) of the things we do with computers today — though *computable* by the old machines — would not be *feasible* (humans live only so long, can wait only so long for interactions, etc.). A fun computer system from the mid-50s was for the SAGE air defense system. It required an enormous concrete blockhouse as its “case”. The bottom floor was just for the power supply. The next floor — about the size of a football field! — contained “the Q7 computer” (actually a ganged together double machine with more than 50,000 vacuum tubes and a physically enormous core memory (the cores were almost 1/2″ in diameter). The next floor up was “operations” and the top floor typically held about 150 graphics terminals with pointing devices. About 24 of these “blockhouses” were built. The last one was decommissioned in 1982! Here’s a picture of one of the blockhouses: Just to emphasize again: it does not take much of a CPU plus a lot of simple memory to allow any and all computations to be expressed.



Does Alan Kay have any updated thoughts on "People who are really serious about software should make their own hardware"?
In brief: I’ve pointed out — long ago now — that “hardware is just software crystallized early”. If we combine this with why we want computers that can take on and run software (so we don’t have to go to the trouble of making special hardware for each functions), we can see that the slogan is actually requesting more reasonable tradeoffs.

In the earliest days of designing computers, the trickiest part was called “CONTROL”, the logic that would read an instruction and open and close pathways to execute the instruction. The great Maurice Wilkes came up with the idea of microcode — a fast machine within a machine with really simple fast instructions — to regularize as much of the CONTROL as possible. Often, simple diodes were used for the (read only) control memory.

It was early realized that it would be “really nice” if the microcode control memory were dynamically reloadable, because this would mean that the computer would be nicely “parametric” — one could customize it as though it were hardware. The tradeoffs here are many, but the main ones often had to do with how fast was the fastest memory the microcode could cycle, vs. how fast was the main RAM (which was magnetic core storage or worse in the old days).

The other parallel line of thought which led to the quote above came from early attempts to make custom hardware for special functions — a main one was to make machines much more suitable for running very high level languages (the nicest and greatest one was the Burroughs B5000 by Bob Barton). The bottom line on this machine was that “it was the most wonderful thing ever” (but) “it was inflexible to new software ideas” (and so probably should be microcoded to get the best of both worlds). Barton’s next work for Burroughs was just that.

We (at Parc) had big goals, but wanted them to run in personal computer sized packages. To us this meant the small machines had to be microcoded (and the budgets meant, the design had to be really good). Chuck Thacker was the main genius who accomplished this.

Also in the 60s, Utah was an ARPA project mostly engaged in inventing what today we call CGI (continuous tone real-time 3D graphics — and 2D graphics along for the ride). We took a mathematical approach and designed special hardware to do the extensive computations involved. Jim Clark, who was part of the project, started looking at a single chip design that could do the computations. This was the first GPU.

Etc. This led to the mantra above.

It’s worth noting that most of computing persisted in a dumb belief: that Moore’s Law would always catch up to special hardware (generally true) and therefore it is not worth it to design special hardware (generally false). We tried to explain that if special hardware is going to give you (say) a factor of 5 or so over regular CPUs then you will be gaining quite a few years before Moore’s Law can equal it. Those few years are critical for research, and resulted in much of the hardware forms of today.

The other thing we tried to explain back then is that if you have new ideas and try to use old hardware, you are likely to spend a lot of your effort just doing work arounds and optimizations that are just wasting your precious time.

I think this is still the case today.



What were the main ideas that eventually led to the design of modeless human computer interaction and modeless interfaces?
The other answers are pretty good, and I’ve already answered this in Quora somewhere else (see if you can find it), but just to be brief here.

The most important idea was that “modes are useful, until you want to do something else”. For example, *inserting text*, or *working on project A*. It’s when you want to change what you are doing that you forget you are in a mode and get pissed off that you have to remember to get out of it before you can do the thing you really want to do.

So “modeless” means that you can start the next thing without explicit exits from what ever you are doing and the system will clean up after you.

This idea originated at Parc, but was influenced by systems such as RAND’s GRAIL pen based system in which quite a few interactions were “modeless” in the sense above.

At Parc, the overlapping window idea I had in 1971 had this character as well. If the window was of a process, then you could touch another window, and that would automatically suspend the old process and start up the new process in the new window. This would allow you to move around at will, etc.

Both Larry Tesler and I were working with text editing and realized at about the same time that you could avoid being stuck in a “replace or insert mode” if selections started and ended *between* characters. This would allow selection of 0 to n characters, so all input actions would be “replace” and you could always select other places without having to get out of what you were doing.

This was a lot like the window idea and some of the good GRAIL ideas, etc., so we decided to try to do everything in such a way that new actions would automatically suspend old actions, etc.

This is what we called “modeless”. Larry eventually had a license plate for his car saying “NO MODES”



Why does Alan Kay call his youth misspent?
That was a joke about how many things I did that *weren’t* computing.

Actually, pretty much everything I got fluent at as a “youth” wound up contributing to my later thinking and results as a computerist. For example, I got degrees in both pure math and molecular biology (with minors in English and Anthropology), was a pro jazz guitarist for a number of years, did a lot of most aspects of theatrical production, served in the US Air Force, read a zillion books, etc., and too many more.

When I accidentally wound up in an ARPA research community grad school (U of Utah), what the community was doing was so interesting that I started focusing on it, and using my “misspent youth” to help (for example, the theatre and anthropology were great helps in user interface thinking … math and biology helped with making a kind of “object-oriented programming”, etc.)

I should point out here that “there was no plan, grand or otherwise” — I was simply following my nose. I did decide to get *fluent* in each interest, but I had no vocational or other plan. I was just “happily curious”. I had no money, so I had to scratch to support myself for all of this, but a “starving student” (even almost literally) on a university campus is surrounded by the riches of the ages to learn and use.

One way to look at all this is that the more you get fluent in, the more ways and things you will have to make analogies to, and these can lead to very different important new thoughts.



When Alan Kay conceived of object-oriented programming as being about objects that hide their state and send messages to other objects, how large were these objects imagined to be, and how closely does current teaching and programming reflect this?
Summary: they were always thought of as “all sizes” — this is what messaging allows one to think — but it took awhile to invent all the software engineering needed to make the nice idea practical enough for real system building.

Though I had been a journeyman programmer for a few years, and had seen part of the object idea a few times (B220 file system, B5000, etc.), it was Ivan Sutherland’s Sketchpad system ca 1962 that got me thinking about modeling and seeing the first Simula a few days later (how to do something like OOP by instantiating Algol-60 blocks) that got me to see the analogies to (a) cells in Biology, and (b) algebras in math.

I was in a maths mood at the time, so it was easy to see that a virtual computer could model anything on a computer — no matter how large or small — and that they could be recursively combined using messaging. This provided a really simple framework of VMs (with the internals being VMs) all on a point to point network (like the logic of the later ARPA and Internets).

The state of software engineering in 1966 was not advanced to either the very large or very small idea being rendered as an object. Most of the things “like objects” had a fair amount of overhead, and were thus rather large entities.

But the “maths” idea was too nice to give up. Meanwhile, I encountered LISP, which was not just a “neat mood” but an actual example about how to do both some of the maths and some of the software engineering needed.

For example, one way to look at passing a message is by thinking slightly differently about APPLY in the context of an FEXPR LISP. Another example is that most LISPs since 1.5 have mapped small integers into the protected LISP “entity space” (and the B5000 maps all numbers into the protected address space, etc.).

The FLEX language (for the FLEX machine of the late 60s) borrowed more SW ideas from Wirth’s Euler than anywhere else, and also used ideas from Shorre’s Meta II and the Floyd-Evans method of parsing.

Dan Ingalls was my indispensable partner for the Smalltalk project at Parc. The first Smalltalks were rather Lisp like, and then — as we got more skilled at the SE, were able to use ideas from more sources (including the FLEX machine) and from original inventions.

We could have done a bit more for arbitrarily large objects, but — since our goal was inventing modern personal computing, etc. — we mainly aimed at “clean, small, simple, and powerful” for Smalltalk. This worked out well for our project at Parc.

However, it’s worth emphasizing that messaging and late binding allow pretty much all improvements within this kind of architecture to be done while the system is in use (and without having to be stopped in order to make any changes, fixes, or improvements).

Note: Instantiation of an “idea” is a very good idea. “Inheritance” needs to be really controlled in order to not turn into a nightmare (this is why I left it out of the first Smalltalk — Simula I didn’t have it, and Simula 67 did). It was pretty easy to see that “some things could be accomplished” with not just inheritance but multiple inheritance, but the potential for *mess* was huge. Mathematically, it is a good thing to contemplate — but as a large idea that is part of questions about abstractions.

I’ve noted elsewhere that “object-oriented” to me is not a programming paradigm, but a definitional scheme — it’s a way to making things that carry their definitions with them. A programming paradigm needs to include enough design principles to guide designers though complexity and scaling. Messaging between instances of ideas is part of this. Experience would lead me to look at compositions before inheritance (the PIE system of Goldstein and Bobrow) used Smalltalk in an interesting way to do this. I think I’d look at “modeling time” before looking at inheritance. Etc. But I do think that establishing a tight enough meaning for inheritance could add more clarity than murk. I haven’t seen a good example of this.

We got away with using inheritance in the later Smalltalks at Parc because we — and especially Dan Ingalls — were very careful with it. A later experience with Morphic in Squeak was not so happy.



Why did Alan Kay move to London?
Short answer is that my wife — the amazing Bonnie MacBird — decided she wanted to write a Sherlock Holmes novel in the authentic voice of Doyle’s Watson. She had been a super fan since childhood, and this project had been on the back burner for years. As she started off, she had the idea that it might be fun, motivating, inspiring, etc., to do some of the writing at the “Sherlock Holmes Hotel” on Baker Street, London. It turned out to be nicely funky, an establishment on different levels that had been stitched together into a hotel many years before. And it was right in the middle of one of London’s many nice “neighborhoods” — Marylebone — with nearby parks small and large, a “high street” with “everything”, the Royal Academy of Music, Wigmore Hall (one of the best sounding in the world for ~550 people audiences), Baker Street Station (which is a main hub with many underground lines), in walking distance of Oxford Street, etc. Many of the buildings around the hotel are Victorian flat “blocks” that were built in the 1890s, Holmes’ fictional era. And there is the larger London, which includes some of the very best English language theatre in the world, and in great profusion. What had years before been very spotty food, had been converted, partly by the influx of other cultures including Indian and French, into more good restaurants than one could sample. Bonnie’s writing was going well, and we kept going back. I made up a “Hauptwerk organ keyboard and pedals” that the hotel stored for us, so I could play and practice while we were there. I should mention something ephemeral and hard to pin down here: but we — and I think most Americans — can detect a palpable “spirit of kindness” from most people in London. Teenagers will actually give up their seats on the underground to older people. If someone sees you need any help they will come and offer, etc. The “feel” of London is a good feel. At one point, I noted that “If we were to live in London half-time we could break even on a flat rental for what the hotel was costing us”. We looked around a bit and found a nice little Victorian era flat across the street from the hotel, one block off Baker. This had a double “sitting room” and Bonnie fixed hers up to be the 221B sitting room of Holmes and Watson. Here’s a view out the window during a very rare snow: Our flat building is like these. Our small flat reminded me of my student days, and we both have liked it a lot. Then came the shock of Trump’s election, followed by the pandemic. We almost flew back to the US for the pandemic, but decided that both the UK and the US would botch it, and that we’d rather try to sit it out in London than in LA. This turned out to be a good choice. During all this Bonnie had a contract with Harper-Collins and has written 5 highly regarded and reviewed Sherlock Holmes novels while we have lived in the flat. The Internet and wireless are good in the flat, so we were very much in contact with stuff we were doing, especially as the pandemic took hold. Then I wound up getting a very rare and bad kind of cancer, and one of the world’s experts happens to be in London. Before 2013 there was no treatment, so I’ve been lucky to have the new “immunotherapy”, which a small percentage of the time will delay matters if you are in Stage 4. There are side effects, but the cancer itself has been delayed so far. At my over 80 age, this is a “cycle of life” kind of deal, so I have no complaints. So: “London” was not a plan, but just gradually happened, and we are both happy how it turned out.



Why did you major in mathematics?
I always was interested in math, and especially after reading in a Robert Heinlein scifi book for youngsters that “No matter how or where you wind up you should understand mathematics”

I never had a plan to *be* a mathematician (I think), and the other of my double major in college was Molecular Biology (with two big minors) and I worked my way through school and bed and board as a journeyman supercomputer programmer in the mid-60s.

I also delved deeply into to many other wonderful things as an undergrad, music, art, theatre, etc. so when I graduated in 1966 I was too exhausted for anything. I couldn’t stand the idea of right away going to grad school in either of my majors, and I knew I didn’t want a real job them — if ever.

So I moped around for a few months until a thought popped. I could program, but like most programmers I knew almost nothing about “computing”. The though was “why not spend a year relaxing getting a Masters in computing?” That should be easy, fun, and restorative.

However, I loved living in Boulder, and CU did not have a Masters in computing. So I went to the library and delved for places that did *and* were above 4000 feet altitude to have that mountain air (Boulder was about 6500).

There was only one — at the U of Utah across the Rockies on the other slope. I sent off a letter not expecting anything, and to my great surprise was accepted.

I arrived “knowing nothing” and found that I’d wandered into the ARPA research community and one of its best venues.



Since Smalltalk programming language has roots in biology, is it a perfect match for various computational biology applications(like system biology, agent-based modeling)? If so, what are some good examples of existing applications?
Smalltalk does have some of its roots in biology — but certainly when I was thinking about this kind of language in the mid to late 60s, I was not using much — if anything — of what I understood about molecular biology (a really early book on Mol-Bio was Watson’s “Molecular Biology of the Gene” in 1965 — which was wonderful, but at scalings that weren’t very applicable to computing at that time).

The parts of biology that had a large influence on me at that time had to do with the immense scalings of components and interactions, both within cells — in the millions and billions — and in multi-celled organisms — in the millions, billions, and trillions of cells.

I was very interested in developmental biology, but thought how it went about it was beyond the scalings of computation back then. What *were* really interesting — and applicable — were (a) the systems integrity involving even trillions of components (large things could still work after decades) — and (b) that the scheme involved just one set of DNA in each cell, and parts were controlled and expressed to get about 250 “kinds” of cells to develop and interact as tissues and with other types — and (c) that there were many levels and kinds of “noise detectors and removers” — including at the cellular level — that were sufficient to quasi-stabilize the whole “mess” — and (d) that most of the metabolism and contents of each cell were used for local stability — very little was used to contribute to the whole organism.

I was also influenced by the “polymorphism” of the endocrine system — but this didn’t get developed as much as it could have.

There were already quite a few “module schemes” in computing — most of them large and with overheads. I greatly desired everything — including “small things” to be a ramification of one kernel type, that could ramify out to the number of types convenient for an organization. (I got sucked in by math because I could see (via McCarthy) that this could be done “from out of almost nothing” — and this is what we did. Later, I realized that putting a lot of effort into designing a capable “Class object” would have been a better path.

I think Smalltalk is still worth understanding. However, I personally would start from scratch with a new language that is much more in accord with “what is actually needed?”. I.e. it would also have modules, but they would be rather different from Smalltalk’s of more than 50 years ago now.




Alan, you have written numerous essays and given many talks on the topic of computing, but you have never published a book on the subject. Is there any particular reason behind this decision? Is it related to your idea of the Dynabook?
Probably the simplest answer to the question is that I like to work on things I have “urges” to do, and I’ve never had the “urge” to write a book.

On the other hand, I do have an urge to answer questions, and Quora reveals that I’ve written 607 answers thus far (so this is answer 608).

I think that I average about 500 words per answer, so the Quora corpus would be a thick book of over 300,000 words as a kind of kaleidoscope of paragraphs and topics.

This wouldn’t be much of a book. A similar process with talks would yield another kind of kaleidoscopic — not very useful — book: an hour’s talk at about about 135–150 words per minute is about 9000 words, so 10 talks would be a “book” (and I think I’ve given hundreds of talks over the years). And similar arithmetic could be done on written papers and articles.

A big deal is summed up by my “one sentence bio”: No one owes more to his research community than I do.

In other words, what I think of as my “actual work” over my life has been done within communities of like interest and directly with major contributions from great colleagues.

Two of my favorite books of “real substance” are Newton’s Principia, and The Molecular Biology of the Cell, by Bruce Alberts, James Watson, et al.

The first is an incredible tour de force in all dimensions and a kind of work for which I’m more than a few notches short. (I do love this book!)

The second book is not just long and great, but spectacularly keeps in mind what its readers need to be “told as prep” — for example, there is a wonderful early section on the chemistry (and atomics) of water which would not be in most biology books, but is included in TMBOTC because most readers will likely not remember the details well enough from their previous chemistry classes. Having a strong visualization of what water does is crucial to the larger understanding of how life works, and this book takes the trouble to make sure you have it in mind as you read on.

The book was first published in 1983, there are now 7 editions (to try to keep up with the explosion of knowledge since then), and the authors periodically meet to work on the needed new editions. (For those who are interested, one of the classic editions was the 3rd …). How they do it is a mystery, even to them!

I think we can see real analogies to computing here — and also see many areas where computing falls short of more established real disciplines of Physics, Chemistry, Biology, etc.

Something interesting and deep to ponder about Biology, is that (a) on the one hand “Biology means variation” — evolutionary processes find many viable organizations of atoms and molecules to make up living entities, and (b) on the other hand, “Biology means speciation” — a larger look than at the variants reveals amazing stable and conservative patterns and building principles, many of the most important are hundreds of millions of years old, and some billions of years old.

The latter means a book like TMBOTC can be written in “only” 1000+ pages and successfully provide a very strong view of the most important building blocks of living entities. The former allows many different examples to be given within the larger framework.

A side comment is to note that the advantage of a physical science is that it has to be based on observations on existing entities. This really helps the start of choosing useful perspectives on the subject.

I think such a book is possible about computing, but no one quite like the amazing Bruce Alberts has stepped forth and mobilized the group of authors needed to pull it off. I think I could be one of the authors — that would be difficult but rewarding — but I couldn’t play Bruce’s role.

In any case, I’m now well into my 80s, and with a not good type of cancer which saps energy. So this is a cop out for me now, but it’s a real one.

A key corollary to the contrast with nature and physical science is that the synthetic — artistic — creation and construction in computing gives rise to way too many perspectives compared to examining nature, and I think most of these perspectives in current day computing are too weak and limited. This makes most books about computing very disappointing to read (yet think about how much work had to be done to make a book at all!)

A “really good book” about our field would need to take a deep and reasonable stand about computing to gain enough context to say much of actual substance.

P.S. I now realize I shouldn’t have mentioned the cancer, and just should have said, “not the energy I used to have”. I sincerely thank the commenters who have wished me well — but let’s get past this if we can: it’s just a “cycle of life” thing that is part of an inevitable process for all human beings.



Does Alan Kay see the educational goals behind the Dynabook concept furthered by AI systems, especially in light of developments like GPT-4? Can this improve computer interaction and computer aided learning?
One of the earliest stated goals — in the late 60s — for real education for children, perhaps helped along with a “Dynabook”, was: to have children grow up to think better than most adults do today.

Most of the time we thought of this as part of a vastly better system and concept of schooling, not as a replacement for what we thought was a currently broken schooling system. This was the case even though I had personally learned quite a bit from reading on my own, I still had quite a bit of help learning to read early, and there were adults around who could be asked questions and often would point to something useful to look at further. A well written book could also do a little of this if the author anticipated some of the questions that the writing would raise, and included further directions as part of the writings.

Another part of thinking about what it might mean to be “better educated” was “epistemological” — one’s way of looking at the world of “out there” and the world of “our opinions” could and should be vastly changed as “education” progressed. An early champion of this idea was Francis Bacon, who called for a “new science” whose methods could help us get around the terrible thinking done by our “bad brains” (what he proposed was what we just call “science” today, but applied to everything, not just the study of external nature, including biology, but also of our own thought processes, and how they could be made less vague and error prone).

We also correlated “civilizations and civilized thinking” with “literacy” — the correlation is definitely there — but it wasn’t until some years later that the breakthrough work of Scribner and Cole — “The Psychology of Literacy” — showed very strongly that literacy alone isn’t enough to do the job — there also has to be present certain kinds of structured schooling in order for reading and writing to do their magic. So, in the early years of the Dynabook idea, we were working on just a part of the idea, that: there is a new kind of reading and writing brought by computing — which includes the old, but adds new very important properties — and there is something like a “new literacy” yet to be discovered, which can have a much more powerful and beneficial effect on civilization than the reading, writing and literacy of the past.

Most of our designs and systems we built for children were not good enough: it took about 25 years to finally come up with a set of ideas and processes that could carry “a new literacy” — and by that time we had gotten a lot more sophisticated about the difficulties of having good new ideas actually having good new effects on children’s thinking and their having fun doing so. This required quite a bit of work with actual schools and schooling.

I think we were able to do an “illustration of concept” — using our system “Etoys” as employed in the Open Charter School in LA in the “noughts” — but the next phases were not done (mostly because of lack of “good funding”).

So an important part of the question here is whether we learned enough to answer the question in any reasonable way.

I think one opinion can be given, and this is that putting children in environments that are devoid of “real and sufficient meaning” — whether in homes, schools, books, or advanced technologies — and especially any environment that acts like a “Skinner Box” is a terrible and unethical idea.

The current round of chatbots are not close to what is needed — and note that far too many adults are no improvement, and are often even worse. And the assumptions behind most schooling today are quite terrible for helping children grow (and with or without technology). Much deeper views, understandings, goals, and visions are required.

After trying to understand this area and find ways to get it above the “What is actually needed?” threshold for more than 50 years now, I still think (a) that Bacon’s insights are valid, (b) that really good schooling with the aid of really good technologies is possible, and (c) that the current confusions about all of this out in the world are worse than they were when the Dynabook was thought up in 1968.

P.S. Much the same kind of response could be given today about questions regarding “What happened to Doug Engelbart’s real insights and goals?”



What is the specific reference for Alan Kay's discussion related to people in business not wanting to hear complaints and why they should welcome them?
I don’t recall saying this.



What is the meaning of Alan Kay's quote "the best way to predict the future is to invent it"? How can this be applied to any field of work?
This came out in a fit of frustration during a meeting in the early days of Parc with the Xerox corporate planning group.

I wasn’t trying to coin an aphorism, but this accidentally happened.

I happen to really like aphorisms — I think of them as a “special domain of prose that is also poetic”. They seem to be trying to tell the truth in a compact memorable way, but I think of them as trying to point the receiver in a direction of more enlightenment.

As with poetry, too much careful analysis can kill the “glow” that is shining in a direction.

Another one of mine that has made its way around the Internet is “Point of view is worth 80 IQ points!”.

(Note that the sign bit is not specified, but needed — but would kill the glow of the larger idea.)

The big idea is to differentiate between “reasonable and accurate thinking” in a random typical context, vs. “the same level of reasoning” in a really strong context. This came from trying to get audiences to realize (a) they were even in a context, and (b) that everything changed when science was invented as a new context to reason within, and (c) that most of our improvements from 200,000 years ago were via gradual cultural learning and teaching, and that in the last few thousands of years were via “inventions of new methods” that could piggy back on our language and cultural apparati.

I once started a talk with the easiest way to predict the future is to prevent it! This is what most people and organizations do. Then went into the aphorism above.

But then — this was at NIH in Wash DC — reminded the audience that the town was full of politicians “inventing the future” (but inventing terrible versions for us).

This is why I think a good aphorism is working when it draws attention to thinking deeper — taking even good ones literally is at one’s own peril.

However — that said — here’s perhaps my favorite one (it’s from Einstein):

As far as the laws of mathematics refer to reality, they are not certain, and as far as they are certain, they do not refer to reality.

This one really gets to the heart of many important matters — far beyond both math and science — and works extremely well just as he stated it.



What does it mean to be an "Alan Kay" programmer? What are some ways to become more creative in programming?
An “Alan Kay programmer” is a person with a few odd, potentially worthwhile, ideas, and deep enough knowledge and skills to work with a group of people with much deeper skills.

For example, this question would never have been asked if I hadn’t started working with people like (especially!) Dan ingalls, Ted Kaehler, etc. I could program pretty well — especially small mathematically dense stuff, that might have to be in microcode — but pretty much everyone I worked with could code rings around me — and I wanted them to! Sometimes I’d code an idea I couldn’t get anyone interested in — knowing that they would most definitely rewrite it to be much better.

Etc.

This group entity idea was not uncommon in the ARPA/Parc community. Even Ivan Sutherland, whom I consider to be in a qualitatively higher level in most things, got a little help to do the amazing Sketchpad, and very much preferred to subsequently set up groups for really hard really interesting problems (and this despite that he was extremely self-contained).

Bottom line: if you can achieve synergy in a group you will usually get exponential factors of improvement.



Do you agree with Alan Kay's comment that a computer science degree teaches students how to write bad code?
I never said that — we should at least pose an accurate question.

One of the (many) things I have pointed out is that a computer science degree is no guarantee of “the kind of perspectives that are needed for (good) computing” — this has parallels with the larger idea that a university degree is no guarantee that the recipient is now at a “what is actually needed” level of learning and understanding.

I’ve also said that I nonetheless advocate going to university if possible, because there are generally more chances to encounter ideas that will trigger real thinking and real desires to learn.

P.S. by accident, and thanks to Richard Strachan, I found the below from Kurt Vonnegut right after writing the above (to me this is priceless!):

In 2006 a high school English teacher asked students to write a famous author and ask for advice. Kurt Vonnegut was the only one to respond - and his response is magnificent:

“Dear Xavier High School, and Ms. Lockwood, and Messrs Perin, McFeely, Batten, Maurer and Congiusta: I thank you for your friendly letters.

You sure know how to cheer up a really old geezer (84) in his sunset years. I don’t make public appearances any more because I now resemble nothing so much as an iguana.

What I had to say to you, moreover, would not take long, to wit: Practice any art, music, singing, dancing, acting, drawing, painting, sculpting, poetry, fiction, essays, reportage, no matter how well or badly, not to get money and fame, but to experience becoming, to find out what’s inside you, to make your soul grow.

Seriously!

I mean starting right now, do art and do it for the rest of your lives.

Draw a funny or nice picture of Ms. Lockwood, and give it to her. Dance home after school, and sing in the shower and on and on. Make a face in your mashed potatoes. Pretend you’re Count Dracula.

Here’s an assignment for tonight, and I hope Ms. Lockwood will flunk you if you don’t do it: Write a six line poem, about anything, but rhymed. No fair tennis without a net. Make it as good as you possibly can. But don’t tell anybody what you’re doing. Don’t show it or recite it to anybody, not even your girlfriend or parents or whatever, or Ms. Lockwood. OK?

Tear it up into teeny-weeny pieces, and discard them into widely separated trash receptacles. You will find that you have already been gloriously rewarded for your poem. You have experienced becoming, learned a lot more about what’s inside you, and you have made your soul grow. God bless you all!"

Kurt Vonnegut



How did Alan Kay recruit and interview his researchers at VPRI?
I just noticed that I have been misreading this question — or it got changed? In any case, my answer below was about my ARPA and Parc experiences. I’ve added a little bit about Viewpoints Research Institute (which I didn’t set up until the early 2000s), and put this below the main answer

Parc grew gradually, in part because — as Bob Taylor told Xerox, he “didn’t plan to hire any ‘good’ people”. This shocked them and he explained that he was only going to hire ‘great’ people, because you can’t really approximate a great person with any number of ‘good’ people, and that ‘good’ people needed to be managed, and he didn’t want people who needed to be managed, etc., and this all meant that it would take a while to staff up Parc (and that computing research at Parc would be relatively cheap (because of the scarcity of ‘great’ people)).

And then a tremendously lucky thing happened. A business downturn put the newly formed “Berkeley Computer Corporation” (BCC - making a new kind of time-sharing computer) into financial collapse, and Taylor was able to hire the entire 8 or 9 people — these included Butler Lampson, Chuck Thacker, Peter Deutsch, etc., (i.e. a whole group of actual ‘great’ people), and in a flash, in Jan 1971, Parc computing research had a critical mass just a few months after starting up.

I was consulting there at the time, and soon after called up my friends at CMU to ask forgiveness for breaking my promise to go work there in a few months — I said, “with Butler and Chuck and Peter and Ed Fiala, etc, we can now do anything”. Part of the key here was the unique brilliance in all directions of Butler.

Still, after this critical mass windfall, what Taylor predicted happened. The hiring process at Parc was slow for both Taylor’s reasons, and that he wanted the existing staff to *all* really want (really really want) the next person to be hired. Taylor’s idea was that it was worthwhile to put a lot of effort into this, because it would pretty much eliminate rivalries when the new person came on board. Of course, we complained to Taylor about the time taken for this, but he was completely right (as usual).

But because of the BCC influx and the few who were already there, lots of really good stuff got started. There are a few interesting digressions about how the small headcount and the way Xerox treated headcount, that shaped Parc culture, but I’ll avoid them here.

As for myself, and the group I put together, I’ll oversimplify (too much) and just say “by literally talking and literally handwaving” (the hand usually had the cardboard model of the Dynabook I had made a few years before after meeting Seymour Papert.

Some of the people who were attracted were already working for others at Parc, and others came as the result of talks I gave around about “destinies”. I was only interested in those who got “stars in their eyes”, and I figured I could help them fill in the rest. This also took quite a while.

However, I got really lucky early on in a casual hallway conversation that Dan Ingalls — who was working for someone else — got “stars in his eyes” and so did Ted Kaehler. The impact these two had (for the rest of my life) cannot be overstated or overpraised. Similarly, John Shoch was a student in a class I taught at Stanford, and he was really interesting. I invited him to do a summer internship and he never left (and wound up being a major factor in networking of all kinds at Parc).

He thought I’d be interested in meeting Adele Goldberg, who was at Stanford doing a project with Pat Suppes, and “to meet Adele, is to meet a truly top-class multidimensional force of nature”, so I was very fortunate to hire her (she also had “the stars”, etc.). Adele brought Steve Weyer along from Stanford.

Diana Merry was a PA for the head of the Physics Lab at Parc very early on, and I happened to see her do something with a time-sharing typing service, that required a “programming kind of thinking”. I said to her, you could be a good programmer, so I hired her away and taught her a few things (which she learned quickly and starting doing helpful things with Dan and Ted). In addition, Diana turned out to be one of the great “bulldogs” of all time — nothing daunted her.

Dave Robson was a student (I think at UC Irvine). He had heard of Smalltalk and got quite far by himself trying to reverse engineer it (just from vague descriptions). We invited him to be a summer intern and basically never let him go back.

So the summary answer is “pretty randomly”, and often via processes like temporary internships that allowed us to see how much “depth and fun” a person might be. There wasn’t a big plan about it. An interesting thing about this group was what I have to call “love” — the people really liked each other, despite the huge differences in personalities and kinds of expertise they had (and this feeling persists 50 years later). This allowed a lot of ground to be covered.

If we cycle back to “ARPA ways” we will find a very loose organization that was not really “hierarchical” nor really “democratic”. Argumentation was almost never for the purpose of winning, but to expose more points of view (this is a hard one to explain to people). Many many times after long “discussions” the group would then defer to the person they thought had the best view of things for the final decision (so “voting” was rare, but things worked).

Bob Taylor was once asked just what he did at Parc (he had been a research psychologist, not a technical computer person). He said “I wanted to get great (and self driven) people, and they tend to be lone wolves and not really manageable (and I didn’t want to try to manage them). I wanted to set up an environment that — when these people needed to cooperate to make much better progress — they would do this”.

This is what happened — it was quite wonderful — people would really be thinking their own thoughts, but would wind up cooperating when that was a good idea. A good example, is that pretty much everyone at Parc could design and make a programming language/OS, etc. (and did). So the agreements there were minimal. However, we also had decided to try to make “100” of anything we invented — and this meant 100 Alto personal computers, the Ethernet had to allow at least 100 taps, etc. This required more cohesion — and this happened — and still without any top down pressure or planning. (It’s also worth examining the ARPA process over some years that created the standards and methods for the Internet.)

In my view (and Butler’s view, etc.) Taylor was a kind of social genius with regard to how to deal with talent, and get it to stay original, yet also get it to synergize.

VPRI was started “AD” (after Disney), and originally was a vehicle for staying with some of my favorite colleagues (this was right after the “bust” of the late 90s, and right before 9/11. The greatest early aid was my friend Bran Ferren (a very special person in so many important ways) who had pretty much been CTO of Disney for a few years. He manage to talk Disney out of a few hanger buildings on Disney’s 100 acre “Imagineering” lot for a new company he and Danny Hillis put together called “Applied Minds”. They liked having us “hang around” so Bran offered us free space if I was willing to site my NPO VPRI there. (This was a great thing and a lot of fun!)

But we had no funding, so we scrabbled around in various ways. “Research means you can change your mind” so I have always avoided working on problems/goals posed by funders, and stuck to this regardless. Kim Rose was completely essential to every part of our process — the highest priority part was that we decided to “never miss payroll” — and we didn’t (sometimes via dipping into my savings, etc.)

The other principle — that I’d gotten from Dave Evans in grad school — was that you should never lie to a funder. His underlying principle was that you can’t do what he called “good science” if you get into the habit of lying. So — he said — you have to start “learning how to tell the truth in very special ways”.

VPRI was on a very small scale for much of the time, but top talent was still involved, so we did what we could do — some of this was to do some major investigations of Etoys in schools, etc.

Some of the additions were really random circumstances. For example, I had spent quite a bit of time off and on in Japan, and even was asked by the Japanese government to help them spend some of their funding they wanted to use to widen their range of talent pool. This turned up a number of “real deals” who had very little connection with the normal Japanese channels.

One was an artist of considerable talent — Takashi Yamamiya — who also had considerable talent with computing, and he was unencumbered with any kind of fear of being weird. He did some really wonderful things with us for a few years, and wound up going back to Japan, got married, and returned to some of the art he had been doing (we miss him very much — he was a great person in all respects).

I will another story — later and elsewhere — about my long time relationship with Yoshiki Ohshima — a real friend and a great colleague. His advisor in Japan brought him to us while we were at Disney with the warning: “Be careful, this young student has strong opinions and expresses them often”. My reply was “Great, I’ll hire him right now!”. Yoshiki is one of the best people I’ve worked with over my career …

I would occasional teach an “Alan Kay mystery hour” class at UCLA, and did a joint one about education, computers, etc with a joint class in the US and one in Japan, using special equipment. We had a variety of interesting kinds of students, but there was one who was *really interesting* — *really really interesting* — who was just sitting in. He had a few insights about the computer tool we were using that were startling, and I invited him to be an intern at VPRI, wound up being his thesis advisor, and colleague, etc. This was/is Alex Warth.

I should mention Bret Victor. I had heard of him and seen some of his stuff — we got together in a park in Palo Alto. About 15 seconds after talking to him, my thought was “Wow, this guy should have his own research group — VPRI is too small a venue for him” — so I hired him as a consultant, until we wound up finding enough funding for YC and Vishal Sikka to set up HARC — and gave him the larger place to play that he needed.

I met Vi Hart — via a lunch we set up — because I wanted to ask her questions about her process of making completely brilliant video explanations of things, especially in math. She was being funded by Salman Khan — which to me was a wonderful thing because I thought Kahn’s videos were almost “anti-real-math”.

It turned out that they had hired her, but weren’t supporting her. My response was “Oh, I’ll hire you right now! Come to HARC and follow your instincts!”. And she did.

HARC did not last long, but we were also able to get Dan Ingalls to move there from Sun, etc.

And so on and on …

A basic idea here is that — although there is nothing really wrong or sinful to improve on things that are around (sometimes if only for safety, this is what you really want to do) — it is also the case that we think our largest thoughts by inventing new contexts that are often very different from those around previously.

This is difficult to do (because you are trying to do this with what you learned in contexts that need to be escaped. By societal definition it is “delusional” and even often considered “crazy”.

And it is very difficult to tell a “new great-contexter” from an actual “crazy person” (or even just a blow hard). Dave Evans liked “other side of lines” types, and he would give quite a few a chance, who eventually didn’t work out. He didn’t care when they didn’t (and ARPA only gave Dave funding, they didn’t control his process), so Dave happily kept on giving people chances — and he wound up with enough “geese who laid golden eggs — rather than lead ones or worse” to staff up a first class ARPA project. (This was very similar to ARPA’s philosophy on the big difference between “error” and “overhead” that I’ve written about elsewhere on Quora.)



Can I become competent programmer if I will start learning Computer Science without attending university lessons, only relying on textbooks, internet resources?
I feel that this is an honestly asked question — and it is a difficult one, in part because it isn’t complete (it lacks many side conditions, such as how you are going to get experience writing programs, and especially how you are going to get feedback to help you improve both your programming and design abilities).

It could have easily been asked the same way with the substitution of “musician” (or “instrumentalist”) for “programmer”.

I certainly don’t think you need to go to university to get fluent at either programming or music — but I also think that both really require good advice and feedback from fluent enough entities (mostly human, though it’s possible to imagine good enough computer helpers for quite a bit of both).

Since most of both will wind up between your ears, the largest part of the process is lots of hard work with good examples over a fair amount of time. With good feedback, you can accomplish a lot.

I don’t know of good computer “advisors” for either (but I have seen low level computer aids for both, that do provide some elementary feedback).

An important caution is that any kind of curriculum — even bad curriculum — is difficult to make, and really good curriculum in most subjects is hard to find, whether from humans, textbooks, or the Internet.

Here’s an example in music. “They” say:

“There are four beats to a measure, and a quarter note gets one beat, a half note gets two beats, and a whole note gets four beats”.

*Musically* this is quite untrue — no musician will play four quarter notes evenly to perfectly divide the measure into four equal time durations. Musicians use another parallel idea called “pulse” (in jazz, called “groove”) that is a human approximation to this, but doesn’t feel metronomic. Singers are rarely metronomic, and singing is a great way to start to become a fluent musician.

(This is much trickier to talk about today than 50 years ago because of the many anti-musical elements that have been forced on the public (like drum-machines, auto-pitch correction, etc.)

This area is much like speaking with literal emphasis on strong and weak syllables. Consider from Shelley:*

If Winter comes can Spring be far be hind?

Here we have 5 pairs of weaker and stronger emphases. This is a kind of poetic meter, but we wouldn’t say it or sing it metronomically. It’s worth thinking about how we would say or sing it to convey the most meaning, while still making use of the nice lilting rhythm as part of the “groove” rather than the control of the “groove”.

The best way to learn this (and music) is to actually listen to others and to do the acts yourself. And try to get someone to help you tune into the meanings and musicalities that are only vaguely implied in the written down forms. (Note how little information is in the script of a play — there is usually less in most written down music.)

In learning to program, you are also using written down forms — and things will happen because of what you write — but the larger object is not a page of code but a running *system*, and the imagination required here can be greatly aided by advice and a friendly helper. Learning to think in systems terms from the beginning will make an enormous difference in how far and quickly you progress in the art of design.

A central idea here is that much learning is really autodidactic, but even strong autodidacts can greatly benefit by asking questions and getting feedback.

Gratefully borrowed from Stephen Fry



I used chatGPT to summarise some of your opinions. What does the real living human think of artificial intelligence condensation of these previously expressed opinions?
Here are two chatGPT summaries, as furnished by Mark Dickson:

Criticism on computer science: Write a criticism of the lack of progress in computer science and software engineering as Alan Curtis Kay would write it.
Alan Curtis Kay, a computer scientist and pioneer in the field of object-oriented programming, has been vocal about his critiques of the lack of progress in computer science and software engineering.

One of his main criticisms is that the field has become overly focused on short-term gains and solving superficial problems, rather than addressing deeper, more fundamental issues. Kay argues that the field has become too focused on the "low-level details" of technology, such as the speed and efficiency of algorithms, rather than considering the larger, more important questions of how technology can be used to improve people's lives. He has also criticized the industry's tendency to prioritize features over usability and design, resulting in complex and difficult-to-use software. In addition, Kay has criticized the lack of emphasis on interdisciplinary collaboration in computer science and software engineering, arguing that the field would benefit greatly from more collaboration with other disciplines such as psychology, sociology, and anthropology. He has also pointed out that the field has not done enough to promote diversity and inclusion, which he believes is essential for creating truly innovative solutions.

Overall, Kay's critiques highlight the need for the computer science and software engineering field to focus on long-term progress and solving real-world problems, rather than being solely driven by the latest trends and technologies. He advocates for a more holistic approach that takes into account the larger societal implications of technology, and emphasizes the importance of interdisciplinary collaboration and diversity in fostering innovation.

2. Write a critique of object oriented programming as Alan Kay would write it.

Alan Curtis Kay, who is considered one of the pioneers of object-oriented programming, has also been critical of certain aspects of the paradigm.

He has argued that the way object-oriented programming is typically taught and practiced does not fully capture the original ideas and intentions behind the concept. Kay has criticized the way that object-oriented programming is often reduced to a set of rules and conventions, such as inheritance and polymorphism, rather than being understood as a way of thinking about problems and solutions.

He has also criticized the way that object-oriented programming is often used to create overly complex and inflexible systems, rather than being used to create simple, elegant, and adaptable systems.

Kay has also pointed out that the focus on objects and classes in object-oriented programming can obscure the underlying principles of computation, and that the emphasis on encapsulation can lead to an over-abstraction of the system, making it harder to understand, maintain and extend.

He also has pointed out that the use of inheritance can lead to a proliferation of classes and subclasses, making the system hard to understand and maintain, and that the use of polymorphism can lead to a proliferation of methods and interfaces, making the system hard to understand and extend.

Kay has also said that object-oriented programming should be seen as a way of organizing and structuring code, rather than as a one-size-fits-all solution to programming problems. He also has said that object-oriented programming should be used in conjunction with other programming paradigms, such as functional programming, to create a more flexible and adaptable system.

Overall, Kay's critiques highlight the need to understand the original ideas and intentions behind object-oriented programming and to use it in a way that is consistent with those ideas and intentions. He also points out that object-oriented programming should be used in conjunction with other paradigms, and that the focus should be on creating simple, elegant, and adaptable systems.

ChatGPT Jan 9 Version.

The first one is more interesting than the second (which includes outright errors (such as what I think about encapsulation), and many other things). Both of the examples (I think) are much more vague than my view of my own writing style, and they have other deficiencies. But, if taken as summaries by another entity (human or otherwise), the vagueness is reasonable to expect.

A key omission (how did the “AI” miss it?) is the lack of central concern I have about *systems* and especially about “what winds up running”, and the “lack of ‘systems consciousness’ in the field today”.

Another is that I’ve repeatedly stated that I don’t think OOP is really a “programming paradigm”, but is rather a universal definition method (a tortured comparison would be to something universal like NAND, and a particular approach to making a computer).

The danger here is that “form very often rules over content” in most of human life, and the current “AI”s have been aimed at mimicking form rather than at understanding content. Given the general state of the public outlook, this is a disaster already well along.



What was the source of documentation and programming knowledge before the internet? How did daily coding look like without having access to Google?
The major HW manufacturers had “SHARE” organizations of their customers to share lore and sometimes code. In the research game, there were papers from the several main conferences and the ACM, etc., that imparted techniques (these were invaluable). For down and dirty, one would read code (and most of the code in the 50s and 60s was still machine code in one assembler or another. Sometimes these were commented (and often the comments were not updated when bugs got fixed, etc.). Typical code looked like this: (This is some of Margaret Hamilton’s code for the Apollo moon mission Guidance Computer.) This had some extreme side conditions, including a tiny slow “odd” computer, and that most of the code had to be put into a very difficult to make ROM (of “core rope” memory). But listings like this were typically what one wanted to look at, especially the semi-readable assembly code columns in the middle of each page.



Are the slides (Squeak image) available for the Alan Kay talk linked in comment?
They aren’t right now, but — without promising anything — something like them might be made available in 2023 or 2024. Here’s the talk in question: (starts at 2:15 if the link doesn’t do it). Smalltalk had unlimited “projects”, which can be thought of as “multiple desktops persistent over time”, where, in each, all the resources of Smalltalk were available *live*. This is a workflow idea that would be very nice to have in modern systems. This (also) means that each project can be used to show an idea — and that (anything) can be interacted with during a talk. Going from one project to another is a bit like going from one slide to another in a presentation system, the difference being that you don’t have to “can” anything and are not limited to e.g. “build” features. I have used versions of this for many of my talks over the years (presentation systems are terribly confining, and “anti-computer”). This one was quite a bit of fun because it uses one of the first Smalltalks (1978, and the one Steve Jobs saw) to have “projects”. I was able to do everything without having to add any new primitives to that Smalltalk (the talk was aided because the old Smalltalk was now running on a machine with lots of memory — so it was possible to include many more bitmap pictures than we had room for back then). However, these freedoms also encourage “Easter eggs” etc., and means that many things — often that seem magical — can be done in the order the speaker chooses on the fly. Today’s presentation systems, on the other hand, have a useful feature that things can be advanced by a single control — such as <space bar> or clicking. This is especially useful when someone other than the author wants to run through the presentation (as in the question here at hand). This would require a few more features in the live Smalltalk. For example, to also “can” interactions with the system, and this would generally require not just capturing (easy enough), but to also allow editing of these (a larger major chore). For this historical system, this would amount to adding important features that we didn’t have or use back then (things would start to get confusing as to just what things were like back then). We would not have to change the kernel, but these changes would be out of the scope of back then. On the other hand, this year 2022 was the 50th anniversary of Smalltalk, and next year is the 50th anniversary of the Alto. Besides museum exhibits and celebrations, the interactivity of the Alto invites an online site so that more people can experience what things were like when the Alto started working in 1973. The (very complete) precursor of Microsoft Word (called “Bravo”) is very usable today — and it would require quite a bit of scaffolding to make enough of it easily learnable for visitors to an Alto website. The old Smalltalks — because they are live programming and development systems — would require lots more scaffolding. And a use of an old Smalltalk — as in the Ted Nelson tribute I did above — would require even more to allow visitors to smoothly run through it. However, it would be great to have all the additional scaffolding done for a historical live interactive website — so we are looking at just what would be good (and possible) to do. We are all indebted to Dan Ingalls for his many versions of the Smalltalks, and now, even more, because he has made a “Smalltalk Zoo” of live interactive versions of many of the old systems. I think the latest version of this is available through the Computer History Museum’s web site — Phillip Remaker found this link to the Zoo: welcome Smalltalk-72 Redux Smalltlak-72 is a beautiful language kernel. Ever since the experience of writing the Smalltlak-76 virtual machine in itself, I have wanted to return to Smalltalk-72 and demonstrate its simplicity with a metacircular interpreter. In other words, to have the Smalltalk-72 interpreter written (and running!) in Smalltalk-72 itself. With this in mind, in early 2020 I wrote a new interpreter for Smalltalk-72 ("Redux") in the Lively Kernel JavaScript development system. At this point, Redux can read in the entire original Smalltalk-72 bootstrap, and it can display a squiral. It has a nearly unlimited object memory and it runs over 500 times the speed of the original Smalltalk-72 (!). You can run it at St-72 Redux If you are serious about playing with this, make sure to copy it to another page, and feel free to contact me about it. https://smalltalkzoo.thechm.org/



What is the history of the first personal computer?
I think we first have to come up with an agreed on definition for “personal computer”. What most people think of is something which cost originally in the range of a television set — but I think we could extend that to include other “personally owned things” such as automobiles and houses (but not Lamborginis or office buildings, etc.). Using this expanded range, we could look at machines like the G-15 and the LINC, etc. Harry Huskey’s Bendix G-15 ca 1956 Mary Allen Wilkes’ LINC computer in her home (invented by Wes Clark) ca ‘62. Please write comments to help the definition, and I’ll try to answer.



What does Alan Kay think of Ruby programming language, since it was inspired by Smalltalk?
As I said to a similar question about Swift: I don’t think my opinion matters.

However, I think the Ruby people did a number of nice things with their design. I think that more kinds of *safe reflection* and *workable MOP*, etc., would enhance the good that is already there.

Mainly though, I think that all the programming languages I’m aware of today — including Smalltalk and Ruby, etc. — have failed to be redesigned and reinvented to reflect how Moore’s Law has affected computing resources.

I think that the support for “Moore’s Law” software pretty much stopped in the early 80s, and the new generation of computerists — many now in corporations — started to optimize and patch the older ever more obsolete forms. Once you’ve put on these hats, you can’t even find your “design hat”.

———— Added Dec 3 2022 ————

This is in response to the 5 current comments, which are all about parallelism. This is about optimizing, and what is needed is something qualitatively different.

Namely, that to actually do a real Moore’s Law for SW it is necessary to exponentially change/raise the level of *meaning* and *intent* that can be given by the designer/programmers. In other words to start building programming systems that know about many things beyond how to give variables values and create simple blind relationships. Such a system would automatically avoid most of the recent large company systems crashes. This is analogous to how an embedded good garbage collector simply does not allow any kind of memory crashes.

John McCarthy wrote a paper in 1958 “Programs with commonsense” and spelled out a lot. He in vented Lisp in order to work on this kind of programming.

Sketchpad was a different approach to the same understanding of the need for a qualitative jump. Basically, in Sketchpad, the programming was done in terms of requirements for what you wanted, and Sketchpad had three problem solvers that could find solutions to the multiple requirements given to it. Its lower level was the first OO framework, but the programming was cosmically aimed at the future where the main programming is all done in terms of *meaning* in ways that are quite separated from any kind of optimizations.



How an object is created? What are the important parts necessary to create an object?
This is an interesting question — I’ll take it to include “… on a vanilla computer with no “object language” already present”.

My view back in the 60s: since any computer with enough memory can simulate any other (including future computers), then a simulated (“virtual”) computer can also define every part of any computation — and this includes not only processes, but what was then called “data”. The simulated computers would be running code of some kind, so multiple processes would have to be protected from smashing each other, etc. So this would be like the dividing of a time-shared computer into separate protected processes that can communicate with each other. The communications would be much safer if they were “requests” rather than “commands”. And so forth.

It should go without saying that any useful computation process should also be able to be instantiated in various ways.

Basically, a software version of computers on a network or bus (and very much influenced by the ARPAnet that was then being worked on). A good larger vision of this is that “everything should be a server”.

This is really easy to do, except for: safety, and efficiency of time and space. So — in practice — a lot of software engineering is required to make the schemes practical and useful. Once done, there is an opportunity to make highly scalable systems from the objects — this is a design task that is quite separate from making objects (terrible systems have been done using object components, etc. — and a few nice ones as well).

You can figure out what to do next to allow “messages”/”requests” to work — note that the form of a procedure call will work — it includes a “target” and parameters for the request, etc., but that the semantics has to be expanded in a number of ways.

For example, the target has to be able to ignore the request (it may be from a dangerous sender), the target may need to acknowledge the request, but delay the actual answer or service, the target may need to gather knowledge over time to help it serve, and so forth, the target may needed to remember and undo its services, … A really simple one is that standard subroutines are set to establish a “history chain” of calls, that then needs to be unwound in reverse order — this has to be modified at several levels — to allow other processes to run in various semi-concurrent ways — to not require a history chain (maybe something more comprehensive. etc.) The instantiation of procedures has to go beyond re-entrancy and recursive use, etc.

The basic idea here is to think long and hard about *systems* and what is needed to make them, and then to make a basic “node” that can do any and all that is needed.

A more sophisticated scheme could be to send requests to a “broker” process that can find the best servers (take a look at what LINDA does, etc.). This could be “call by *need*” — in other words, you don’t know the name of the service, but you can describe what you need, and the brokering processes will do the finding for you …

Bottom lines here are that (a) “something vaguely like an object” can easily be made from any vanilla computer resources, (b) most such things in most current “OOP” languages are neither very good renderings, nor is their use enlightened, (c) in the current day of many doublings via Moore’s Law since 1966 (about 35 doublings perhaps), a version of OOP is needed to deal with current scales and needs (d) having great objects, does not say all that is needed about how to organize them — this is one of the greatest ongoing issues (I.e. “OOP” is not really a programming paradigm, but a universal process definition scheme).



What does Alan Kay think of Neil Postman's invited talk at Apple in 1993, and are his main arguments even more pertinent & relevant now 30 years later?
I think the referred to talk was:

(there is some noise on the left audio channel — balance to the right to help.)

Neil and I were good friends. I set up this talk and invited him to have a go at a room full of rather set opinions.

He once told me a story that gives a lot of insight into him and his approach. When Neil was in grad school in the late 50s, he and his fellow student Charlie Weingartner got interested in Marshall McLuhan and started following him around to his talks. Neil said they soon noticed that McLuhan did not respond to questions or arguments from the audience. Instead, he would say “how about this one?”, and come up with another of his “from right field” weird cryptic aphorisms. Neil and Charlie eventually realized that McLuhan didn’t care whether anyone agreed with him — what he cared about was whether the audience was actually thinking at all! He viewed his purpose as a wake-up call.

It’s also important to understand that Neil Postman not only could really write, but that he was extremely conscious of his writing process, control of rhetoric, and the vast differences between oral and literate societies, world views, and expressiveness. The result was both some of the most readable serious prose during his time, but also his oral talks were actually written beforehand, and so well that he could read them and still be “oral” for his audience, but “literate” as to depth.

In my opinion, this made him the best, most accessible, and most important public gadfly of his time. He could write “Education as a subversive activity” at the beginning of his career, and a bookend at the end: “Education as a conserving activity”, and make both views be even more valuable together and supporting than their initial seeming opposition.

He could watch television, and condemn it without being a hypocrite. He could inveigh against “technology” in deep ways, while still using it, or becoming a Luddite. Neither “conservative” nor “liberal” terms fit Neil well.

He was essentially an Artist reporting what he saw and thought about during his life. I found him to be one of the most endearing, genial, and civilized people I’d ever met.

He knew — as did McLuhan — that, down deep, public utterances are at best “commercials” and “carrots” for the much harder and longer pondering that most of us need to —have to — do to start grappling with the real issues. I miss him very much.

So, with the above as context, what do viewers think Neil is trying to get the audience to think about?



Do you think that Alan Kay was more focused on children than education? Why?
H.G. Wells said it better than I can: “Civilization is in a race between education and catastrophe”. His notion that education could be a deterrent to catastrophe was part of the age old “nature vs. nurture” questions that gained ground as Anthropology and Psychology delved deeper into trying to understand humans, their cultures, and behaviors.

Children who are deprived of a human cultural surround when growing up — such as “Genie”, who was raised in a closet with no contact, or the various “wolf children”, who were raised with animals — do not develop normally. Much of what we consider to “be human”, such as language, etc., are not embedded by genetics in a child’s brain at birth — instead, genetics embeds “desires, interests, propensities, etc.” that draw the child’s attention to specific examples in the environment to be learned as behaviors. For our species, this process is so strong that it forms the “prime idea of anthropology” — a child at birth can be taken anywhere on earth to another culture, and will grow up as a member of that culture, thinking the thoughts of that culture, being that culture, etc.

A key idea here is that we humans treat what is more or less constant around us as “normal” — and, critically, we shortcut “normal” in our minds to “reality”. Because our nervous systems are tuned to differences, we pretty much disappear “normal” as an idea that is relative to surrounds, especially cultural surrounds, and simply take most of our circumstances as “real” rather than “mental constructions”. What we believe mostly determines how we see and interpret the world.

The deep “nature vs. nurture” question is: to what extent can our impulses from genetics be constrained and augmented by learning — and especially: early learning? For example, could the US Navy SEALS slogan actually be true? I.e. “Under pressure, you don't rise to the occasion, you sink to the level of your training”.

There are many examples where training/education does augment — even replace — some of the behaviors arising from our genetics. But could this be enough to make important differences in larger human societies?

I don’t know the ultimate answers here, but have long been interested in the wide spreads of behaviors that are associated with “humans in the raw” and “humans in society”, and especially the extremes, which often cut both ways. An often examined example is the transformation in the 20s and 30s — in less than a generation — of Germany from one kind of stability to a very different dangerous society — and then, after WWII, back to a very different kind of system from either.

Readers who like to think in terms of systems will reflect that examples like these don’t necessarily mean that every individual in e.g. Germany, was swung from one extreme to the other and back again — but that the societal system — the overriding culture — manifestly did (and rather quickly).

Another systems perspective of this has to do with thresholds for indiviual elements of systems that are needed to have the larger organization be stable enough to “work”.

A famous reply by Jefferson in response to whether democracy could ever be made to “work” was: “I know of no safe depository of the ultimate powers of the society but the people themselves; and if we think them not enlightened enough to exercise their control with a wholesome discretion, the remedy is not to take it from them but to inform their discretion by education”.

I think it’s fair to say that this central idea — that education in a democracy, above all has to raise citizens above the threshold needed to participate — has been absent in US education for so long that the education establishment doesn’t realize it ever existed as an idea and goal.

This doesn’t mean teaching a party line, but it does mean teaching e.g. how to argue with others in such a way to illuminate difficulties to aid solutions (as opposed to “trying to win an argument”).

This idea about arguing to make progress was deeply embedded in the ARPA research community, and was a vital part of its success: it allowed the widest view of ideas and issues to be brought forth and cooperation — and damped out fruitless contention and competition of egos. (The ARPA process apparently started in the radar project at MIT during WWII.)

An interesting question beyond whatever the needed educational/training processes might be, is “when in life can they best be taught and retained?”.

Most indications point to childhood, especially the first 7–10 years — this is when many mental commitments to “world view” (aka “reality”) are most strongly made.

My own interests in all this went from “vague understandings” to “vivid thoughts” after meeting Seymour Papert in 1968, and seeing what he was trying to do with children and thinking, via giving them a rich environment to play with ideas/processes through the interactive LOGO computer system.

A vital point about what Seymour was doing is that — even though he was an excellent mathematician and came up with many mathematical examples for LOGO — his main aim was “world view” (a fancier larger term for this is “epistemological surround”).

He wanted to find ways to qualitatively enrich what children grew up taking as “reality”. When he said that “the best way to learn French is to grow up in France”, he didn’t just mean the language, but also the culture (metaphorically meaning: modern ways to view and think about our situation).

This strongly echoed Maria Montessori, whose main underlying aim was the same — she went about it by pointing out that “you can’t learn the 20th century in a classroom”, but must somehow be able to grow up in it. She decided to embody this in the fabric of her school to make it *be* “the 20th century”.

There are literally many more critical ideas that are part of this: too many for a Quora sized answer.

A big one — in size and amount of understanding required — is how we are affected by “media” — intermediary things, tools, processes, ideas — that are part of our environmental surround. McLuhan quipped “We become what we behold!” via intermediaries — in other words, there is nothing neutral about tools/communication systems —what counts is what changes happen in our brains to learn anything, especially tools and media — but he said almost nothing about our difficulties in really beholding.

For example, what we most learn from a tool — such as a hammer — is that we “can hammer” and “how to hammer” — the pragmatics of “hammer” — but we have great difficulties in coming up with larger ideas about “hammering” (i.e. don’t hammer others when you get upset, etc.). Doug Engelbart realized this in his first papers about “augmenting humans”: that providing humans with ever more powerful tools is a disaster — between the human and the tool has to be education/training, not just in use, but in “use for what?”.

This means that when a new powerful tool/medium comes along, the burden is not just how to teach the use of it, but the “use for what?” and the “what is it really?” of it. In other words, we need to learn how to insert barriers that prevent simple-minded pragmatism between us and our ever more powerful intermediaries.

Vi Hart: “We must ensure that human wisdom exceeds human power”.

Much of modern science is about how and why to go past “the world is as it seems” to understanding that the world is “mostly not” as it seems, — much of what’s important is “behind the curtain” — and thus everything needs to proceed much more slowly and deliberately in order to get more valid views to “make the invisible more visible”.

Thus the main “world view” of science strongly obtains in most other walks of life as well, especially in matters of society, its “discretion”, and how to raise it. The perspectives of the new branch that is systems science, can be particularly helpful.

We should be able to see that simply “teaching and using tools” is not just futile, but very dangerous with powerful tools (this includes not just recent tools — such as networked personal computing — but also older tools — such as reading and writing).

So, in the race between education and catastrophe, we must learn to deeply understand catastrophe, education, intermediaries, society, humanity, and children, and then take major actions for all.



What did Kay mean by “I wanted to get rid of data. The B5000 almost did this via its almost unbelievable HW architecture.”?
One way to think about classical “data” is that it is something passive that can be munged by an assignment operation. Procedural languages generally have variables and data structures that can be munged — so-called “OOP” languages generally permit “setters”, which also allow munging.

The Burroughs B5000 computer with many very different features was revealed in 1961. It was designed to efficiently run Algol 58, and much of Algol 60, more or less directly. Much has been made about its hardware stack, but this was just the tip of the iceberg. The most interesting features of the machine had to do with safety of both state and processes, and especially how memory was structured.

A word (48 bits) in memory was marked as either a number or a “descriptor” — a protected pointer to hardware or software entities. In “regular mode” these pointers could not be forged. One use was to implement virtual memory, via a “presence” bit in a descriptor. If on, the core address of the segment was in the rest of the description, if off, the rest of the descriptor had the disk location of the segment, and automatically brought it back in.

An important idea is that the “byte-codes” of the B5000 did not contain storage references and could not directly see memory. So the OS (also written in an extended Algol) would *grant* an environment for code to run that the code could not hurt. All this was the basis for “capabilities”, and made the B5000 almost uncrashable in practice.

I should also mention that the machine had at least two CPUs and an automatic process switching mechanism.

A very interesting feature was how the B5000 handled code such as

A <- B

The byte codes looked like

<name call: A> <value call: B> <operation: <- >

The first syllable wants an address for A on the stack. Next a value of some kind is put on the stack (B stands in for any expression). Finally the <- is done to the top two items in the stack.

The interesting wrinkle is that if A contains a procedure descriptor, that procedure will be called with a flag indicating that a *name* should be the result. I.e. that the A is on the left side of the assignment statement. (This later led to the idea of “L-values” and “R-values” in languages like BCPL).

This meant that the B5000 could hold off munging and protect structures at will. This was pretty neat!

I thought about this a lot as a grad student. One example, that couldn’t be done “right” with this scheme was to model sparse arrays because the name call was early and separated. This eventually got me to abandon the idea of “<-” as an operation, and to think of it as a kind of parameter:

A <- B should be something like A(“<-”, B) … which (wonderfully could actually be done in Algol).

In other words, the idea is to have encapsulated entities which can only receive messages as requests (not commands), and they get to control what state and state change means. I thought of this as abandoning imperative programming for “request programming”. So the entities would be much more like “servers” than passive vulnerable bits.



Why is school & college the way it is, limiting most children's potential?
This is why good questions are very often better than answers—especially when it is about systemic problems.

Sometimes, when in this position, just working on “how things should be” will get further than trying to figure exactly how things got to be so bad.

So, let’s turn the question around to ask: “How can we best help children to maximize their potentials, while using the best conceptions of education we can come up with?”

Again, this doesn’t have complete crisp answers, but it does give us some starting points.

For example, a really good starting place would be to have society really understand what Maria Montessori wanted to do and did to help children — in part because she took on our second version of the question as her life’s work, and — as an authentic genius — came up with many great insights and starting places.

I think reconciling an enlightened understanding of the best world for a child with best conceptions of what adulthood should be like is a very good way to start working on the future.

It is hard to see how a start can be made without a combination of idealism and realism that is the heart of modern thinking.



What is the major obstacle in teaching computing in general and coding in particular in primary and secondary schools?
This is a good question that can lead to even better questions.

First, I think the problems with most of education — including about computing — are systemic, with many parts failed or missing. In addition, the current failed system is large and active about protecting what’s left.

Second, “coding” today is both a trick word that has been co-opted for a too tiny subset, and its larger meaning from the past misses what the larger meaning of “programming” used to signify.

Curriculum needs to not just be better, but qualitatively different. Like writing, it is a lot of work to even produce a flop, let alone something really good.

Of the many problems of curriculum, one is the strong tendency today is not include any important matters where it is felt that “the teachers are not up to it”. (I’ve been in many meetings where good ideas never made it for this reason.)

Most teachers are working like mad, and are in very difficult situations regarding students, cultures, management, politics, etc. before even grappling with 21st century high knowledge. And one of many causes of these problems is that today’s teachers are generally themselves the product of the failed system.

However, the state of the teachers are a critical part of any improvements that are attempted to be made.

Human beings are set up by nature to learn from their cultures: however, writing, math, science had to be invented, etc. In many senses, school exists to teach the hard to learn stuff that doesn’t arise naturally in cultures via genetics and traditions. One of Montessori’s great ideas was to make the whole school the embodiment of the way the outside culture should be by embedding it in the processes of the school (rather than trying to teach the ideals directly in a classroom). I think this is a very good idea!

However, no teachers were more trained or deeply vetted than Montessori’s. Right now our society does not actually value education, and is not up to supporting it by vastly improving teachers in every way — including paying the good ones today what they are really worth.

If we take “computing” seriously — and I do — then in its relative youth, it needs analogies to similar difficult fields to help see how it should best be taught. Two that can be helpful are “writing, reading, literature”, and real mathematics fo the general public.

The first of these is easiest to discuss. School “likes output from students”, and it has shown that it doesn’t care much about the quality or relevance of the output. Processes have tended to shrink to the easily testable output that the school systems feel comfortable with.

Frank Smith points out that *ideas* are at the roots of writing and reading, and these are not only the core of “literature”, but are larger in many ways than current literature itself. For all these with regard to children, we need to look at children’s books of ideas that are not only readable, but upliftable. If we look at ideas in the 19th, 20th, and 21st centuries, we need to recognize that many of the most interesting and central involve science, systems, mathematics, technology, and improving how to think. (It is hard to find children’s books with much real content about any of these — or to find teachers.) On the other hand, one of the most important traits of science, systems, math, etc., is that they are *arts*, and thus defunding the arts to teach e.g. science is robbing Peter to pay Paul, missing the point really badly. Without the ability to sense and appreciate and make beauty, much of modern striving is a dead end.

It’s worth thinking about writing for a moment. It is a big deal to learn how to write sentences, but an enormous error to confuse this with “writing”. What’s most difficult in writing is to create a larger system of connected ideas while retaining something graceful in the small.

I think we can apply this directly to thoughts about computing and its learning. Computing is, over all, really about systems, not computers. And learning about systems, and thinking in systems terms, is right up there as a new standard subject for all children on their way to adulthood. I think we can put “ideas in systems terms” into our version of Frank Smith’s core ideas. Now we have to think about — and in the world of how children think about things — what should “reading” and “writing” be?

Good answers will require not only a revolution in public schooling, but a revolution from what is now a kind of pop culture around computers, that much of the time can’t see the systems for the bits.

Lots of real deep work ahead.



What was the staff size at Parc?
Parc had 4 labs: computer science, systems, physics, optical. The ARPA culture that formed most of the computer part of Parc pervaded most of the first two, and none of the others. My group and I were in the Systems Lab and both Systems and Computer Science labs worked closely with each other (the reason for two labs instead of one was mostly a funding ploy by Bob Taylor).

A lot of what Parc is known for was done in the first few years by both labs. Butler Lampson — a principal figure at Parc — has estimated that the number of researchers was about 25. The balance of researchers to support staff at Parc was about 1:1.

An important historical fact was that Xerox did not allocate headcount on the basis of budget, but used “slots” (which were like gold). Whenever a slot became available we would try to find and hire a star. This led to a very unusual (and effective) makeup of lab denizens!

This is one point in time. A few years later I could count about 40 or so researchers.

The other two labs were similar in size and support. A simple calculation shows that Xerox paid for all of Parc many times over with just the profits from the laser printer alone …

A possibly interesting side note is that the salaries were not high at Parc, but that the average yearly hardware support for each researcher was very high: much more than their salaries. This is because the aim and reality of Parc was to spend enough money to “live in the future” by anticipating what would be possible with Moore’s Law in 15 years, and to duplicate those conditions for the researchers by inventing and building that future. This was relatively expensive per person, but the low head count made it affordable.



Could Xerox PARC have been done remotely with today's technology (not necessarily methods)? What does Alan Kay think about "remote work" for collaborative research?
This is an interesting — and tricky — question. For one thing, attempts to create “a Xerox Parc” in any form have not been successful. I think the previous ARPA community was critical to helping shape most of the researchers who formed Parc, and that many of the critical researchers at Parc were not replaceable.

For another, among many aims, Parc was part of a tradition of research to create “remote work for collaborative research” predating Parc (e.g. Engelbart) — and Parc had several active projects around this idea, as did Negroponte’s Arch-Mac group at MIT.

The “today’s technology (not necessarily methods)” part is also critical because serious technological resources are required to even have a chance. For example, while I was at HP and Disney I got to see and use HP’s “Halo” collaboration technology done originally for Dreamworks to do live media movie editing between the UK and LA, and then was able to late see Cisco’s experimental collaboration tool (which had been used between Hong Kong and LA to build itself). And later, to see Ishii’s “Liveboard” inventions, Applied Minds’ schemes, etc.

Another tricky part is to notice that what we were working on at Parc would have been easy or not needed at all if the technology to create it remotely had existed.

The best answer I can come up with here is that some of what made Parc work could have been done remotely — but I don’t think all. I think much of the actual fabrication of SW could have been done remotely — but the *main work* of the SW (the problem finding and design, etc) would have been difficult to impossible remotely.

Part of the problem of remote collaboration is to solve the problems of both “getting things done together” and “arguing without alienation” — especially in groups — mismatches. I think the HP and Cisco technologies did a very good job of transmitting enough “presence” to get a lot done, and also to allow for a fair depth of arguing.

There have also been enough experiments in eye contact and how to do it, to go further. There are deeper problems of trust and trust building that are really hard to do with remote technologies, and also problems of “intimidation” and social coercion that need to be looked at. (For example, take a look at “focus” at a live concert or theatre vs. canned versions of the same thing, and think about intermediaries to these experiences.)

Both “goofing around with each other” and the nature of the “goofing” were critical to Parc, and quite a few of the most important visceral processes are not readily subsumed by any kind of technology.



What are some notable reimplementations of SketchPad by Sutherland?
I don’t know of any reimplimentations — but I wish there were.

Alan Borning’s “ThingLab” thesis at Stanford/Parc improved on a few of the Sketchpad ideas, but did not cover the whole territory. He is also responsible for numerous constraint solver systems — including Cassowary, which is the heart of many of the mechanisms for doing automatic layout, especially of web pages, etc.

Sketchpad was one of the deep roots of CAD — I don’t know of any CAD systems with the “deep feel” of Sketchpad, but there is lots of “surface influence”.

The bridge demo in Sketchpad has been redone a few times — I have used a version by Yoshiki Ohshima and Vanessa Freudenberg in a number of talks now online (one was the “Turing Centenary” talk).

Brian Silverman has wanted to emulate the TX-2 to run the original Sketchpad assembly code, but for a variety of reasons, including the ever changing TX-2, this has not happened.

Ivan’s thesis is likely clear enough so that a reengineered version of it could probably accomplish a lot. This would be a good target for an MS degree I think.



How did Alan Kay decide to take inspiration from a biological system - (operations of a biological cell) to an (human) engineering system namely object oriented programming…was it just random inspiration or was he intentional about it?
The actual history of the particular set of inspirations is given in “The Early History Of Smalltalk” written for the ACM “History of Programming Languages II in the early 90s.

Many of these kinds of decisions depend on how loosely or strictly one sees analogies (I’m on the loose side, but short of being a mystic).

One of my major concentrations in undergrad was Molecular Biology (the other was pure Math). In 1965 Jim Watson published “The Molecular Biology of the Gene”, which among other things had one of the earliest assays of a living organism. And of course, I learned the biochemistry of the level below life.

I was a working programmer at the National Center for Atmospheric Research, but was not “thinking systems” at this point — I was basically a “data app” programmer.

The next year I went to grad school, and accidentally wound up an ARPA computer research department. ARPA was all about systems and the first one I saw shocked me into waking up and looking around. This was Ivan Sutherland’s Sketchpad. A week later I had to learn the up to then unknown language Simula. Then I learned about the plans for the ARPAnet. Etc,

As I recount in the history I was finally presented with enough examples to get me thinking about how to handle complexity in somewhat the way Biology has done it since the Cambian Era: by eventually being able to combine billions, even trillions, of components using universal building blocks (for example, our human bodies start with one cell and one set of DNA, and wind up with about 10 trillion cells of about 250 cell types (still with the same DNA). Each cell uses quite a bit of “overhead” to maintain itself, and contributes a little to the whole organism.

When the practical world of computing is factored into this (especially in the late 60s), what one gets is a vision that is a bit more comprehensive than either Sketchpad or Simula, but more or less along similar lines. This requires omitting many interesting other things that Biology has to offer, but you still get a lot.

Finally, I should mention that I was not trying to invent a programming language, but rather an extensible kernel — a definitional scheme — for specifying universal “cells” which could then be organized into a desired system. For example, Smalltalk is actually the *idea* of definition, it doesn’t tell you how to program. The “image” — an example system that is shipped with Smalltalk is but one of many ways to use the definitional idea.

(In practice, we should have explained this distinction better.)



What can you say that everything you see in nature is governed by the patterns based on mathematical concepts and theories? What is it about mathematics that might have changed your thoughts about it?
IMO, this question seems backwards — it is far more likely that nature is just the way it is, and we are the “pattern assigners” to the phenomena we can detect. I say “IMO” because the overall question is more philosophical than scientific (which IMO is a higher level of opinion than this).

We call the patterns we use to reason with *mathematics*. We like stories that seem to have some coherence, and the deductive/inductive framework helps makes better stories. Explanations of phenomena are never absent from the thousands of cultures that have been studied.

The big shift from “math thinking” to “science thinking” started to happen when phenomena and their explanations were looked at much more closely, and more evidence of special kinds in phenomena started to be demanded.

For example, there is no evidence that “nature is governed by the patterns based on mathematical concepts and theories”.

Two quotes frame up this point of view. The first, from Kenneth Clark: “People in the Middle Ages were as passionate about Truth as we are, but their sense of evidence was very different” — and from Albert Einstein: “As far as the laws of mathematics refer to reality, they are not certain, and as far as they are certain, they do not refer to reality”.

People who like pondering ideas like this should get and read the well known physicist Eugene Wigner’s essay “The unreasonable effectiveness of mathematics”.



How did Xerox PARC invent the mouse?
Xerox Parc didn’t invent the mouse. The original mouse was invented by Doug Engelbart and Bill English in 1964 as part of investigations into how to use pointing and gestures as input to interactive computing system. This uses an old idea — that dragging two disks at right angles will produce differential spins of each disk depending on the direction of drag. In this case, each disk is attached to a potentiometer that will change the voltage though it at the same rate. This can be interpreted as distance traveled in x and y. This worked just well enough, and mice were relatively easy to make. Engelbart and English soon moved to a smaller 3 button mouse which was used for the NLS system of the “Mother Of All Demos” in 1968. For the Alto at Parc in the early 70s, we initially wanted tablets, but good ones were still quite expensive to make. We looked at the NLS mouse for a variety of reasons (another story) and wanted one that was of higher resolution, and that was small enough to allow drawing with it. The first Parc mouse used digital shaft encoders attached to small ball bearings, and IIRC had a resolution of over 200 to the inch (250?). I really loved that mouse. However, it would get dirty, so other attempts were made, including a very successful “ball mouse”, and an equally successful first optical mouse (Dick Lyon). Later, at Atari, we did a “flea” which was a very small optical mouse (it worked really well).



What became of the work done at VPRI?
The website for papers still exists:

Viewpoints Research Institute
After 16 years of continuous research and important contributions toward its mission - "Improve 'powerful ideas education' for the world's children and to advance the state of systems research and personal computing" - Viewpoints Research Institute concluded its operations at the beginning of 2018. This site remains online for historic and archival purposes to preserve the documentation and writings produced during the years Viewpoints actively conducted its work. What is Viewpoints Research Institute? Viewpoints Research Institute (VPRI) is a 501(c)(3) nonprofit public benefit organization incorporated in 2001 to improve "powerful ideas education" for the world's children and to advance the state of systems research and personal computing. Many of our themes co-evolved with the inventions of networked personal computers, graphical user interfaces and dynamic object-oriented programming. Our globally dispersed research group comes from a tradition of whole systems design developed by ARPA in the sixties and Xerox PARC in the seventies. Our ideology is motivated by user-centered systems design. Using this vantage point, our group invents computing technologies, content, curriculum, graphical user interfaces (GUIs), programming languages, implementation systems and processor and memory structures. Where does Viewpoints focus its research? Our research covers four areas: Teaching and Learning Powerful Ideas, Powerful Ideas Content and How to Represent It, User Interfaces that Aid Learning and Doing, and Inventing Fundamental New Computing Technologies. How do we view "Design for Learning"? We want to help children develop real fluency in many important areas of learning, including thinking, math and science. Each of these subjects is outside "natural learning" (such as learning to walk and talk). Quite a bit of time and energy needs to be spent to gain an above threshold fluency. There are interesting similarities to art, music, and sports, each of which also requires quite a bit of time and energy to gain fluency. These arts could be termed "hard fun". Mathematicians and scientists know they are doing art and hard fun as well. "Thinking" is a higher category than "just" math, science, and the arts. It represents a synthesis of intuitive and analytical approaches to understanding the world and dealing with it. Our Researchers and Research Community In the Spring of 2016, VPRI joined as part of Y Combinator Research's (YCR) HARC -- Human Advancement Research Community. HARC was founded based on conversations between Alan Kay and Sam Altman of YCR. harc.ycr.org Although VPRI remains a separate non-profit organization, HARC's mission to ensure that human wisdom exceeds human power, by inventing and freely sharing ideas and technology that allow all humans to see further and understand more deeply resonates deeply with VPRI. Joining this larger research group to share ideas, collaborate on projects and develop new technologies allows for a broader reac
http://www.vpri.org/
You can also look at “Croquet.io” for ongoing work. Also take a look at “worrydream.com” for Bret Victor’s work.



What was the oldest computer or device you remember using? How does this compare to the machines you use today?
The first programs I wrote that ran on a computer were done ca 1962 in the Air Force for an IBM 1401 with 8k 6 bit characters.

However, while in grad school a little later I got to write a few programs for the Lincoln Labs TX-2, some of whose HW dated to ca 1956.



If Alan Kay was doing a PhD thesis today, would he write all software from scratch? Is that more than what is actually needed to get the point across?
Originally Answered: If Alan Kay was doing a PhD thesis today, would he write all software from scratch? Is that more than What Is Actually Needed to get the point across?
Ivan Sutherland once pointed out that “A thesis is something 3 people will sign”, and Dave Evans saw getting a PhD as a “necessary evil”, a “union card” that you needed to get but shouldn’t spend much time on. These were two of the three signers of my thesis.

Both of these computing greats were talent oriented, and wanted to get as many talents of as many kinds into the research community as possible. Here “What Is Actually Needed” was “a significant advance in computing without using up much future research time”.

They also encouraged consulting “because that is the real world” etc.

One way they did this was to encourage and allow students — if so minded — to choose and work on their own projects, to see how they functioned as “real researchers who didn’t yet have a PhD”. These projects could be big ones or smaller ones. They wanted you to finish the latter, but didn’t require it on the big ones.

Dave Evans got me a job consulting on a small desktop computer project at an aerospace company almost immediately after starting grad school, and several years later when I asked Dave what I should do for a thesis, he said “Just write up what you are doing”. That turned out to be a very untidy mishmash of ideas, but many helped later on at Parc. “Three people”, including Dave and Ivan, signed this, and I didn’t look back.

I did write code as part of this, including microcode, but the best code was written by better coders later. The “thesis” was “It is now time and very possible to invent an interactive graphical personal computer”, and the mishmash was a lot of the things that needed to be done, and how they could be approached, and sometimes with real examples.

I think it was signed because — as rough as the mishmash was — I had made my point.

On the other hand earlier, Ivan had sole access to a supercomputer 3 hours a day and programmed and designed a tour de force — Sketchpad — in a year. He still used some tools done by others. He really made his point (I think much better than I made mine) — but both points were made.

A side comment: If we look at the question, and take “what is actually needed” (WIAN) as a threshold that implies a qualitative difference between below and above, then it seems clear to me that we only have to do “all software from scratch” if that is part of the point that needs to be made (and in a few cases it might be, or there might be no other route.

Another comment: It’s worth noting that the “before computer” standards and norms don’t really work well for many computer PhDs. One facet of the field can be looked at as “math”, and so at least part of this can be done solo. Another facet can be looked at as “design” and can sometimes be done solo (but good design often requires testing and iteration, which often requires building. And then we have that much of computing has to do with real world (and real time) issues, which do require not just building, but often a fair amount of engineering (in my experience, the best stuff has a combination of invention and engineering and iteration).

But grad schools generally don’t cater to the actual processes of computing. This puts the “single contributor” idea of a PhD in question.

Another problem is that — today especially — most grad students do their computing on vanilla laptops etc, which means they are computing in the past, and also competing with highly optimized SW done by teams. In my era, we used or built “time machines” — supercomputers etc — that allowed a single user to “compute and think in the future” (that would come via Moore’s Law). This was one of the keys for the best stuff in the 60s and 70s.

So: ways to do a real PhD as part of a large real invention team would help. Being able to use supercomputer cycles to avoid having to optimize would also greatly help. Getting universities to revamp their theories of the field and what a PhD should mean would really help. Etc.

Also: see the comment by Jennifer Ramirez below, and my reply.



How does Alan Kay think Ivan Illich performed as a problem-finder?
I think anyone interested in education past, present, and especially future should read Illich’s books — particularly “Deschooling Society” — for an early deep critique of standard schooling — and “Convivial Tools” for crucial perspectives on our tool-based species.

I would rate him quite high as a “problem finder”.

Illich’s own words about the problems of schooling:

The pupil is thereby "schooled" to confuse teaching with learning, grade advancement with education, a diploma with competence, and fluency with the ability to say something new. His imagination is "schooled" to accept service in place of value. Medical treatment is mistaken for health care, social work for the improvement of community life, police protection for safety, military poise for national security, the rat race for productive work. Health, learning, dignity, independence, and creative endeavor are defined as little more than the performance of the institutions which claim to serve these ends, and their improvement is made to depend on allocating more resources to the management of hospitals, schools, and other agencies in question.



What does Alan Kay think of James Kunstler’s “The Geography of Nowhere”?
Summary: worth reading overall, could be more aimed at *why* people/society are the way they are rather than just the nutty, even crazy things people have done and are doing without the notion of consequences.

This book is on Kindle (for $0) and I encourage taking a shot at it.



Why did so many of the innovations at Xerox PARC not make it into mainstream computing until several decades later? What lessons can future innovators learn from this?
It’s worth looking at the history of the Ethernet. It was of interest to the outside world because there wasn’t a workable solution to “local area nets” in the 70s, and the Ethernet worked nicely (if a bit oddly to conventional thinking). Still, it had a pretty straightforward path involving other companies (e.g. DEC) and making a new company (e.g. ThreeCom), and doing considerable engineering on connectors, speeding it up a little, etc.

The GUI was even simpler: to see it was to get the basic idea — a bitmap display and pointing device more or less required, along with enough memory and processing power to do the rendering. This required an executive with influence (Steve Jobs) to see the power of the idea (this happened 6 years after the invention of the Alto), but the Lisa had a good version of the GUI about 3–4 years after Apple decided to go for it.

It’s interesting to note that these two inventions did not have an alternative viable form, so there were niches they could fill. Note that both got easier and easier to do as Moore’s Law expanded.

As an extreme example in the other direction, we could look at some of the programming practices are Parc, quite a few of which have not been generally adopted even to this day. There is a lot of evidence that the main reason for the difference was that everyone in computing already thought they knew how to program, but it was clear they didn’t know how to do a LAN or GUI.

My bottom line is that I think there is a general aversion to really learning new things in the outside world, so: NIH! partly from misplaced pride, partly from faddism, and partly from laziness.

The ARPA/Parc community was generally better at being willing to learn and use most already existing good ideas, and especially to share new results. I think this might have been partly due to the long history of this community, which stretched back to the WWII years.



Why was Charles Babbage unsuccessful in creating a working mechanical computer, when the Jacquard loom had already been invented?
It’s interesting to compare the recreation of “the difference engine” with the Jacquard loom. The approach to engineering and practicality could hardly be more different. The difference engine was much simpler than the analytical engine, but could barely be cranked because of the amount of backlash in the scheme (I’ve cranked both recreations …). Of special note is to look at many chains of base 10 gears in precise alignment of the engine vs. the “rattley ad hocness” of the wires and guides of the loom (below). Jacquard’s approach was to “let things flop around” — basically to be noisy when it didn’t count — and to get preciseness only when needed by getting the flopping parts — usually wires — to go through guides when precision was needed. The bottom part is a pretty conventional loom. The left side is the card mechanism. The top is the card reader that — nice and simply — uses the holes in the cards to shift hooks for the many wires that reach down to pull up or not the warp threads. Take a look at Jacquard machine - Wikipedia Control device attached to weaving looms This portrait of Jacquard was woven in silk on a Jacquard loom and required 24,000 punched cards to create (1839). It was only produced to order. Charles Babbage owned one of these portraits; it inspired him in using perforated cards in his Analytical Engine . [ 1 ] It is in the collection of the Science Museum in London, England. [ 2 ] The Jacquard machine ( French: [ʒakaʁ] ) is a device fitted to a loom that simplifies the process of manufacturing textiles with such complex patterns as brocade , damask and matelassé . [ 3 ] The resulting ensemble of the loom and Jacquard machine is then called a Jacquard loom . The machine was patented by Joseph Marie Jacquard in 1804, [ 4 ] [ 5 ] [ 6 ] [ 7 ] based on earlier inventions by the Frenchmen Basile Bouchon (1725), Jean Baptiste Falcon (1728), and Jacques Vaucanson (1740). [ 8 ] The machine was controlled by a "chain of cards"; a number of punched cards laced together into a continuous sequence. [ 9 ] Multiple rows of holes were punched on each card, with one complete card corresponding to one row of the design. Both the Jacquard process and the necessary loom attachment are named after their inventor. This mechanism is probably one of the most important weaving innovations, as Jacquard shedding made possible the automatic production of unlimited varieties of complex pattern weaving. The term "Jacquard" is not specific or limited to any particular loom, but rather refers to the added control mechanism that automates the patterning. The process can also be used for patterned knitwear and machine-knitted textiles such as jerseys . [ 10 ] This use of replaceable punched cards to control a sequence of operations is considered an important step in the history of computing hardware , having inspired Charles Babbage 's Analytical Engine . A Jacquard loom showing information punchcards, National Museum of Scotland Traditionally, figured designs were made on a drawloom . The heddles with warp ends to be pulled up were manually selected by a second operator, the draw boy, not the weaver. The work was slow and labour-intensive, and the complexity of the pattern was limited by practical factors. The first prototype of a Jacquard-type loom was made in the second half of the 15th century by an Italian weaver from Calabria , Jean le Calabrais, who was invited to Lyon by Louis XI . [ 11 ] [ 12 ] He introduced a new kind of machine which was able to work the yarns faster and more precisely. Over the years, improvements to the loom were ongoing. [ 13 ] An improvement of the draw loom took place in 1725, when Basile Bouchon introduced the principle of applying a perforated band of paper. A continuous roll of paper was punched by hand, in sections, each of which represented one lash or tread, and the length of the roll was determined by the number of shots in each repeat of pattern. The Jacquard machine then evolved from this approach. Joseph Marie Jacquard saw that a mechanism c https://en.wikipedia.org/wiki/Jacquard_machine and to get a sense of the truly remarkable set of inventions and mechanical approaches that allowed these to work — early on with just one operator power — at the rate of about 1 or 2 cycles per second. Babbage could see that the card programmable loom could also be programmed for other things, but was unable — unwilling, probably — to work out what should be done downstream. It would probably not involve gears, would not be base 10 for most of the mechanism, and would still be very difficult to make work. For example, I think both the engine and the loom approach would need some form of restoring logic in order to work — this can be done mechanically — and not easily at all — and, for example, is much easier to do pneumatically. The analytical engine was thought about ca. 1837, just about the time the “Barker Lever” was invented in England as a pneumatic force amplifier mainly used to “power assist” the key action in large pipe organs. This is done by using a weak pull to open a valve for much more powerful air that can then do the work (usually opening a much stiffer valve). The greatest interest and early use was in France with the first large instrument completed in 1843. The “trackers” on these organs are rather like the “wires” of the Jacquard loom, and it is possible to see how a device that uses mechanical wires can be given intermediate power boosts via the Barker mechanism. A pneumatic computer was certainly possible using the technology of the mid 1800s, but there is no record of such a device. I think that Babbage lost out because of his drive for precision rather than logic, and though he was a terrific mathematician, he was not in the same class as the great engineers of his time. It’s also worth pondering the likely negative influence of the difference engine over the analytical engine. A perspective to consider is that the difference engine “almost works”, in that it can successfully calculate polynomials by cascaded addition, using a scheme that is on the order of an old desktop calculator. The problem is that the scheme doesn’t scale mechanically, yet it is how Babbage was thinking at the time. With his new thoughts about programmability, he also needed a very different physical paradigm to make a programmable engine. The Jacquard loom had more than the cards to offer: what was so interesting about it was how it handled much larger scales of information, noise, and mechanisms. The difference engine made it difficult for him to discard it in favor of what the new problem needed.



Hi Alan, I am a huge fan of yours. I'm just a regular computer/phone user who is trying to get a job in the industry, but I want to help advance computing in the ways you described. What can I do? I'm interested in utilizing today's powerful hardware
I suggest looking at the idea of “What Is Actually Needed” (WIAN), which I’ve written about in Quora, and occasionally described in talks. The center of the idea is that just trying to improve something often doesn’t take enough context into account, and may wind up making the larger situations worse — especially if the larger situation is already a bad idea.

One part of this process is to learn how to really criticize — especially that which has been around long enough so that it seems “normal” and “tested by time”.

Another part is to find the WIAN threshold, which is often qualitative. For example, WIAN might be something radically different than a “better old thing”. One route is to get sensitized to seeing “almost a new thing”.

Computing is full of “unfortunate old things” that computer people are happy with repainting (for example, the standard OSs and the WEB). It’s worth trying to understand what went wrong to allow “containers” in their current form to be considered as “reasonable”. Why are most people still programming using the mostly weak ideas of the 60s.

Etc.

In order to find which knowledge you need to learn and invent, you first have to imagine the qualitatively better contexts that require that knowledge.



What is the list of civilization's most "powerful ideas"?
I think the top of the list is

There are powerful ideas
From here on, they come in many shapes and sizes. For example, one of the biggest is

2. Human brains are not great for thinking, but can be taught how to think much better

This is part of the fact that raw cleverness (IQ) can’t get far without Knowledge, and Knowledge can’t get far without Perspective/Context. Another way to look at this is that our main improvements have been in *method* — for example, the powerful idea:

3. Science

Another powerful idea is

4. Look for powerful ideas: try to figure out what the phrase means …

Years ago Seymour Papert and I tried to make such a list, and had quite a bit of trouble organizing it. We concluded that while articulating them was good e.g.

5. When something is complicated, find how to divide it into parts and try to understand the parts: rinse and repeat

it was number 4 that was even more powerful …

and so forth … (you can see why these are difficult to organize!)



Where does Alan Kay draw the line between tinkering and inverse vandalism?
I don’t think it matters where I draw the line, but having a real sense of “inverse vandalism” (making things just because you can) can be very important in an age where the industrial revolution can not just make in quantity, but can oversupply.

One key idea is that we don’t have curbs genetically built in for many things (from e.g. sugar to fantasies/stories, social interests, etc) because these were all naturally limited. But they turn into “legal drugs” or encumbrances when overproduced and over distributed.

It’s striking, and sometimes fun, to go to a large flea market (like Portobello in London) and muse that everything there was not just the product of tinkering, but they were once mass produced.

I don’t think tinkering is the main problem (it is very useful in so many ways). For tinkering, a big problem is to confuse it with engineering (they should work together). The larger problem is to avoid overproducing underthought things that can “redefine ‘normal’ lower”.



What is considered an early version of a graphical user interface (GUI)?
There were a number of excellent GUIs before Parc (most of which provided ideas we used at Parc). My favorites were Ivan Sutherland’s Sketchpad system (ca 1962–3) and the RAND Corp’s GRAIL (GRAphical Input Language) system). This research at RAND was part of the “DoD community”, and GRAIL was yet another ARPA project. GRAIL was a followon to the wonderful JOSS system at RAND, and a leap from a typewriter based interface to one that was completely done by gesture. A fairly true story is that the wastepaper baskets (and also notebooks) were examined at night to determine some of the processes used by RAND analysts. They found a lot of flowcharting and diagrams, and decided to try to do a computer based flowchart editor. To do GRAIL, a very good tablet had to be invented from scratch (Ellis: 1964 — the same year as the mouse), a terrific hand drawn symbol recognizer was done (Groner: 1966), and the larger whole system (ca 1968). To make things more fun and difficult — but realistic — they decided to program the entire system in itself. There is a whole movie done by RAND, but it is worth looking at the excerpt I’ve used to show in talks, and then looking at the full movie. This is demoed by Tom Ellis, but I talk over him to point things out to the audience: The 2 minute Excerpt Next, the whole movie, which shows many more details, including the astounding fact that the flowchart programming ultimately rested on a foundation of IBM machine code for the huge 360/44 mainframe used for this. The tablet itself was handmade at RAND, and you could get one for about $18,000 1960s dollars. The display system was over $180K in 60s dollars. About 1/3rd of the single user mainframe was used for the gesture recognizer. The Movie This system was many years ahead of its time — to the point that something as good hasn’t been done since! It was by far the most *intimate* feeling GUI I’ve ever used. Some of the YouTube comments today failed to understand just what this research was after — which was primarily “human-comptuter-symbiosis” and how could the computer be “inserted” as an amplifier of thought without feeling like a drag or distraction. I.e. can everything that is great about pencil and paper doing and thinking be simulated successfully by the computer (and can some useful magic be added to this)? So the flowchart part of this (and machine code part of this) was ancillary to the main goals. The larger intention was to have the symbolic language be more like JOSS (a higher level language), but then they thought it would be quite a demo if they did it in itself in IBM 360 machine code … Part of the idea here was to see how far one could go in “connected smoothness” (and in the 60s). And some of this was very related to the work of Engelbart and his NLS group at SRI at the very same time. One of the connections was the problem of how to get text characters to be part of the visual world in a reasonable way. If you are working and thinking graphically and using pointing devices etc then it is annoying and distracting to have to go to a text keyboard to type. Engelbart solved this with a “chord keyboard” for the left hand (for typing commands and text without interrupting navigation, etc.) and GRAIL did this with character recognition. Both of these were slower than a regular keyboard, but faster when the need was just for a few characters. An important side note is that the Gabe Groner gesture recognizer for GRAIL actually pipelined the recognitions, so the human did not have to wait for the previous character to be recognized! The Engelbart system also provided a typewriter keyboard, which you could go to for larger paragraph-sized volumes of text. So hands were “out” (for navigation etc) or hands were “in” (for inputing larger volumes of text. This worked well. The Engelbart NLS Lap Control Setup Some of us thought that this would work for GRAIL even better (with the mouse being replaced by the stylus). We also thought that two-handed UIs were generally better for any system one was going to spend hours each day on.



What does Alan Kay think independent researchers in computing should do for money if commercialization was part of the problem? What if "good funding" for research isn't available, but your ideas are worth pursuing?
I don’t know. There is no lack of money, especially when one looks at what the computer part of Xerox Parc cost, or even what the ARPA-IPTO community cost.

In my own experience, I and the other researchers within ARPA/ONR/Parc, etc knew we were in a wonderful situation. Our appreciation went up by a thousand fold after the great era ended in the early 80s, and commercialization of computing went into high gear.

One of the reasons the ARPA etc community was so productive was the level of funding it gave to universities and students, and the students grew to become PIs and mentors in the community. This started to fall apart after the ARPA funding ceased, and the pretty top down NSF became the main funder (and of much smaller projects). I’ve always thought that ARPA’s funding of large groups, often whole departments, in universities made a big difference (larger goals, less rivalries, no tenure pressure, etc.).

I was able to get smaller funding that would allow some things to be done with small groups, but I never was able to find “critical mass funding” that would allow a big area to be explored deeply and then made practical. This got more and more difficult and painful over the years, and I finally ran out of energy around age 78 or so.

A key point here — again — is “community”. I’ve always had roughly the same sized groups, but we were much more productive in the large at Parc because of the synergy with others.

As an old fogey now, it’s worth pondering the roles of business, government, and universities. In theory — and often in fact in the past — the universities were the best places to do breakthrough work — in part because of students, and in part because universities are supposed to be primarily about ideas. A good source of funding is the government, in part because it is supposed to be about the progress of the whole society, and in part because it can and does do various forms of redistribution of wealth to benefit the society as a whole. Businesses as they are currently constituted generally have much too small goals — they setup is not psychically good for making real progress.

If we look at these three, we can easily come up with great improvements not just on the current situation, but also in the past. For example, the “golden age funding” for computing came mostly from hot and cold wars, the government/citizenry has a hard time with “research for progress” per se. Universities became businesses from the baby boom, and essentially became more and more like trade schools and sellers of degrees. And businesses in the US form of capitalization and its markets have a hard time seeing that they must have higher aspirations than just making money.



Why wasn't the GUI by Xerox PARC or Xerox Star developed earlier? There was Sketchpad in 1963, which was a predecessor of a GUI, 20 years before.
The other answers are pretty good.

Let me first point out that the Alto at Parc with its bitmap display and overlapping window interface was done in 1973, which was only 10 years after Sketchpad, and for practical reasons required the first integrated circuit RAM (just 2 years after the Intel 1103 appeared in 1971) instead of the more expensive cores. Still, the Alto was expensive in 1973 dollars (and really expensive in today’s dollars) but over the years we made almost 2000 of them to serve as a “time machine” so we could work in the future rather than the present or the past.

Second, there were some truly excellent GUIs done in the 60s after Sketchpad and before bitmap displays, for example GRAIL at RAND Corp, which used a terrific tablet and gesture recognition to go along with great UI ideas. It had to use calligraphic CRT displays. Another system with many great ideas at the same time was NLS by the Engelbart group at SRI. Also a calligraphic display, but used video as a scanning conversion multiplying device, and had multiple panes, etc.



How could Xerox come up with advanced innovation tech (i.e. GUI) since their managements or leaders know nothing?
First, *some* of their management *did know things*. For example, the new Chief Scientist they got from Ford — Jack Goldman — was a first class physicist who also had a good feeling for what large companies needed. He urged Xerox to set up a Long Range Research Center away from corporate headquarters that would look at doing “critical inventions” in a wide range of sciences and technologies (including computing).

This wound up with another physicist — George Pake — being recruited from Wash U (StL) — which happened to be an ARPA project — to run Parc. The ARPA connection found Bob Taylor to set up the computing research, and he had funded both the ARPAnet and a wide range of top young PhDs, whom he recruited starting in late 1970.

By early 1971 he already had a critical mass of some of the best computerists in the world. He was able to get them because Congress inadvertently took away much of the “nice funding” for ARPA computing, and Taylor was able to convince that a company could possibly fund as well.

Part of the deal was that Xerox management was not to be in top down control of Parc — nor was Taylor — instead the top scientists “followed their instincts” and cooperated to do the “advanced inventions” (most were beyond “innovation”).



In the movie “Hidden Figures” is the depiction of the delivery of an IBM computer. Did the components actually not fit into the computer room as depicted in the movie, or is that “artistic licence”?
My original answer (that the movie scene was “unlikely”) starts with the third paragraph. I then got interested in the movie and liked the characters and the acting, but didn’t think the writing and directing did justice to the historical characters or the science. Comments on the movie follow the original answer. Then I decided to get the book “Hidden Figures” (which I found to be much better than the movie (comments on this follow). And then I decided to get more historical documents, especially including the autobiography of Katherine Johnson.

Bottom line: the larger subject matter, story, and people are really important and need to be widely told. More accuracy could have been achieved with more work, and would have made the portrayals even more important.

Original Answer: I think the sequence mentioned in the question occurs around 0:41 in the movie with the delivery of an IBM 7090 mainframe. The depiction seems quite unlikely to me for a number of reasons.

The largest reason is that IBM historically used an army of support people for every aspect of their products. “Support” was their defining feature.

So *everyone* handling the equipment especially at early stages would be IBM personnel, and there would be *lots* of them. And planning was IBM’s middle name. Their whole deal was not making great computers but to be the most comfortable manufacturer to their clients.

In those days you got *permanent* people with the computer after the installation who would take care of every form of maintenance, including fixing the machine and training both operators and programmers. (This was the case even with the tiny IBM 1401 we used in the Air Force ca 1961.)

Movie Comments

Also, the later “we can’t get this machine running” sequences seem quite false to me. As do other sequences involving this machine. That just isn’t the way IBM operated (I didn’t like their architectures, but I admired their processes).

Other scenes were just silly. For example, putting the oscilloscope probe on a different wire never seen before and getting a positive result (nope!). This is too bad because what these talented and skilled ladies were actually able to do was really good and very important — but the movies are a tough place to explain reality (same with the Turing movie, which never really touched what Turing did, how the Bombe was actually done, or what he was most famous for).

In the very beginning of this movie there is the question “Isn’t there anyone around here who knows analytic geometry?” This in front of the head engineer, who certainly does (as would all the other Langley mathematicians, engineers and scientists involved (of which there were already hundreds)). This was worrisome to see.

Another side comment is that the 7090 was a ca 1960 transisterized version of the physically even larger vacuum tube 709 mainframe (which they would already likely have — they actually had a 704 in 1957 (see below)), so adding in a new mainframe and dealing with it would be nothing new. Similarly, FORTRAN was launched in 1956, and ran on most IBM computers — both large and small — including the predecessor 704, etc.

Poking around to try to find a history of computing machines at NACA, I found this picture of an IBM 704 (even more a predecessor of the 7090) at NACA in 1957:


Note, there are both a male and female operator. The book reveals that NACA installed an IBM 650 (a rather fun drum oriented vacuum tube computer) in the mid-50s and started to calculate trajectories etc with it. The manual they would have used would look like this:


The book also reveals that the 7090 in the movie was actually one of two that went to Goddard Space Center in DC, and the 704 above was the computer at NACA (and there was no 709 in between). Also, the tape drives didn’t look right in the movie. IBM would have been using 729 drives in 1961 (too much of a nit).

Another side comment is that the trajectory for Alan Shepard’s flight


has to get the capsule downrange in a small enough area to be picked up by the Navy. It’s worth noting that the V2 rocket in WWII traveled about the same distance (about 235 miles to London) and as accurately (it generally targeted about a 10x10 mile area). This still had to be calculated for Project Mercury, but it was not a new problem — and the path was not actually a parabola (see below).

About race and gender

The movie is set in 1961 in Virginia — the very same time period I was in the US Air Force stationed in Waco, and then Randolph AFB, San Antonio Texas. My best friend was the best programmer at Randolph, and was a supersmart black guy from New Orleans. We once got refused any service in a bar in San Antonio because he was black — I was from the Northeast so it took me a while to understand what was going on — but inside of Randolph there was no hint of any prejudice.

Nor were there any hints of prejudice towards women: my boss was a woman, and an extremely talented programmer, and many of the programming bosses at Randolph were also women.

However, all the women programmers on the base were civilians, who were mixed in with (fewer) male civilians. So we could guess that the Air Force was lagging in how its military women were used (in looking this up, I found that women were in a separate organisation called the Women’s Air Force — which was “kind of part of” the Air Force — until 1973, when this separation was dissolved).

I think the same situation was pretty much the same within government science/technology institutions at the time. Some of the best SAGE (air defense) programmers in the 50s and early 60s were women — Margaret Hamilton was one of them: she soon went on to write software for the Apollo Guidance Computer at MIT, and quickly wound up as the head of all the operational software for the AGC (and eventually wound up winning many prizes for her SW that saved the first moon landing mission). She recounts “some, but not a crushing amount, of male resistance”.

So I can believe that the “out in Virginia” sequences in this movie still had lots of prejudice against blacks in 1961, but the “inside NASA” sequences with both black prejudice and gender prejudice don’t ring true to me from my Air Force experiences. What counted back then, in my direct experiences, was ability. If you could show it in the Air Force, it didn’t matter whether you were black or a woman or both.

I have a pretty strong feeling that the same was generally true within NASA (for example, I really doubt that NASA had segregated restrooms whether in the south or not) — the Air Force at the same time (1961) definitely did not have segregated anything, even in Texas). Of course, NASA was civilian, and there could have been differences.

I felt that the movie had some important things to say, but didn’t do a strong job with them. For example, the more true to life contrasts between the prejudiced outside world, and the much more equitable inside world could have made great realistic drama and impact.

On the other hand, the principal actors are able to transcend the material they were given with high quality performances that most of the time portrayed “higher points” and rendered the many flaws in the writing and directing irrelevant.

Another answer (by Donjanelle Robinson) provides additional notes of errors and facts:

What are the historical inaccuracies in the movie "Hidden Figures"?
https://www.quora.com/What-are-the-historical-inaccuracies-in-the-movie-Hidden-Figures
The Book “Hidden Figures”

The book is many levels of quality higher than the movie, not just in historical facts, but in quality of writing and in depth of coverage. It’s worth realizing that a movie script is not just limited on each very spare page, but also in length (they average about 1 page per minute of film, so: the script would be about 126 pages).

By contrast the book is well over 300 pages with considerable background information, and with its main story starting much earlier in WWII with the expansion of NACA’s (not yet NASA) pool of mathematicians and “computers” (human beings, most often women, with mathematical backgrounds who used mechanical calculators as prime aids) — to include black women.

The complexity of segregation in each era is well told in the book. In many ways, this is the center context of the book. For example, the state of Virginia had many segregation laws that were the opposite of what was supposed to be the case for federal employees, and what resulted were tugs and pulls back and forth (in fact, for a variety of reasons, including Senator Harry Byrd, Virginia could easily be rated as one of the most backwards states regarding segregation). It’s brought out in the book that many of the technical men were from the north rather than the south, and this turned out to be a positive factor, both for women and blacks.

Also, in real life there are often many people involved in events worth making a movie about. These become also “Hidden Figures” who are omitted in a movie version (and in this movie, having three distinct female protagonists also made it complicated even with its over simplifications). There are many more key figures in the book, and all are worthy of attention.

I wouldn’t omit any of the great number of facts in the book, but I found myself making little charts to help me keep more clear who was doing what, when, and where. I think I would have preferred longer sequences staying on single characters.

I was sorry that the author didn’t make more of an attempt to explain the nature of the work, for example what “analysis” means mathematically, and what “numerical analysis” means to continuous mathematics, and why having “computers” helps. This could be done without requiring the readers to understand much more than how difficult to handle math relationships can still be approximated numerically. There isn’t much more explanation than the movie (but less of a feeling of “rabbits out of the hat”).

For example, everyone can understand that a smooth curve of any shape can be approximated very highly by a sequence of connected short lines that are as close to the curve as possible (to get closer you make the line lengths shorter). This idea was used by Newton in the 1600s, and a nice way to do it was found by Euler (this is “Euler’s Method” which is not explained) — a big deal here is that quite a bit of all the computations done at Langley would have treated difficult problems by approximating them with line segments and solving for each line (this might require several thousands of calculations). For example, Seymour Papert used a mechanical robot “turtle” with a pen in it to teach children about parts and wholes, analysis and synthesis, and forms of analytic geometry. Here’s what 10 short line segments look like when hooked together into a polygon, and the second looks like a smooth circle curve but it is 360 short straight line segments. The LOGO program for this is repeat 360 [forward 1 right 1].


The differential equation for this idea (in English) is “change of curvature = 0”. This means the curvature of a circle is constant — and we can also easily see this from the program, where the step and the turn at each step are constant.

“Analysis” means “take apart” and we can see how all this works if we are trying to understand how a complex shape — like that of a wing — interacts with air at different speeds.

You can use a board for a wing but the streamlining makes it much more efficient. Often the streamlining was achieved by watching smoke streams in a wind tunnel and then carving wood models more smoothly to eliminate turbulence. The final shape might be difficult to handle algebraically, but (second picture) we can approximate it with line segments. This is “taking apart” (“analyzing”) the curve into separate simpler pieces.


The third picture has the arrowheads removed, and the fourth picture is with many more straight line segments. Each one of which can be treated with easier mathematics (still lots of things to worry about) separately. The separate computations of each little segment of a complex curve, was a large part of the work of the human computers.

This is both why “math can be lots of fun” and “made very understandable”!

The same process was done for the flight paths of the capsules. For example, the Alan Shepard flight was not actually a parabola, but a much more complex curve because of the various resistances of air during the climb out and return (similar calculations had been done by hand for artillery shells for many decades, and one of the main jobs of the original ENIAC was to calculate trajectories). This was a more complex calculation for space vehicles because they wanted them to be as streamlined as possible going up, and as blunt as possible coming back. It’s the green more complex curve below that would be approximated with line segments. Note: that each line segment is a hint for what the attitude and speed of the spacecraft should be at that time and place.


I also wish the book early on had not stooped to the fake mystique of “faster than the speed of sound”. Most bullets go faster than sound, and more interestingly, in the 40s, the V2 rocket went much faster than the speed of sound and was not at all torn apart.

The book is at its best in providing a large scale picture of many dimensions of prejudice, segregation, tolerance, determination, etc., in NACA which morphed into NASA after Sputnik in 1957. This portrayal was done enough better than the movie to make up for its lack of explanation of what the women computers were doing (and also to get a more accurate timeline on the many events). The book shows that a lot of clean up changes had already happened by the time the events in the movie started.

All this got me to look at more direct sources, including an autobiography Katherine Johnson wrote for junior level readers “Reaching For The Moon”.

Katherine Johnson autobiography: “Reaching For The Moon”

This book is pure gold on so many levels, and I wish the movie (or a movie) had been made from this story.

<more to come>

This picture is of Melba Roy Mouton, Program Production Section Chief at NASA Goddard Space Flight Center in 1964




What is the significance of Butler Lampson's work?
A good reply is much too large for a Quora sized reply.

Basically: Butler is one of the most significant computer scientists in history — and has been one since he moved from physics to computing almost 60 years ago. He was always a “principal scientist, investigator, and thought leader” even as a grad student. He is a quintessential example of the word “brilliant” (and the term seems to have been coined for him).


In the 60s while hearing a rapidly delivered perfectly formed Butler talk, a fellow grad student turned to me and said “You know, it’s just a privilege to know someone like Butler”. When asked about “computer science” and “computer scientist” I’ve said “Look at Butler to see an example of a top computer scientist — it’s hard to put into exact words, but he is one”.

The list of his accomplishments is long, and so is the list of awards. Much of his significance has been in how he was able to both understand, invent, and influence the large world of systems of many kinds. These include a number of significant operating systems (including SDS-940 and Tenex), computers (including the Xerox Parc MAXC and Alto), networks (including the Ethernet and the Internet), and programming languages (including Mesa).

He is both a great thinker and a great doer, and both of these plus his larger than life personality almost literally dragged the whole field forward, and made all the rest of us into better computer scientists. I.e. a kind of “magical person”.

I will return here periodically with a little history and a few stories, but they are really commercials for readers to look at larger accounts of Butler’s larger than life history.

The citations and bio for his 1992 ACM Turing Award (the highest award in computing) are good to start with:

Butler W Lampson
Butler Lampson was born in Washington DC and educated at The Lawrenceville School , an elite boarding school 6 miles from Princeton New Jersey. Lampson’s first computer was an underutilized IBM 650 at Princeton, which an enterprising high school classmate found and got permission to use while it was idle. It had a drum memory with 2000 words of ten decimal digits, and the only I/O aside from the console was a card reader/punch, controlled partly by the computer and partly by a plug board. As an undergraduate at Harvard, Lampson learned the APL programming language from Ken Iverson , who spent a year on sabbatical there. At that time there was no implementation of the language, and Ken didn’t want one because he was sure that it would make compromises that would wreck the language. While still an undergraduate Lampson also programmed a PDP-1 to analyze spark chamber photographs from the Cambridge Electron Accelerator and wrote a display editor for the PDP-1’s one-point-at-a-time display. Lampson went to the University of California at Berkeley in the fall of 1964 as a graduate student in physics. At the Fall Joint Computer Conference in San Francisco that year he ran across Steve Russell from MIT, who told him about the Genie project hidden behind an unmarked door in Cory Hall. There he met Peter Deutsch and Mel Pirtle, the Principal Investigator, and was quickly seduced away from physics to study computers. The Genie project modified a Scientific Data Systems SDS 930 minicomputer to create the first time-sharing system with character-by-character interaction. Later SDS marketed it as the SDS 940, the first commercially available general purpose time-sharing system. Lampson wrote parts of the operating system and several programming languages, notably Cal, an interactive language for numerical computation derived from Cliff Shaw’s Joss , and the QSPL system programming language done with Peter Deutsch. At Berkeley he also designed the Cal time-sharing system for a CDC 6400 in the computer center, together with Jim Gray , Charles Simonyi , Howard Sturgis and Bruce Lindsay , who all went on to later fame. This was the first capability-based system to have a real user community. It pioneered the ideas of shadow pages and redo logs, but also taught us that capabilities are not a good basis for long-term security. The Genie project researchers couldn’t figure out how to build the much more grandiose second system at Berkeley, so in 1969 they started Berkeley Computer Corporation (BCC) to do it. After two years it burned through $4,000,000 and built just one working system before folding. Lampson designed and coded most of the microcoded parts of the operating system and worked on the SPL system programming language. Around this time he also devised the access matrix model for computer security, unifying the ideas of capabilities and access control lists. Luckily, as BCC was ending the Xerox Palo Alto Research Center (PARC) was getting started, and Bob
https://amturing.acm.org/award_winners/lampson_1142421.cfm
Next, the Wikipedia article —

Butler Lampson - Wikipedia
American computer scientist Butler W. Lampson (born December 23, 1943) is an American computer scientist best known for his contributions to the development and implementation of distributed personal computing . Education and early life [ edit ] After graduating from the Lawrenceville School (where in 2009 he was awarded the Aldo Leopold Award, also known as the Lawrenceville Medal, Lawrenceville's highest award to alumni), Lampson received an A.B. in physics ( magna cum laude with highest honors in the discipline) from Harvard University in 1964 and a PhD in electrical engineering and computer science from the University of California, Berkeley in 1967. Career and research [ edit ] Professional Developers Conference 2009 Technical Leaders Panel During the 1960s, Lampson and others were part of Project GENIE at UC Berkeley. In 1965, several Project GENIE members, specifically Lampson and Peter Deutsch , developed the Berkeley Timesharing System for Scientific Data Systems ' SDS 940 computer. After completing his doctorate, Lampson stayed on at UC Berkeley as an assistant professor (1967–1970) and associate professor (1970–1971) of computer science. For a period of time, he concurrently served as director of system development for the Berkeley Computer Corporation (1969–1971). In 1971, Lampson became one of the founding members of Xerox PARC , where he worked in the Computer Science Laboratory (CSL) as a principal scientist (1971–1975) and senior research fellow (1975–1983). His now-famous vision of a personal computer was captured in the 1972 memo entitled "Why Alto?". [ 1 ] In 1973, the Xerox Alto , with its three-button mouse and full-page-sized monitor , was born. [ 2 ] It is now considered to be the first actual personal computer in terms of what has become the "canonical" GUI mode of operation. All the subsequent computers built at Xerox PARC except for the "Dolphin" (used in the Xerox 1100 LISP machine) and the "Dorado" (used in the Xerox 1132 LISP machine) followed a general blueprint called "Wildflower", written by Lampson, and this included the D-Series Machines: the "Dandelion" (used in the Xerox Star and Xerox 1108 LISP machine), "Dandetiger" (used in the Xerox 1109 LISP machine), "Daybreak" ( Xerox 6085 ), and "Dicentra" (used internally to control various specialized hardware devices). At PARC, Lampson helped work on many other revolutionary technologies, such as laser printer design; two-phase commit protocols ; Bravo , the first WYSIWYG text formatting program; and Ethernet , the first high-speed local area network (LAN). He designed several influential programming languages such as Euclid . Following the acrimonious resignation of Xerox PARC CSL manager Bob Taylor in 1983, Lampson and Chuck Thacker followed Taylor colleague to Digital Equipment Corporation 's Systems Research Center . There, he was a senior consulting engineer (1984–1986), corporate consulting engineer (1986–1993) and senior corporate consulting engineer (1993–19
https://en.wikipedia.org/wiki/Butler_Lampson
Pretty much all of his more than 100 papers are not just well worth reading for content, but also to savor his crystal clear style.

Microsoft researchers and engineers working around the world
Get to know Microsoft researchers and engineers around the world who are tackling complex problems across a wide range of disciplines.
https://www.microsoft.com/en-us/research/people/blampson/publications/
Early Butler Projects

Butler got an undergrad Physics degree at Harvard ca 1964, and traveled to UC Berkeley for a PhD. As he later put it, he “walked in the wrong door” and found students (one of them the legendary Peter Deutsch) debugging what would be called the “Project Genie” time-shared computer and OS sponsored by ARPA-IPTO. He immediately pitched in, and essentially never left this room. Soon, he was the main designer of the OS, a powerful “thought-force” for the HW modifications and whole project, and quickly wound up as an ARPA co-PI.

The resulting system was so successful that ARPA pushed SDS to commercialize it so that others could buy it (as the SDS-940). It was the computer used by Engelbart for “the mother of all demos” in 1968, and by TYMSHARE Corp to launch the first time-sharing business. And the design was recapped again for the next generation as “TENEX OS by BBN) for the DEC PDP-10, and again heavily used in the ARPA community.

The design and approach to the design were both highly significant. The base machine was about 0.5MIP and 64K 24 bit words and a bit shaky (as so much of the HW was then), so the OS had to be both comprehensive, lightweight, and as fail safe as “lightweight” could be. I had started grad school in late 1966, and found the system to be a revelation of “sweet parsimony and balance”.

Also, Butler’s “hobby project” on this system was “CAL” an optimized version of JOSS, that constituted a “sweet spot” for interactive computing by the larger public.

All this was essentially Butler’s PhD project. Via his obvious brilliance, and in the midst of the ARPA-IPTO community he was let to run free by his advisors: supreme computerists Harry Huskey

Harry Huskey - Wikipedia
American computer design pioneer (1916–2017) Harry Douglas Huskey (January 19, 1916 – April 9, 2017) was an American computer design pioneer. Early life and career [ edit ] Huskey was born in Whittier , in the Smoky Mountains region of North Carolina and grew up in Idaho . He received his bachelor's degree in mathematics and physics at the University of Idaho . He was the first member of his family to attend college. He gained his Master's and then his PhD in 1943 from the Ohio State University on Contributions to the Problem of Geöcze . [ 2 ] Huskey taught mathematics to U.S. Navy students at the University of Pennsylvania and then worked part-time on the early ENIAC and EDVAC computers in 1945. This work represented his first formal introduction to computers, according to his obituary in The New York Times . [ 3 ] He visited the National Physical Laboratory (NPL) in the United Kingdom for a year and worked on the Pilot ACE computer with Alan Turing and others. He was also involved with the EDVAC and SEAC computer projects. Huskey designed and managed the construction of the Standards Western Automatic Computer (SWAC) at the National Bureau of Standards in Los Angeles (1949–1953). He also designed the G-15 computer for Bendix Aviation Corporation , a 950 pounds (430 kg) machine, operable by one person. [ 4 ] He had one at his home that is now in the Smithsonian Institution in Washington, D.C. After five years at the National Bureau of Standards, Huskey joined the faculty of the University of California, Berkeley in 1954 and then University of California, Santa Cruz from 1966. He cofounded the computer and information science program at UC Santa Cruz in 1967. He became director of its computer center. In 1986, UC Santa Cruz named him professor emeritus. While at Berkeley, he supervised the research of pioneering programming language designer Niklaus Wirth , who gained his PhD in 1963. During 1963-1964 Prof. Huskey participated in establishing the Computer Center at IIT Kanpur and convened a meeting there with many pioneers of computing technology. [ 5 ] Participants included Forman Acton of Princeton University , Robert Archer of Case Institute of Technology , S. Barton of CDC, Australia, S. Beltran from the Centro de Calculo [ 6 ] in Mexico City , John Makepeace Bennett of the University of Sydney , Launor Carter of SDC - author of the subsequent Carter Report on Computer Technology for Schools, [ 7 ] David Evans of UC Berkeley , Bruce Gilchrist of IBM-SBC, Clay Perry of UC San Diego , Sigeiti Moriguti of the University of Tokyo , Gio Wiederhold , also of UC Berkeley, Adriaan van Wijngaarden of the Mathematisch Centrum in Amsterdam , Maurice Wilkes of Cambridge University . Huskey was Professor Emeritus at the University of California after his retirement at the age of 70 in 1986. In 1994 he was inducted as a Fellow of the Association for Computing Machinery . Dag Spicer, senior curator at the Computer History Museum in Mountain View, Calif
https://en.wikipedia.org/wiki/Harry_Huskey
and Dave Evans

David C. Evans - Wikipedia
American computer scientist David Cannon Evans (February 24, 1924 – October 3, 1998) was the founder of the computer science department at the University of Utah and co-founder (with Ivan Sutherland ) of Evans & Sutherland , a pioneering firm in computer graphics hardware . [ 2 ] Evans was born in Salt Lake City . He attended the University of Utah and studied electrical engineering; he earned his Bachelor of Science in Physics in 1949 and his Doctorate in Physics in 1953. [ 3 ] Evans first worked at the Bendix aviation electronics company, where he acted as project manager in 1955 to develop what some describe as an early personal computer that ran on an interpretive operating system. The Bendix G-15 was a bulky unit about the size of a two-door refrigerator. He stayed with the company just long enough to manage the G-20 project. [ 4 ] Evans became a faculty member of the University of California, Berkeley . His first important work with graphics dates from that period, when he did several experiments on an IDIOM display hooked up to a Digital Equipment Corporation PDP -5. [ citation needed ] In 1963, he was co- Principal Investigator (with Harry Huskey ) for project Genie to produce an early multi-user timesharing system. Students from this period include Butler Lampson and L. Peter Deutsch . [ 5 ] The system, which included key developments in the field of virtual memory , was sponsored by the US Defense Department's Advanced Research Projects Agency . [ 4 ] In 1965, the University of Utah recruited him back to start their own computer science department. When he was building up the University of Utah department in 1968 he managed to convince Ivan Sutherland (who had funded Evans' DARPA research) to come to Utah , accepting the condition that they start a computer graphics company together. Evans retired from the company in 1994. Evans's students at Utah included Alan Kay , Edwin Catmull , James H. Clark , John Warnock , Alan Ashton , Frank Crow , Jim Blinn , Bui Tuong Phong , Gary Hodgman , and Henri Gouraud . Evans was a member of the Church of Jesus Christ of Latter-day Saints (LDS Church). He served as a branch president , a counselor in bishoprics and stake presidencies , and as a scout master for a total of 27 years. [ 6 ] Evans was awarded the Silver Beaver for his role in scouting. [ 4 ] Evans married Joy Frewin. They had ten children, only seven of which lived to adulthood. One of these is David F. Evans , who became a general authority in the LDS Church. From 1984 to 1990, Joy Evans was a counselor in the general presidency of the Relief Society to Barbara W. Winder . At the time of his death on October 12, 1998, Evans had 39 living grandchildren and great grandchildren. In 1996, Brigham Young University established the David C. Evans Chair of Computer Engineering and Graphics. Evans was at the ceremony where the founding of a chair in his honor was announced, but due to his suffering from Alzheimer's disease , did not make any rema
https://en.wikipedia.org/wiki/David_C._Evans
who a year or so later at Utah birthed modern 3D graphics (at Utah I was also lucky to have Dave as my PhD advisor).

A lot can be learned about design from looking at the Project Genie OS and subsequent work (hint: look at his succession of papers and commentaries). I’m trying to work on the “significance” part of the question, so I’ll mention just two of the many interesting insights and features in this system.

The first is that I think it was pretty much the first OS that “was designed to fail”, in that contrary to most code that was assumed to work (and didn’t), Butler knew that there would be both SW and HW bugs that would cause crashes (and this is really a bad thing for a time-shared system). So, a percentage of the already meager resources of the HW were used to both mark pages as “dirty” or “clean”, and to continuously try to write out clean pages to the secondary storage device. This meant (a) that a need for a written out page could almost always find clean storage in core to put it (without having to write out beforehand), and (b) that a crash generally had most of the end user’s pages in a clean and written out state, and this along with other provisions almost always made crash recovery very swift.

The second had to do with “training programmers” to pay more attention to taking care of their working set. The scheme had to do with the HW maps forcing a working set to be smaller than the physical core storage. This allowed the next user’s working set to also be in core ready to go. The wrinkle was that a programmer could predict to the OS what pages would be in their next working set time quantum, and if true, the OS would reward by running on the fast queue. If the prediction failed (and an unpredicted page fetch was required) then the programmer was punished by being run on the much slower slow queue.

Both of these — along with other special characteristics — allowed the 940s to have remarkably efficient performance. It could run several dozens of users with 0.5 MIP and 192Kbytes of RAM — and it was also able to be the workhorse for the Engelbart’s group amazing “Mother of all demos” in 1968.

Next

If I were to write a next section here, I think I’d talk a bit about Butler’s work and influence on design, safety, security, confinement, and meaning in systems. An important idea he emphasized from early on is “In computing, design rules change dramatically every few years”. A lot of this is from both Moore’s Law scalings, and from the amount of learning that can happen in a few years.

This means that trying to carry old ideas and techniques forward is fraught with disaster, and if done, has to be done very carefully. Butler followed his own “hints” and tried to do new designs that fit the new situations for each new system he worked on. Often this had to be done with less than ideal HW from manufacturers. With the SDS 930 that became the 940, for Project Genie, they were able to make a new memory mapping system, and found very successful ways to get considerable power from what seems like meager resources. A User Machine In A Time-Sharing Environment covers some of the most important ideas and insights here.

On the later CDC 6400 at Berkeley, they were pretty much restricted to using the existing HW while trying to come up with a radically new way to structure, protect and make an OS that used the idea of “capabilities” — a secure unforgeable reference privilege with limits — but without having HW support.

On Reliable And Extendable Operating Systems (1969) gives a clear account of the thinking and resultant designs of this unusual system design.

<more to come>



How does a computer know what to do with binary code 01000001? How does the computer know this means A who told the computer?
To get started thinking about this, it would be a good idea to not use the word “know”. A better really simple analogy is how keys and locks work, where the lock is set up physically to accept or resist the carved patterns on keys. We generally would not say — except perhaps in poetry — that the lock “knew” the key. The so-called weak “AI” of today cannot be said to “know like a human”, so we can leave out “know” here also. A good starting analogy here is how Morse code has been used in the telegraph for more than 150 years. Lots of methods were tried, but things worked best if the simplest signals were transmitted over the wires (long ago they picked long and short noises). Then they had to come up with patterns of these to stand for the letters and numbers used by humans. For example, the pattern for “A” in Morse Code is “short long” or “dot dash” (and written as “. -”.). To send the letter “A”, the sending human taps out “short long” using the telegraph key — this sends a short and long impulse of electricity over possibly many miles to an electromagnet and “clicker” which turns the electric pulses back into sounds that the receiving human can hear. If this person knows that “short long” is supposed to mean “A”, then they can write it down. On a computer you can use “Voltage and no Voltage” for the two codes. These are often written 0 and 1 (even though they are just “Voltage and no Voltage”. Imagine a panel with 8 switches. Now can we connect wires so that only the switches that correspond to 01000001 will turn the light on. How? We can use double throw switches in series: decide that an up switch will signify “0” and a down switch will signify “1”, and wire them so that: up=on dn=on up=on up=on up=on up=on up=on dn=on And this will only pass current to turn on the light when only the switches are thrown correctly (something very like this is used in computers for decoding a binary number into unary selections). Can you see how this is like a “lock” that can match to a particular “key”. Humans can then assign a pattern like this to stand for an “idea” (like “A”). Whats fun about the computer is the extent of things that can be done just by manipulating patterns. Most transistors in a computer act like switches that can be thrown off and on via receiving electrical signals (so computers are mostly organizations of “switchable switches”). At some point it should be possible to make something just out of patterns that behaves like a mind — even a simple animal mind — but this has turned out to be surprisingly difficult, and is still a ways off.



How does a computer work? Explain with an example.
Originally Answered: How do I explain how a computer works in my own words?
In the answer to the parent question, Jay Nabonne gives an excellent suggestion: to find ways to understand computers, then try to explain them yourself to others (which will generate your own words for this).

So I think your question is mostly: “How do I understand how a computer works?”

There are many answers on Quora already for this (including from me). When people ask me for something good to read to start off absolutely from scratch, I usually recommend “The Pattern In The Stone” by Danny Hillis. It is a small masterpiece.



What does Alan Kay think about the harmony-oriented programming paradigm (https://dl.acm.org/doi/abs/10.1145/1449814.1449872)?
Originally Answered: What does Alan Kay think about the harmony-oriented programming paradigm (https://dl.acm.org/ DOI/10.1145/1449814.1449872)?
This URL results in a 404 error (the correct URL is in a comment). This is a very short speculative paper. In the same comment is a reference to a 2 page poster, and a Master’s thesis by Kyle Martin - “Harmony Oriented Architecture” https://stars.library.ucf.edu/cgi/viewcontent.cgi?article=2765&context=etd, which has more content.

In the spirit of “Yes, and … ?” that is the center of improv and tentative thinking, I think what these papers are calling attention to — the difficulties of scaling code and systems, and the need for great improvements — should be greatly encouraged, experimented with, discussed, etc.

I think the call for ways to have looser couplings being part of the larger scaling solutions is good. And some of the citations, especially in the MS thesis, point to praiseworthy places to start. For example, I’ve long been a fan of LINDA as a way of thinking — especially as a metaphor — for how to get started, especially to try to coordinate via descriptions rather than domain labels.

I also like the goal of having the system work successfully in the preseence of noise and errors.

The rub here is partly that we need to actually do implementations to vet ideas, and that feasibility studies in scaling are not easy to set up. For example, the Internet was very carefully thought through and simulated — and Parc even had its own real “internet” (the PUP network) — nonetheless, the actual making of the Internet still required more heuristics to be added to TCP/IP in order to stabilize the larger system.

Similarly, the massive over-all-Internet replicated pseudotime system — Croquet — which had its birth in the PhD thesis of Dave Reed in 1978 — had to be implemented and tested and redesigned and implemented many times before it really started to scale well.

And earlier, Smalltalk had 5 distinct designs and implementations over 10 years (the last 3 were of great practical use, and all were used extensively).

This is partly a general principle, and partly a direct consequence of the combination of scaling and degrees of freedom, etc.

For “Harmony, etc.” there is not enough practice experience for vetting that I can see.

Its fans see possible good properties (and I think there are a few in there). But I also worry about the level of solution proposed. For an example of a nit, the rhetoric about “encapsulation” misses that an object can expose any part it wishes to, while still being able to protect the parts that shouldn’t be exposed.

I also feel that much too much of the discussion is trying to preserve “bad old ways” of doing things. I would feel more comfortable to see sample code in a new language made for making the new kinds of system proposed.

However, I would definitely sign Kyle Martin’s MS thesis because it is a good enough, wide enough discussion to count as an intellectual contribution to an ongoing discussion of very important issues.



What are some tips for a 16-year-old that wants to be a professor in computer science, specifically in type theory and programming language theory?
I think the best way to be a professor in a subject is to first become a first class practitioner in the subject, and gradually learn how to explain it and help all levels of interested people to learn it. A good doer does not automatically become a good teacher — both parts require amazing amounts of work (and I think the “good teacher” part is the hardest).

One of the signs of first class practitioners is that besides being very skilled, they can also deeply criticize any and every part of the subject. This is especially critical in computing because so far it is much too ad hoc. One way to think of this is that good criticism is opening the door to much better ideas (and at the least allows good warnings of “danger ahead” to be part of the thinking process).

In order not to get buried in what people have done and are doing, I suggest using the question “why?” instead of “what?” or (worse) “how?”.

For example: why types? An early impetus was to help compilers generate code. E.g. computations with numbers don’t semantically need to know *int* or *float*, and in many cases an algorithm will need both. Another possible use for typed variables is as documentation — this is using the variable name as a stand-in for an entity that will now mostly help human programmers. A worthwhile case to consider is what to do if someone sends us a module over the Internet that we want to make use of. What is its “type”? What are the “types” of its API. Etc. Right away we should see that we need dynamically adjusting systems/languages rather than the old-style static approaches to semantics. I.e. We need “semantic types” and the field doesn’t have them yet. A stopgap is to have a dynamic language — it can be fixed so it can’t crash — that can dynamically adjust to both changes and unknowns.

The above paragraph amounts to the start of really big criticisms of most parts of computing (much of which insists on living in the past, and especially using old ideas that don’t scale).

We can see that some form of “typing” can be really useful, but that none of the currently used approaches is very good (I think most forms don’t really pay their way).

Similarly, one should ask about programming languages: why C or C++? Why Haskell? Why Smalltalk? Etc. A good answer for any of these will beg for something qualitatively better …

A good exercise — which most in computing flunk — is to be able to really criticize the WWW and especially its browsers …

I was very lucky as a grad student to have been influenced by Bob Barton — the main inventor of the B5000 — an authentic genius. In his advanced systems course he handed out a list of things for us to “read and remember”, and used the actual class time, as he said “… to disabuse you of any fondly held notions you might have brought into this classroom”.

Basically, he destroyed the existing field for us (including some of the things I knew he liked). Those who survived this class were set free of “the tyrannies of the past and present” and were now open to inventing better futures (sometimes radically better futures).

This could be tough today because even the “leading” CS departments do much too much training for jobs in existing business rather than actually helping students become real computer scientists and teaching then not just to think for themselves, but how to really think well.



Is it possible to have a computer without critical section?
Somebody (a gremlin?) changed the question after my answer to include the word “multiprocessing” (which makes a different context).

New answer is still “more or less yes”, but with a few twists. For example, some of the earliest multiprocessing was done by “roll in and roll out” of entire memory images, so it wasn’t possible for one process to write into another process. This could be tricky, because the nascent OS code was visible to every process.

Original answer to original question: Is it possible to have a computer without the use of critical sections in software?

Yes, and most programs didn’t have them in the early days.

The first programming job I had was in the Air Force in 1962, initially doing conversions from punched card machines to the IBM 1401. Interestingly (ironically?), both the multiple punched card machines *and* their plugboards were often run in parallel, whilst the tiny (8k 6 bit bytes) and slow 1401 generally didn’t have enough memory for any kind of concurrency.

And this was also the case for the slightly larger (and also slow) Burroughs 220 in our shop.

So there were no interleaved threads that could mysteriously clobber shared variables. And no critical sections to be protected. However, interrupts acted like transient threads and could clobber anything anywhere (in this case the entire program was a “critical section”, but some care would be taken …).

And, variables were still shared in single threads, and these could clobber a value in a variable that was expected in another section of code (this is STILL one of main causes of error even in relatively simple programs).

When multiprocessing was first introduced, the idea of critical sections was thought up as a simple way to suspend threads looking at the same section of code to avoid races between threads clobbering shared values. This was initially a practical solution, but it scaled poorly.

For serious multiprocessing, and especially time sharing, the parallel processes were hardware protected. This was unfortunately but pragmatically combined with MMU and swapping protections also in the HW. The combination did not scale well, and the HW manufacturers did not fix this (to this day). Then programmers reintroduced unprotected threads within the processes (a really bad idea also … and to this day).

Meanwhile, in the late 50s John McCarthy was specializing in “great ideas that ran poorly on contemporary machines”. One of these was Lisp. Another was the idea of “fluents” (or “pseudotime”). The basic idea was not to let the CPU determine time but instead to simulate time along with everything else. This allowed a real definition of “simultaneous” and was a scalable (upwards) way to deal with and eliminate race conditions. (I’ve tried to explain this idea in a few Quora answers …).

Soapbox: computing has a strong tendency to hang onto an idea — often by patching — long after it has become obsolete — and also tends to avoid learning new ideas, even when tremendously useful. This usually results in dangerous bloated messes.



What is the origin of model-view-controller?
Ivan Sutherland — the inventor-originator of interactive computer graphics — had the idea of a clipping window to see a part of the very large Sketchpad world. This wound up being a program structure roughly like a camera which had an aperture that could be of any size relative to the world, and would show that portion of the world on the screen. Each Sketchpad object had a method that would render the object in the world, and the windowing apparatus did the job of transforming world coordinates to window coordinates (this was done as part of the rendering operation).


Ivan Sutherland in 1962 at the building-sized TX-2 computer working with Sketchpad on a truss bridge design with the screen showing part of the whole bridge that has been zoomed in on. He is holding a light pen used for pointing and graphical input. This year is the 60th anniversary of Sketchpad.

The next version of Sketchpad was a 3D system (done by Timothy Johnson), and it supported 3D multiple views of the world.


Ivan’s next major project was the first virtual reality headmounted display, and this required tracking the head of the wearer, and using this to sample from the interior 3D world for each eye.


So there was a similar software structure like a telescope or camera that had one end in the virtual world, and had the other on the display face. Both the to be visualized objects and the “cameras” had six degrees of freedom. The virtual world side was sometimes called the “camera”, and sometimes called the “window”. The real world side was often called the “viewpoint” (at Xerox Parc, this was renamed the “window” to help explanations to Xerox).

When I was a grad student at the U of Utah, I overlapped with Ivan, the HMD project, and its approach (meaning: there is a world with virtual objects, and these include camera objects whose purpose is to look at parts of the virtual world and send these to be displayed on windows on a display). The simple linear equation for coordinate transformations was now a many degrees of freedom matrix that required quite a bit of special HW support (that had to be built from scratch).

At some point I started thinking about other than graphical virtual objects in the “everything is an object” ideas I was pursuing. The idea was that every object should know how to render itself, and this should be combined with a windowing-viewing mechanism. A fun thing was the idea to use multiple windows to show different aspects of the objects in the view. I first saw this in Sketchpad III, then in the Engelbart system in the late 60s (it was mostly used for filtering detail), and used the idea in the FLEX Machine that Ed Cheadle and I did.

The earliest versions of Smalltalk were simple and slow, and included multiple windows and “turtles” that could be part of a window (that would then clip what the turtle drew).


A Smalltalk-72 screen showing an editable view of an Elvish character, which is used in one of the paragraphs in “the galley editor” (an early experiment in desktop publishing). The galley is a collection of views of both paragraphs of text and a painting view with a sketch of Frodo and Gandalf. The user interface for a painting view pops up around it, and it in turn is made from views of the controls.

Another early plan — not done until later — was to have a slippage scheme, because the simulation “frames” from the model could often be computed much faster than graphical rendering of the view, and thus one would like to decouple the updating of the semantics from the updating of the display. This was later actually done in a principled way when MVC was implemented.

Shortly after Smalltalk-76 was done, we were visited by Trygve Reenskaug from Norway, who was very interested in large scale planning (such as for the Norwegian ship building industry). He was indefatigable, and decided to do a real planning system in the new Smalltalk. I attribute the first formal and working examples of MVC to Trygve, who among other things used them to view and change a complex changing schedule and to have different windows show lists, Gannt and Pert charts, etc. each of which could be edited directly and would automatically cause the model to update itself.


Trygve’s first planning system ca 1976–7 showing three different dynamic views of the same plan with the same item (A21) selected and editable in each view. This is arguably the first principled use of MVC.

Adele Goldberg and Dave Robson got very interested in this and took it further to the general idea that there is a “model” system, which has the semantics of the system, a “viewing system” which can get the needed parts from a model for each view, and do the screen painting, and a “controller” system that deals with the input devices employed by the user, does the sometimes difficult inverse transformation on the view to find what is being interacted with, and informs the model about needed changes.

This more formal system was put into Smalltalk at some point — maybe not until Smalltalk-80 — and both overkilled and underkilled. Part of the underkill was that originally there was a class “Model” with a protocol for things to be viewed by members of class “View”. But since you wanted to view everything, the model protocol should be part of class Object. Etc. This eventually happened.

And so forth. For the “simple things should be simple” part, there needed to be default views that would be automatically included in any class definition. One of these later that was much used—Morphic—was done by John Maloney — originally for Self — and then for Smalltalk. Similarly, having default controllers was very useful.

The whole framework was a very useful way to separate concerns (and I think even more should be done pragmatically with the details of creating MVC subsystems).



What was the output of the computer when there was no screen and speaker?
My vote for the first really working programmable computer is the EDSAC by Maurice Wilkes. One of its intrinsic output devices was three CRT display tubes that looked at various portions of the machine, including the memory.



Using bits in memory to show a tic-tac-toe board.

So: the first real computer did have displays — and used standard paper tape inputs and printer (from earlier teletype technologies)



Computer Scientists: if you had to recommend just a single book to try and teach someone computer science what would it be?
My parallel to this question would be to replace “computer science” with “Physics”.

60 plus years ago when I went to college, good answers would have been “Sears and Zemansky” or “Halliday and Resnick”. These were both thick tomes aimed at more than a year of first year Physics, and covered most important aspects of the field in a first encounter form (but with enough math depth to be like more advanced courses.

When I wrote this answer, there were 2 other answers and a comment. The recommendations were two books about programming, and one, more general, but without a needed next level of detail (the Hillis book “The Pattern in the Stone”).

I like all three of these books, but computer science is not the same thing as programming, so I recommend “The Pattern in the Stone” quite a bit as an intro to “computing” — it’s a terrific book — but it doesn’t have needed details, etc.

Nowadays, there is quite a bit more confusion about what “computer science” actually should mean. Most students I’ve asked over the last 25 years or so, have given an engineering — rather than science — definition. In the 60s, the first Turing Award winner — Al Perlis — said “It’s the science of processes: all processes”.

Science is trying to make theoretical representations that mirror phenomena and their possible causes. The phenomena could be generated by nature or via constructed artifacts (like bridges and dams, etc). In the latter cases, we have both theories that guide the constructions and the usefulness of trying to find better theories for the artifacts. In the case of programming, we are essentially getting a computer to run a theory to produce phenomena.

And so forth. I’d love to see a single book that has that level of sweep! And there might be one that I don’t know about. Right now, I think it would require cherry picking for topics from a variety of separate books and papers combined with a particular pretty unified conception of what “computer science” today should be about.

The lack of this makes me think that we don’t quite have a real field yet, in the sense of “science field”. But I do think that you could do a lot by fleshing out Hillis’ approach to encompass Perlis’ early posit. For example, you could look at an amalgam of approaches like that of Nissan & Schocken, and of SICP, and general systems (and these can be simplified without removing depth, etc.).



Can they discover a way for programming digital devices without programming languages? If yes, like what?
Punched card machines, e.g. from IBM were digital devices that were programmed with plug board wiring, not programming languages. Some of them were quite capable.


I did a little of this in the early 60s before learning the 1401 in the Air Force …



The dominant paradigm is modern programming languages is Object-Oriented. Why should developers abandon this paradigm and adopt Golang if their application doesn't have huge number of concurrent IO requests?
As I tried to point out in a previous answer, I don’t think that “object-oriented” is a “programming paradigm”; instead it is a way to define computational things at will (because the building blocks are semantically whole computers). So it is basically a meta-framework.

I think a “paradigm” for programming should be both a style and somewhat prescriptive as to how to go about using the basic materials to make things.

For example, “object-oriented” allows things like *data structures and procedures* to be defined, or the kind of *functions* that “functional programmers” like, or highly modular systems that integrate separations of concerns, and so forth, including *old style AI* expert systems with complex relational ontologies, etc. Some of these scale well, and most have great difficulties.

I always judge a programming language by how “meta” it can be when needed. What is required when “Paradigm X” is required to best program a new system? One hint: is to ask to what extent and practicality can new features be made part of the programming language being used?

This requires the ability to define and install new features — with regard to syntax, semantics, and pragmatics — at any level of a language (including its own building blocks).

This has been experimented with over the years by various top researchers (one that was pretty nice is described in the first 100 pages of “The Art of the Metaobject Protocol” — this is not the only way this could be done, but it is very suggestive of good pathways). TaotMOP was done almost 40 years ago, so there’s a good chance that something better has appeared since, and I just am not aware of it.

The key idea is nicely independent of the exact methods used, so there is more leeway than most people think. I think this whole question is a difficult area for the computing of today to contemplate without feeling overwhelmed, because it is so design intensive, and today’s general approach is very “agglutinative” with less forethought and overview than most situations actually need.



What does a computer without an operating system look like?
Like many computers of the late 40s, the 50s, and into the 60s. They were “bare” HW plus a few external utility programs, for example, the IBM 1401, or early DEC computers.

Sometimes one would write what today might be called a “mini-operating system” to aid work flow. For example, we had an 8K 1401 in the Air Force, and were able to put a tiny batch job “OS” (in the upper thousand characters of memory) that would automatically deal with running batch jobs, telling the operators what tapes to mount, etc.

We also had a Burroughs B220 in which a data tape had as its first segments, code that understood the format of the tape and “object style” had a table of operations for the semantics and pragmatics of the rest of the tape.

In any case, the notion of an “OS” was early in one respect (as code to provide a better programming environment than the bare hardware did). In many examples in the early days these were supplied by macros from a standard library.

The ca 1960 desire to share resources for multiprocessing and time-sharing forced development of something more like modern operating systems on one hand and memory protection and mapping on the other. The latter lagged the former (for example, CTSS on the 709 at MIT rolled in and rolled out whole core images (it had no map). IBM was very late on most of this.

On the other hand the early Burroughs B5000 had a quite elegant fine grain memory allocation and control scheme, and early in the 60s, not only had a resource shared OS but multiple physical processors, all controlled by the first OS written in a higher level language.



How did Ada start programming when computers were not invented?
You don’t need a computer to write a program, or to execute it. For example, an early algorithm that was done by hand for many years was the greatest common divider (one of the earliest is attributed to Euclid).

One of Ada’s programs was how one would compute Bernoulli numbers on Babbage’s proposed Analytical Engine. The Engine was never built, but she probably executed at least parts of it by hand (and also left a few bugs). Take a look at

What Did Ada Lovelace's Program Actually Do?
In 1843, Ada Lovelace published the first nontrivial program. How did it work?
https://twobithistory.org/2018/08/18/ada-lovelace-note-g.html
to get more of a feel for this program.

A side note here is that in the early days of computing, most programmers did most of their debugging at their desks, via hand execution (I certainly did when I started in the early 60s).



What guidelines would Alan Kay recommend to model something confusing?
First, is the area “intrinsically confusing” or can the modeling clear things up?

I think of most things that have a “systems nature” as being the former, so I like the idea of “multiple covering heuristics” that together can handle the errors etc of the whole — a simple great example is TCP/IP, whose combined half dozen or more methods operate and “stabilize enough” the whole Internet.

In more semantic areas, more semantics — especially that allow reflection — are usually what is needed to provide “multiple covering heuristics”.

I think most people who think of what they are doing as “making and modifying systems” will also approach things from the “error detection and correction” standpoint.

My past research groups got a lot of mileage by building a combination of powerful simple languages combined with the most powerful debugger we could come up with.

I think that — today — most of this needs to be done by a “coherent goal system” itself, and that what’s needed is a second or third order organization of elements that can deal with itself. For example, Facebook has many hundreds of system integrity tests, but they were not up to knowing the goals of the system (for example that it needed to stay on the Internet!) and this led to at least one very large crash of everything.

The lack of semantic modeling and reasoning etc — especially of overview goals that are dynamically involved with software systems — is one of the most glaring lacks in software today (and something that was more or less abandoned 30 or more years ago).

Added April 2nd:

Ken Kahn directed my attention to the excellent, very recent, paper by Doug Lenat: Creating a 30-Million-Rule System: MCC and Cycorp Creating a 30-Million-Rule System: MCC and Cycorp

This is an interesting example of how to think about and deal with “modeling something confusing” i.e. human common sense reasoning — and it is also an example of the kind of system that should be behind the programming systems of the future.



What paradigm is the successor to OOP?
“OOP” (as I think of it at least) is not really a programming paradigm, but is much more a workable “universal definition” scheme, and that is especially suited for defining large systems.

It is basically an abstraction of an unlimited number of complete computers networked together (and where — by definition — the interior of the computers can also be a system of computers (both real and virtual). If you only worry about semantics, this provides a very powerful universal building block at all scales.

Metaphorically, this can be thought of as a “universality” like that of NAND or NOR, vastly scaled up: you can build any kind of functionality you want, but there is no hint of how to design the organization of universals.

In practical terms, since you can imitate (simulate) any idea, you could choose to use a real OOP framework to simulate old familiar ideas — like data, procedures, etc. — or you could choose to use the framework to deal with vast scalings and new situations brought by Moore’s Law.

Historically — and unfortunately — “OOP” starting in the 80s has generally chosen to simulate old familiar kinds of things (via its subset use as Abstract Data Types). For many reasons, this kills “graceful scaling” (and has done so)).

So — for general/commercial use — “OOP” needed to be packaged not as a programming language — too many degrees of freedom for most programmers — but as a framework loaded with powerful design schema to help programmers learn ideas far beyond mere programming. That didn’t happen.

One way to think about “declarative programming” is via an analogy to a system of “simultaneous equations”. If there is a solver that can solve them, then it is extremely handy to just add a new equation for each new situation, and let the solver find a viable solution for all together.

Note that a system of simultaneous equations, quickly gets difficult to gist — and some systems — even of linear equations — don’t have solutions. And many systems of equations don’t accurately describe the desired system.

Eventually, this will give rise to a higher way to think about this e.g. matrix algebra. (But still, how can this be grounded in more *meaning* that transitory goals in the minds of programmers and management?)

Declarative programming is all this and much more (an insurance system I’m aware of has over 100,000 requirements — and the working system is a partially unknown approximation to those requirements).

To me, all this (and more) implies that a next real paradigm (in the deep sense of the term) would be “knowledge based system building” of various kinds.

I think the minimum would be to have the system pretty much understand the goals of any new system added to it, so it can do a lot of the feasibility checking (and much deeper) that humans sporadically and randomly do today.

Such an active knowledge context, for example, would have prevented the recent Facebook crash (in knowledge terms, Facebook’s code didn’t even know that it was hooked up to the Internet and that it didn’t want to unhook, etc.)

Making a minimum knowledge context — and languages that could be in terms of it — is a monumental amount of work and invention (not unlike the amount of work and invention required to come up with our current wave/paradigms of computing).

I don’t see much taste for this amount of work and invention in the field today, nor any inclination from funders to try to do a restart of the field.

Paradigms in Physics are large qualitatively separated and different whole points of view (like Newtonian Physics vs. Quantum Physics). If we take that to heart with our field, then we should be looking for something revolutionary (and likely: large).



What does Alan Kay think about Douglas Lenat's Cyc?
CYC is such a large complex and long lasting effort, that it doesn’t fit within the scope of a Quora answer. But as with most artifacts in computing, commentary can be somewhat separated into the *idea* of a system, and the processes to *implement* the system.

The original idea that led to CYC was articulated by Doug in the early 80s as: “Instead of automating the words in an encyclopedia, we should automate the ‘white space’ “ — that is, we should automate what a reader of an encyclopedia needs to know in order to read an article in an encyclopedia, i.e. all the commonsense knowledge that is used as context. And let’s do it as an “engineering project”, for example, can we (by brute force if needed) create an artifact that can respond really flexibly (rather than brittley) to many different kinds of contexts, etc.

Doug likened the expert systems of 40 years ago as brittle islands, and what was needed was a “commonsense canoe” as a universal fall back process to navigate where there was little or no expertise.

This is a great insight and idea. It is pretty much lacking in the so-called “AI” systems of today (and it needs much much more than ML type correlations in order to be realized).

An enormous amount of work went into a number of CYC systems, which wound up with millions of relationships to try to model commonsense reasoning. These went deeper than any other artifact that I’m aware of, and had many important successes. This work had to struggle in the “AI winter” as ML drew interest and funding away from the deep requirements of “real AI”.

I should be clear here that the CYC symbolic approach is not antithetical to ML: both are needed. The problem today is that the symbolic approach has been omitted or is too weak to do its deep part of the job. In today’s terms — now that Kahneman wrote his book on Thinking — what is lacking is the deep slow thinking in terms of models of the world, rather than the fast correlations and reactions of the Type 1 quick neural apparatus.

A bottom line here is that CYC is well worthy of study, both for what it can do that other systems can’t, and also to learn more about how such a system should be organized for a next pass.

In any case, we should be grateful that Doug Lenat has been a bulldog on this project. Without his persistence, we would have no artifact “good enough to be criticized”.

Ken Kahn provided a very useful comment below that I’m copying in here:

This led to me to find and read the excellent, very recent, paper by Lenat: Creating a 30-Million-Rule System: MCC and Cycorp Creating a 30-Million-Rule System: MCC and Cycorp

And the paper ends with a nice example of how ML and Cyc can work together as Alan pointed out.

One minor quibble is the discussion of expert systems which while it was popular at Stanford, a few other places, and some startups Lenat describes, but it was NOT popular at the MIT AI Lab, Yale AI, or some parts of CMU AI.



Where does Alan Kay see computing in both science and industry going, in terms of systems and how we think about them?
I’ve been much more effective at doing things than in armchair predicting, so I don’t know what to say here.

Over the last several years I’ve been asked to do a number of talks on “Software Engineering” (they are on YouTube), and each talk took on different aspects and problems, especially in industry (which is so large as to defy easy generalizations). Still, the latter parts I did look at, seemed pretty far off from much better much more scalable approaches, and their influence on university “computer science” courses mostly disastrous.

As to “systems”, I think there has been very short shrift all around — in many ways, the past of both CS and SE have drowned out both good ideas in systems research and also prevented needed inventions.

To just pick one area, “graceful scaling” — a proper area of systems research — is pretty much not found at all in industry. “Systems with integrity” — in a wide variety of meanings for this phrase — are essentially non-existent — witness recent meltdowns e.g. Facebook’s.

Systems meanings and reformulations etc is another big area that should be taken up in the large.

So, the most I can say is that I don’t feel good about current practice — quite queasy actually — but this not much of a “prediction”.

The initial answer to this is given above. But I just realized that “science” in the question might mean “Science” rather than CS. The former is about negotiation between teased out phenomena and the models we make to try to explain the phenomena — the former is “external” and the latter “between our ears”.

As Judea Pearl has pointed out, ML is pretty much large scale “curve fitting” and that “correlation is not causality”. But the kind of loose coupling and the size of system and training sets today has allowed some progress to be made in previously vague areas such as visual and audio representations, and with various large data sets.

Unfortunately, this has quite overshot where it is actually useful, to become a blinder more dangerous way to try to compute in general.

Correlation is good for drawing attention, but it would be nice to see computing catch up with real causality and causal thinking and reasoning. This also requires a kind of loose coupling, but one much harder to set up and keep sane because of the much more stringent modeling that is being done.

In theory — looking at our present from the past — today’s and tomorrow’s computing could be really usefully put to work to start creating and exercising real semantic models of the complexities of our world — and to be able to tell us and teach us about itself.

This fits into another large facet of “systems research” and it is a good one.



What's the point of life?
The point of life is to live life with a point



Has Alan Kay voiced any opinions about cryptoeconomics, blockchains, and related "crypto" ideas currently gaining a lot of exposure?
I haven’t. I feel I need more supported opinions — right now I have “feelings”



Is it true that the first computer was a loom? How is a loom a computer?
Here is an answer that specifically covers your question about looms: How was the first computer operating system made if there were no programming languages?



How and when did digital computers come into existence?
This question needs to be crisped up a bit. “Digital” happened before computing. And many computers were not programmable. Computing was done long before computing machinery was made and worked.

Etc.

Lots of people realized it was possible, and wanted it. I tend to give the prizes to the first all around “everything” with both hardware and software, etc. I like three groups of pioneers: Konrad Zuse in Germany during WWII, Maurice Wilkes at Cambridge, and Williams et al at Manchester, with the latter two groups setting up the earliest working foundations after the war.



What do you think about the idea of writing at least parts of law (some legal acts) in a technical language that would actually be a programming language? (So "computable law")
I got asked to give a keynote last year at the Stanford FutureLaw conference on Computational Law (yes there is one). Start at 1:25 if needed.



All of the existing programming languages are based on English. And some are far more problematic, as seen by the code poem "Black Perl." What would an inclusive, progressive programming language look like?
Start by looking at “chimera” artificial languages, such as Esperanto or Interlingua. Personally, I don’t think this helps much if at all for programming.



What does Alan Kay think are the most interesting people, projects, and organizations working on education today?
I’m not comprehensive enough to give a good answer. But the most interesting computer related one is definitely Bret Victor’s “Dynamicland” (check it out on YouTube).



Why did the designers of Smalltalk decide not to have traditional operator precedence for basic arithmetic? Did this decision lead to a more efficient interpreter?
For the same reason as Ken Iverson chose for APL. When you have an operator rich expression language, it’s better to have one rule than 100. Lisp did it the other way: by explicitly showing the bracketing of scope.



Why did Dijkstra say that “Object-oriented programming is an exceptionally bad idea which could only have originated in California.”?
When I heard this, I thought it was really funny for a number of reasons — Edsger and I were friendly (via Bob Barton) and he loved to come up with snide funny comments.

However, he certainly knew Nygaard and Dahl (of Simula fame) and that they were Norwegian.

He possibly knew that the earlier Sketchpad had been invented in Massachusetts.

He knew — via Barton — that I had my ideas while in Utah. Etc.

Perhaps more interesting is that he was the inventor of the Semaphore coordination mechanism (ca ‘62), and a semaphore is an object that is an instance of an idea about synchronization, each of which has a local protected variable and only two protected operations. An irony is this was a bit ugly to make “neatly” in the Algol of the day, and in Simula (the encapsulation could be violated). When added to a HLL, it was usually done so as a “feature” (a special kind of variable rather than to think through the larger consequences of being able to make many such useful things). (And Hoare’s “monitors” were also objects, etc.)

The other fun side-note is that McCarthy had invented “fluents” a bit earlier, a much more useful idea than semaphores and monitors for avoiding race conditions, and one that was not well enough understood at the time to catch on.

He did like to pull chains …



How might computer science evolve to be worthy of winning a Field Medal or a Nobel Prize?
Or vice versa? That would be interesting also.

In any case, it is strongly rumored that Nobel didn’t like mathematicians, so for whatever reason there is no Nobel for math.

And there is this to contemplate (which equates a variety of prizes that the Nobels don’t cover):

Heidelberg Laureate Forum
Mathematics and computer science have evolved into a matter of utmost importance in our modern society. We are confronted with the product of mathematical and computational research in all situations and aspects of our daily lives. To ensure that this technical revolution continues to thrive, science in all of its facets has to be persistently promoted and encouraged. The Heidelberg Laureate Forum rises to this challenge by bringing together the most exceptional mathematicians and computer scientists of their generations. Each year, in the last complete week of September, the recipients of the most prestigious awards in mathematics and computer science, the Abel Prize, ACM A.M. Turing Award, ACM Prize in Computing, Fields Medal, IMU Abacus Medal and Nevanlinna Prize, meet 200 selected young researchers from all over the world. Participants spend a week interacting and networking in a relaxed atmosphere designed to encourage scientific exchange.
https://www.heidelberg-laureate-forum.org/



What are Dijkstra's major objectives for the development of structured programs?
It’s very difficult to do sufficient justice to Edsger. Besides being a foundational computer scientist — in the original strict senses of the term — he was also one of just a few critical gadflies for our whole field, and especially both CS and SE.

One way to think about a great person is that they are “great and interesting” whether they are right or wrong, or in between. This is because it is their unique perspectives that really count: they help us see from directions we couldn’t.

Those whose tiny sense of self and outlook allow themselves to be insulted by the great gadflies completely miss what “great” and “gadfly” and “progress” are all about.

One of the difficulties in dealing with the ideas of a great person is to be distracted by some of the “really good stuff” they are able to accomplish — which tends to be more rooted in an historical time — and to miss the “really great stuff” they are advocating — which tends to be for much larger durations.

In other words, the bug is to mistake the artifact for the bigger underlying idea. This has happened with a number of greats, including Dijkstra, Bob Barton, Ivan Sutherland, Doug Engelbart, etc.

One of Edsger’s greatest lifelong interests was to find out how to design and write programs that really worked and were “bugfree” — including “knowingly and meaningfully” bugfree.

Following the idea that a programmer *should* have enough of a goal in mind to both try to write a program to do something, *and* to be able to tell if it works as intended, what he wanted was to find ways to *get the goal out of the programmer’s mind and into the code*.

Most goals are predications, and computers present the twin problems of *representing* predications and *running* them. The systems that could do this really well — like Sketchpad — though incredible for any time — were not general purpose enough to do either comprehensibly enough.

So the question is “how can you organize — and confine — what the computer does do to approximate “runnable math”. The vast degrees of freedom of computing are fabulously powerful: how to be able to use as many as possible really safely?

Edsger came up with numerous approaches to this, many of them important. With reference to the question here, he was not looking for a religion or for “golden methods” (this was more the way Wirth thought about this).

You can learn a lot about the way he did think about things by looking at his extremely early Algol 60 compiler, followed by his approach to the THE operating system for the EL X8 computer.

One of the big ideas of the 40s — especially 50s — is that the notion of a mathematical function could be imitated very well, if done with care, and — even better — that the idea could be widened to provide a machine independent abstraction for an idea, and especially for programs that could accomplish goals. This created ways to create a design language and scheme with separated concerns for design, but provided ways to combine the concerns in higher level more goal oriented ways.

This quickly gave rise to several different kinds of programming cultures: one pretty ad hoc, and one “like Dijkstra’s” i.e. an attempt to be as “scientific” and “engineering” as possible with respect to design and building.

The “like Dijkstra” one had the disadvantage of not knowing how to “do everything right” but it did have many starts to deep and correct criticisms of the other culture — which pretty much did everything “wrong” and were so blinded by pragmatism they couldn’t see it.

I think these two extreme cultures are very prevalent today, and both are quite entrenched — from many reasons besides personality leanings — I think there is a lot of the “loss aversion” and “sunk cost fallacies” hurting progress on both sides as well.

A way to end this too long answer is to look at Edsger’s quip that “programming is logical brinksmanship”, and realize that the largest psychological difference between the two factions is that his strict computing and engineering approach wanted to avoid brinksmanship via more care and art. The term “structured programming” as used by him was larger than what it degenerated into.

A companion quote from his contemporary Tony Hoare is: “Debugging is harder than programming, so don’t use all of your cleverness writing the program!” It’s a reasonable generalization to say that many of the computer scientists, software engineers, language designers back then were trying to find way to write programs that could be reasonably debugged. It’s also fair to say that most programmers today code expecting their program to run and be easily debugged (this doesn’t work well).

Another quote along these lines of Edsger’s that is quite telling 60 years later today is:

At least in Holland, the intellectual level needed for systems design is in general grossly underestimated. I am more than ever convinced that this type of work is *just difficult*, and that any effort to do it with other than the best people is doomed to either failure or moderate success at enormous expenses.

All the best systems designers I’ve known over the years have put a lot of effort into *confinement* — and have come up with numerous mechanisms both hardware and software — some of which are completely critical today.

By in large, most programmers violate useful and needed confinements right and left regardless of the tools they have (some of which can really confine well). Or what is done is via extreme kludgery (naming 5 of these right now will help understanding of this note)

The HW vendors especially recently have done very poorly dealing with and helping these issues.

And — it would be great to have those who do realize the importance of confinements to devise much better tools that fit the needs and scalings of our time.

This is already much too long, but I can’t resist telling “The Sphere Story”. One of the lab managers for a few years at Parc was quite a character. He had been the Chief Scientist at a large company, and while there decided that he would like a stainless steel sphere on his desk for a paper weight. He ordered a 4″ one from the shop and eventually forgot about it. 10 months later it showed up with an invoice for $10,000 (about $85K today). After screaming at them on the phone they informed him that he only said “4 inch diameter” with no tolerances, so ….

One of the many reasons that Sketchpad worked so well is that Ivan Sutherland was an engineer’s engineer, and engineers work to tolerances. He decided to make Sketchpad worth within reasonable tolerances e.g. minimizing the least square error fits of the multiple constraints. This allowed very difficult non-linear multidimensional problems to be handled right off the bat.

Compare this with the desire of mathematicians to have “not-true” mean “false” (disclosure: I have a degree in pure math). I remember being shocked by many things in Sketchpad — and this was one of them!

And compare this with the amount of “noise-limiting and correction” in almost all other parts of computing. If you are “thinking system” you are going to have noise and will design the system to work in spite of the different kinds of noise present (including human beings!). It’s that a very different kind of mathematical thinking has to be done besides “A is not not A”.

Note that this doesn’t invalidate Edsger’s large goals, but it shows how poorly the early conceptions of programming have scaled. This non-system view is present in most programming languages in heavy use today, and just doesn’t match up to what is actually needed.



What were interviews like at Xerox PARC?
In brief: in the first few years of Parc, Bob Taylor wanted “only top people” (meaning “really top people”), and wanted to eliminate rivalries as much as possible.

So he set up a laborious process in which everyone had to interview a candidate, and then discuss, and take a vote that had to be unanimous for the candidate to make it. This took forever.

Of course we all bitched about this, but Taylor’s process really worked. Each new star was greeted with open armed warmth, and they immediately fit into the somewhat ad hoc non-managed research collaboration that Parc enjoyed.



Why is human communication being advanced by so many revolutionary and very popular technologies while human collaboration isn't? In order to communicate, one must first collaborate, right? Why so much help to "share" and (almost?) none to promise?
There is a lot of excellent content related to this question already on Quora.

It’s worth pondering two built-in human drives — competition and cooperation — and how they have played out in various cultures over history. We are social, and both want to (and generally need to) live in groups to survive. This requires degrees of cooperation. But then we generally compete at most levels.

I think the large claim that most advances in human civilization have come mostly from cooperation can be substantiated (and that various forms of competition are mostly not positive, and even when they seem to be, are of less worth than cooperation would have been).

And I think that we have to consider “human communication” to be much more than getting signals of one kind or another from human to human. It’s worth comparing the thought patterns of oral societies to those with writing and schooling: they are qualitatively different, and it is the latter that is associated with the building towards “civilization”.

Most recent media technologies — from the telephone on to radio, TV, etc — have facilitated oral modes of thought, including the oral structuring of thought (e.g. chat and Twitter). I.e. the facilitation is of means that require little to no learning, and thus require no major changes in skills/internal-structurings.

One of the large shifts in perspective about learning media in the 20th century is the realization that it is the *difficulty of learning* that requires large changes of skill/internal structurings, and if this is coupled with new powerful ways to represent ideas, large positive changes can be expected. These insights were famously associated with McLuhan, but many others contributed — Innis, Havelock, Ong, Eisenstein, Postman, etc.

Engelbart’s large contributions to the world included the idea that a reasonable “augmented human” is *not* created by just giving a powerful tool to a person.

Instead, he pointed out that (a) “augmentation” included most strongly the *non-optional requirement* for deep training and education to acquire deep skills in using the great new powers of technology and (b) that since most things are done by humans in groups, it is critical to also “augment the collective IQ of groups” with training/education to use powers with collaborative tools.

One of the underlying ideas here is that available powers plus the impulse to compete generally results in poor to bad uses of the powers. Simplistic image: cave people with atomic weapons (which is pretty much what we have today).

Another way to look at this issue is that deep training/education is part of what “developed cultures” require in many different areas — including science, engineering, etc. and in developed music and arts. They all require quite a bit of education and training to deal with. They turn into trivial and often ultimately dangerous pop culture expressions and weapons without the deep prep.

So — to gesture at the question — I would say that “human communication” with respect to important discourse has been mostly retrograded by poor untrained uses of modern technologies, and that our stronger competitive impulses are overwhelming needs for cooperation.

McLuhan pointed out in the early 60s that a networked world would produce a kind of “global village” (which he was not in favor of) where, among many results, would be that everyone would know what everyone else was doing. A disastrous consequence would be massive loss of identity, and fierce efforts — including war — to reclaim a feeling of it. One of the manifestations would be demands to *participate* but not coupled with interests to “cooperate”.



Who is the person to whom the invention of the computer is really credited to? There are different views on this particular issue; Alan Turing, John Von Neumann, Charles Babbage?
Goethe once wrote: “We should all share in the excitement of discovery, without vain attempts to claim priority”. I like this because it invites celebration of many ideas from many sources at many times. And it also still allows us to identify real “superheroes” whose efforts made significant qualitative leaps — these people have been critical for progress, and I think it is important to identify as many as we can.

It’s also important to keep in mind that “computation” created the paths for “computers”. You seem to be asking about the latter, but it’s the former that is more important (and which includes the latter). The former has to do with representations and manipulating them, including algorithms and notations — so coevolved with the development of maths — and the latter has to do with making machines that can help do this (and many analog means for computing can also be found in deep antiquity).

I’m guessing that by “computer” you also mean “stored program computer”, which is quite a subset, albeit an important one.

For modern times … It is possible — even likely — that Babbage and Ada had a good idea of the universal nature of being able to manipulate what they first thought of as representations for numbers.

I like to start with Charles Peirce in the latter 19th century, whose analysis of Boole’s book produced the universal logic operation of “NOR”, and who said that you should be able to make a computing machine from these.

During that period, there were many paths taken to represent and remember ideas via mechanisms (some of the earliest most interesting ones for the combination actions of pipe organs). An electronic (vacuum tube) digital memory — flip flop — was made by Eccles and Jordan in the early 20th century, and much experimentation was being done using relays for both calculation and remembering.

I think Turing’s paper in the 1930s was quite significant — not for the math — but for the essentially engineering approach he took, and especially the demonstration that very little mechanism plus general memory for “marks” could do everything. This strongly suggested how practical people should go about making stored program computers, and all that happened subsequently used Turing’s model. (What is called a “von Neumann machine” today is really a rendering of a universal Turing machine.)

However, the work of Konrad Zuse during WWII seems to have been mostly motivated by his own ideas — some of them more advanced than the rest of the world. And many secret, practical and large efforts were made at the Bletchley Park code breaking center during the same time.

So: two things to keep in mind (a) there was a lot of experimentation going on regardless of Turing and (later) von Neumann — for example Charles Stibitz at Bell Labs made an interactive complex number calculator from relays that could be used via dial telephones, and (b) that the first three stored program machines to work (not counting Zuse) were all British, and had different architectures than the Institute for Advanced Study computer proposed by von Neumann.

An interesting sidelight is that making the “simple general memory” turned out to be much more difficult than the “little mechanism” (early on using vacuum tubes). The latter were big, expensive, etc, but relatively easy to deal with. By contrast the various attempts at using mercury delay lines and persistent spots on CRTs were shaky and fraught with errors. It wasn’t until ca 1952–3, that Jay Forrester made coincident current core memory practical for the Whirlwind computer at MIT. This launched the computing industry.

So let’s just “share the excitement of discovery without vain attempts to claim priority”!



Why do some media amplify cognition more than others?
A comprehensive answer is beyond the size of a Quora response.

But we are “thinking” all the time, and part of our thinking is highly influenced by the information given to us by our senses: especially kinesthetics/touch, visual/configurative, symbols. The latter is usually via primary sense impressions via visual or auditory.

A central question to ponder is whether “thinking” can be improved by practice and by method — in other words, it is an activity that has things in common with physical learning e.g. sports (and this seems to be the case).

One kind of media that can be invented is that which tries to represent what we are used to getting from the world: for example, visual and auditory phenomena, perhaps in the form of paintings or movies or recordings. This is both of great convenience and also requires very little new learning to be done in order to make use of it.

The next central question is to ponder a difficult to learn medium, that requires quite a bit of new method and skill and exercise and repetition (even some psychic pain). What state will one’s brain/mind be in after fluency is gained?

Could the new mechanisms set up by learning to make the new medium fluently accessible also be new mechanisms for how thinking is done?

That seems to be the case for reading and writing. Many studies over the last century indicate that literate humans/societies think qualitatively differently than oral societies. The biggest differences — by far — are found with the combination of literacy and schooling.

There is a large literature about this including important contributions from McLuhan, Postman, Ong, Innis, Eisenstein, Cole, and many more.

I should add in here a little about “non-conformal” visuals — such as diagrams, graphs, schematics, etc. These have many of the properties of the “writing and reading of words”. They require a lot more learning and skill training, and what is built in the brain as fluency is attained, also seems to provide new ways for thinking about many ideas — and often new ideas themselves.

(One of many fascinating excursions with these kinds of visuals was done by Ron Thornton at Tufts U in the US, where it was found that (a) a student’s ability at reading and interpreting graphs predicted well their final grade in a first year physics course, and (b) that teaching students to get fluent in reading graphs vastly improved their overall performance (and grades) in the physics course.)

One of many more facets about this large area, is to ponder how humans respond to “wall to wall environments”: such as cities, countries, cultures, etc. and to media environments such as writings, television, social media, etc. In brief, we humans will try to accommodate ourselves to the norms of the environments we are embedded in. This is because we are wired genetically to respond to “cultures” in ways that allow us to live in them.

So, it’s worth pondering the kinds of re-norming that happen to human notions of “reality” from being embedded in 24/7 environments.

Simple answer to the question here is that all media amplify cognition but in different manners and directions, some of which increase “civilization” and some which are “anti-civilization”.

It’s worth learning which and making choices …



How can I incorporate a historic/literary approach to a topic about computer science?
I like to bring in important events in the history of computing when writing about contemporary issues. The main reason is that a very high percentage of computerists are quite ignorant of what has happened — especially since things got rolling in the 40s.

It has generally been the case that the reinventions and rediscoveries have been more like “reinventing the flat tire” than “reinventing the wheel”, and many important ideas from the past have not been rediscovered in any form, and perhaps, more unfortunate, many recent ideas have been worse than any from the past.

If you are looking for examples, you first have to delve into the history of the field. There are many starting places, but I think comparing what Engelbart wanted to do — and what he and his group did do — in the 60s to the almost non-existent notions for the WWW, will surely give you pause. Berners-Lee has apologized for not knowing about Engelbart, but this didn’t help the last almost 30 years (for example, in was in the OS of the Engelbart system that conferencing was not just sharing face to face and voice and screen images but to also provide access to all simultaneously for interacting with the shared content).

As to the “literary” part of the question, I’m not sure quite what is meant — but one part of it could be to encourage computerists to learn how to write well, and especially to include computer processes and interactions as part of the media they write with.



According to Alan Kay, LISP's metacircular interpreter serves as the "Maxwell's equations of software". What are then the Einstein's field equations of software?
I think I said “Maxwell’s Equations of programming” … and I think this metaphor works pretty well for the late 50s and early 60s when Lisp appeared. It showed “in one eyeful” a very powerful way to model a superset of the kind of programming that was being done then.

Metaphors only go so far, so bringing Einstein in might violate the spirit of the original observation. But one could imagine a similar deep succinct treatment of “process-oriented programming systems” that would capture the essence of “internetworked processes”.



What did people at Xerox PARC think of the work Apple was doing?
Here is a different way things could have gone …

The three largest intertwined visions of the computer future we were committed to, all burst into view in 1962: Ivan Sutherland’s Sketchpad, Doug Engelbart’s “Augmenting Human Intellect”, and Licklider’s “Man-machine Symbiosis” and “Intergalactic Network” via ARPA funding.

This was all the same community. Sutherland was funded by MIT/SAGE, Englebart’s was a proposal to the Air Force (later taken up by NASA, and then ARPA), and Licklider was asked in 1962 to use ARPA funds to do something about the future of interactive computing.

By the time Parc was set up in 1970 — and stocked with young researchers from the ARPA community — it was getting to be time for another pass at both interactive computing and the the “Intergalactic Net” — partly because ARPA funding had been curtailed via the politics of the Viet Nam war, and partly because an inflection point in ongoing VLSI was just about to happen.

The basic sentiment at Parc was that we wanted to do “Engelbart, Sutherland, and Licklider for everyone, including children”. A number of systems considerations, especially UI ones, suggested that the larger schemes thought about for the Internetworking of all should also be employed at the personal level (and that this choice would be better and better as Moore’s Law moved along).

The decision at Parc was to build “time machines” that would start appearing in 1973, but would be in the form of networked personal computers of the 80s ca 1986–88 or so. This would provide a platform to realistically solve many systems, apps, UIs, graphics, and other problems that would pay off handsomely down the road. About 2000 of these machines were eventually built.

The catch was that it would be very difficult to do either “Sutherland” or “Engelbart” with these machines. For example, Sketchpad was programmed in terms of “executable solvable requirements” (which is just what the future demanded) but it would take years and more powerful machines than were feasible in the 70s to do this.

And, one of many important features of Engelbart’s system was that any content could be completely shared real-time between any number of users, and all of them could touch and manipulate. This was built into the bottommost layer of the system and was a wonderful thing. Just how to do this gracefully on a distributed system was still an as yet to be invented scheme.

These considerations upset Engelbart to the point of causing a schism in his lab at SRI. One group stuck with him and implemented their next system on a mainframe time sharing system (as the previous one had been) and this gave them what they were used to for a few more years.

The other group — which could see that the problems had to be solved in a distributed way — mostly wound up at Parc. And the first versions of the distributed system were quite a bit weaker in many ways than the mainframe version, but stronger in a few important future directions.

“All other things being equal” (but they aren’t), Parc would eventually have produced a qualitative leap along the directions of Engelbart’s “Augmentation” (that’s what we wanted).

The silicon inflection point that allowed us to make our time machines also allowed simple inexpensive “8 bit micro” computers to touch the consumer and low end business markets. This started a wave in the late 70s that had pretty much nothing to do with “how computers should be used in society”. The people who made them and the people who wanted them had completely different motivations — and most especially completely different contexts and understandings about “computers for people”.

This created a “societal normal” about every part of computing that was both low, but also touched a very large unsophisticated market place.

A deep point is that the Parc and Engelbart approaches required more computing power than people were willing to pay for if they didn’t understand what it was for. The weakness of the 8-bit and early 16-bit micros (and lack of microcoding) took programming back about 15 years into rather low level approaches. New programmers starting learning C etc instead of the much higher level languages that were needed. When Intel et al started to make processors that were aimed at C rather than the high level languages, the game was pretty much over.

If you look at this from the “ARPA Dream” perspective, almost nothing from ARPA made it out except for things for which there was no competition. For example, the Ethernet was manifestly better than other attempts at local area networking. The Internet was manifestly better (and pretty much unique). Apple liked the UI work at Parc but — as Steve later said — missed pretty much everything else.

The Parc work at the end of the 70s lacked a lot of prime needed features that Engelbart had already done. It is doubtful that Apple would have picked these up if they had been there. Microsoft was much less idealistic than Apple and just as unsophisticated.

Different readers will have different reactions to this account.

For my part, after I left Parc in 1980 and first went to Atari and then to Apple, I was shocked at the naivete and general lack of knowledge and understanding of what computing was and could be all about — and especially the lack of any curiosity about possibly great ideas from the past.*

The present that happened was not about great ideas, but about generally unsophisticated individuals wanting to do things that made them feel better about themselves and that they could sell to other unsophisticates (that is not a basis that is going to give rise to great ideas).

Sic transit gloria mundi

* note that Tim Berners-Lee did not know about Engelbart when he patched together what was to become the WWW. He has apologized for this since. But the fact that a computer person did not know prime history is as usual as it would be unusual for a physicist not to know about Newton and what he did. This is the difference between a “pop culture”, and a “developed culture”. And it has hurt computing terribly, almost to the death of its best ideas.



How did people at Xerox PARC navigate dealing with the corporate side of Xerox?
There were different personalities at Parc with differing relationships and needs with corporate; likewise, Xerox was a large rich corporation with many divisions and somewhat like “England before the Magna Carta”.

So some relationships were cordial, some were warlike, and quite a lot in between.

I wasn’t involved in most of this — I had a research group there and lab managers, especially Bob Taylor and Bert Sutherland, viewed a large part of their jobs to be finding ways to “let the researchers research”.

I had pleasant relations with Jack Goldman — a New Yorker and a half — the Chief Scientist of Xerox. I had many skirmishes with Horace Becker, the head lawyer, who was both a bulldog, and one without understanding of how science makes progress. I found out later that what I’d interpreted as “skepticism” from a Xerox planner was actually deep dislike bordering on hatred.

In other words, the usual human circus that happens when some of the participants are not very high minded. All this intensified as Xerox moved from the could do no wrong phase to starting to have difficulties in its markets. Money issues often bring out the worse in limited horizons.



How was software complexity changing as new technologies were introduced from machine code, through assembler to high-level languages and beyond?
A too short but pretty accurate answer is that HLL languages and architectures “almost always” lagged the demands of the software, and this resulted in more complexity than could have been the case.

Some of the exceptions are interesting. E.g. the B5000 HL machine with OS written in ESPOL, a HLL systems language. And: the Alto and Smalltalk system at Xerox Parc were design together similarly to provide a match to the software requirements. I thought Erlang was a good HL solution to the problems of the Ericsson switching system.

But most projects try to tackle newer larger problems with older, often obsolete languages and tools which add on a lot of gratuitous complexity



What is the idea that is better than semphores from John McCarthy in 60s? KEYNOTE 1: Alan Kay - Rethinking Design, Risk, and software watching this on YouTube 38:00
I’ve explained John’s idea of “pseudotime” in a number of Quora answers. Perhaps you or someone might locate these …

Thanks to Dane Clarke for these references:

Here’s a few that are fairly relevant: Alan Kay's answer to What's the best way of combining functional and OO programming in practice?

Alan Kay's answer to What will happen if one rewrites the entire Linux using Smalltalk?

Alan Kay's answer to Rich Hickey, creator of Clojure has said that object-oriented programming is unable to model “time” properly. What does "modeling time" actually mean?




Why hasn't Xerox PARC explored the stylus in the Alto era?
We did aim for the stylus at Parc, and this was from experience with the GRAIL system at RAND especially (and GRAIL featured dragging via a stylus).

My thesis project — The Flex Machine — also had a stylus/tablet that was less expensive than RAND’s and not nearly as good).

The sensor technologies in the early 70s that were good enough for a stylus were much too expensive to fit into the “at least 100 Altos” plan (e.g. a commercial good tablet was about $7000, and the Altos were supposed to be about $15K - $20K total).

A great solution — I’m pretty sure by Chuck Thacker — was to make a small mouse with high res shaft encoders (I think it was about 250/inch) and this could be used to draw with and also had all the advantages of a mouse.

These were in great favor in my group. Eventually they got displaced by lower res easier to clean larger mice: they were not so good for drawing, etc.

Parc also did a variety of touch sensors for displays — the Notetaker came with one, and there were experiments with Altos. These worked for coarse pointing and dragging but not for fine cursor control.

The mouse for the Mac was mostly about reliability — it was large and low res.



Why didn't Alan Kay show up when Adele Goldberg arranged the meeting with Carl Helmers in PARC?
First, I’ve heard of it. When was the meeting?



Why was there a mismatch between Xerox management and PARC?
Parc was set up in mid-1970 as an initiative of the Chief Scientist Jack Goldman and backed by CEO Peter McCullough.

Bob Taylor — former ARPA director, and funder of the ARPAnet — was hired ca Sept ’70 to set up the computing research lab. He was interested in this because Congress was in the process of putting the D on ARPA and changing the funding rules in ways that would hurt the edge of the art research that Taylor had previously backed.

Bob had dealt with companies before, and one of his stipulations was a legal agreement that Xerox couldn’t mess with the direction etc of research for 5 years. He then hired spectacularly well, and things got rolling.

A few years later XDS — a mainframe company that Xerox had bought in the 60s — failed and Xerox had to write off a billion dollars (a lot in those days). This cost McCullough his presidency — he remained COB — and a very poor choice was made to replace him. Meanwhile, there were critics of the “elitist” Parc researchers, etc.

The poor choice made things worse, and was eventually replaced. But in 1976, the desktop Japanese copiers — and a big surprise: the Kodak copier — hit the market and started to put big pressure on Xerox.

The trend here was less concern with the future and more about protecting their “heartland business” (as they termed it).

And the 5 year agreement was now up.

More and more pressure of various kinds was put on any part of R&D that wasn’t generating revenues.

Taylor sacrificed his career to hold the ship together, and most of the edge of the art research at Parc was done successfully, and at “living lab” scales much larger than demos.

Things were quite tough around the late 70s, and Taylor was eventually fired in 1982 — his entire lab resigned with him.



Is Alan Kay the most influential person in the CS and IT fields?
I couldn’t possibly be …



How is it that "coding" is a superior term to "conceptualize", if they mean essentially the same?
As far as my 60+ years of experience goes, the two terms don’t at all mean the same.

In the 50s and 60s especially, “coding” was the act of writing code, and was distinct from “programming” which included design, and often in terms of an abstract framework — such as a flowchart — rather than actual code. The results here would be considered more than a “conceptualization”.

Typically, one started off as a “coder”, essentially hand compiling designs into working programs, meanwhile gradually learning how to design.

Today, I think that most people use “coding” and “programming” interchangeably.



What is Alan Kay's opinion on the Akka actor library?
Hard to have an opinion without trying something major.

However, I like the idea of “actors” for a number of reasons (a) they are a reflection of the way I thought about “object-oriented message passing systems” in the 60s, and instigated Carl Hewitt’s work on them, (b) the actor research in the 70s went further than we did at Parc (our goals were to write whole systems for personal computing on the Alto, whereas MIT’s were to do research).

Both of these paths worked out (we did write the systems we wanted, and the actors work at MIT came up with valuable insights).

A simple-minded theory of mine is that every research project needs a genius like Dan Ingalls, who can successfully turn big ideas into whole systems. The Smalltalk work at Parc benefited from this, and most other interesting languages haven’t (e.g. the lack of whole systems for (say) Haskell, Clojure, Akka, etc).



How do I master the art of Smalltalk?
Let’s widen the question to: “How can I understand and use Smalltalk-like language/systems?”

A few basic ideas here are:

One way to model any system is via parts/wholes relationships where a part can be a whole system, and relationships are in terms of intercommunicating messages.

If the fundamental building block is a “whole virtual computer”, then each part at any level can always be represented *and* the interior will always be safe from external meddling.

(So far this gives us a software rendering of the Internet (which indeed is what Smalltalk was all about — it came from the same research community that invented the Internet, and its invention was contemporaneous).)

We can get something like a programming language by choosing the form of the messages that are sent and how they are received.

We can get as many parts as we need if we have the facility to “instantiate” any thing we make.

We can use these facilities to make our programming language, development system, the systems we want to create, etc., all down to the metal.

All of these facilities are constantly live. And it is possible to make the system out of itself in such a way that it can not only debug itself whilst running, but it can debug the debugger whilst both are running, and — like the Internet — it need never be stopped for any change or repair or addition.

Now here’s the tough part for most computer people at any time, and especially today — and that’s “we still have to do good design!”

A given Smalltalk — like Squeak or Pharo — will have a kernel like the above but will be furnished with a “library” of many prior design choices covering a wide range of needs and felt solutions to this and that. In today’s world “mastering” generally means learning all the choices (very similar to mastering an OS) so that as many of them as possible can be used going forward.

I think this is a good ploy for about 5–10 years of a feature, but is a disaster for very old features to have this power.

And — for example — it would be crazy to use the definitional capabilities of such a system to simply model “bad old ideas” from previous programming experience. For example, an enormous bug in many so-called OOP systems is to use this great power to model old not well scalable ideas like data structures and procedures.

It’s important to understand that the perspective here is “easily universal” e.g. if you feel that a “functional style” is good, it is extremely easy to model that with this “material”. There are no incompatibilities with programming styles here (this is a huge misunderstanding in the “normal” workaday world out there). This is a “meta” approach, and you do need to learn to “think meta” in order to make the best use from it (it is very hard to do good design in any area without allowing some “meta” to be part of the thinking process!)

To me: “mastering Smalltalk” means understanding what it is as a fundamental system building material, and seeing how to make the most expressively powerful thing from this that then can be used to create desired systems for many kinds of uses.

Smalltalk dates to the early 70s and its current general form to 1975–76. It ran on a personal computer with 128KBytes RAM (half of which was used for the display) and a 2MByte disk (somewhere between a floppy and a hard drive). It is of interest today because the kernel ideas were aimed at “the ages” and the sets of choices for the first library of extensions was carefully done (much of the credit for how well this turned out should go to Dan Ingalls).

As an example, here’s a tribute I did for Ted Nelson a few years ago using a revival of a 70s Smalltalk. Everything in the system including tools, development, the equivalent of an OS, and the equivalent of “apps”, was done in a total of 10,000 lines of code. This is why we put so much work into the design of Smalltalk — we felt that the VHLL abstractions we could make would allow a lot of functionality to compactly fit into the Alto — and that the Alto microcode (1K instructions) could run the kernel fast enough to make the result usable. Demo/talk starts at 2:15

Moore’s Law has had 30 doublings since then (and secondary storage even more). This is roughly a change in scale of a billion (some of which was used to create personal computers of much lower cost than the Xerox Alto). Also, Smalltalk was done before the Internet got done, and this represents cosmic scalings.

What this means is that you are much better off thinking things through again from scratch.

But let’s suppose that despite this you want a “modern Smalltalk-like system” to “master”.

To my eye, the kernel ideas have held up pretty well, but there are a few fundamental things that could be made nicer and more clear. For example, though Smalltalk can do any kind of messaging in a system, we didn’t put much effort into multi-machine massively parallel facilities. More effort is needed today.

The most expedient “bad” thing we did in Smalltalk was to use a relational scheme that heavily shared (borrowed from Lisp). This was coupled with “too many dependencies” etc.

Today, one would start with “marshalled objects” that can be transported over the entire Internet, and find ways to keep them as general as needed when reified back into life. This is a really interesting design problem, and a very important one.

One of the implications of scaling is that conventions that work locally start being of less use. For example, names are pretty local tokens, one needs search for global scales (and search by properties rather than tokens). Both Smalltalk and Lisp have this idea at a fundamental level, but not much has been done at the programmer levels to make it useful.

There are many more considerations that come with a “scaling of a billion plus the Internet”. I’ll end with just one of these. Programming traditionally has been about specifying “hows” and leaving the “whys, and the whats” in the mind of the programmer. Encapsulation allows more of a why and a what to be captured. The programming of the future has to start with representations — understandable by programs — of “why” and “what” — and has to be much more about this than any apparent “hows”.

This is a really important long known research problem for which almost nothing has been done or funded over the last 40+ years.



Are there any known jazz guitarists who prefer playing on nylon string guitars, and if so, why do they prefer it, the sound or the feel of the instrument?
Sure: e.g. Charlie Byrd and more. I was a jazz guitarist who later learned classical, and much later started experimenting with jazz on classical. Basically, with my small hands, it was tough to do a lot of the jazz “reaches” on the classical instrument, both via action and scale size. Also, some of the differences in the actual physical frets can add more difficulty. The reward is the reward of the classical guitar expression and its sound. Pretty much every physical property of a classical guitar is more difficult to deal with than with a well set up jazz instrument. The sound and expression possibilities started looking for “crossover” instruments with more accommodating overall properties — of which there are now many. E.g. Godin makes nylon string instruments with better actions, cutaways, etc. The best maker I found was Kirk Sand in Laguna Beach California, who makes superb crossovers of very high quality. I wound up getting two of his instruments. http://www.sandguitars.com/products.htm Eventually age got me and my left thumb joint wore out to “nothing” — some of it from the extra efforts no doubt. Now at age 81 I happily play the lute — classical — starting because of the much lower action pressure, but winding up loving the instrument and its music. It happens that much of early classical music requires improvisation as part of the art, and this is especially true of the lute. In fact, 17th century music in general is rather “jazzy” in many ways, some just like 20th century jazz, and some that are parallels to 20th century jazz. So it’s been fun learning and adapting to this as well.



What is the main difference between a software engineer and a Computer scientist?
Worth starting with “differences between Engineering and Science, in general”.

Engineering was able to predate Science by thousands of years because “things can be made/repaired/etc. in principled ways” without having to understand just why a working thing works. “Cookbooks” of things that are known to work, serve well (as indeed they still do in cooking).

Math started off very cookbooky also, and then was revolutionized by the pre-CE ideas of connected reasoning and proofs.

Science started off very cookbooky also, and the word originally meant “gathering of knowledge” (often via rather indirect ways). Both Engineering and Maths eventually had big influences on what Science thought it was about.

Two enormous leaps in the 17th century for what we call “Science” today were (a) Bacon’s call for “a new science” that should be centrally about dealing with “what’s wrong with our brains” via the invention of heuristics and technologies that can help us do better, and (b) the earliest really good scientific theories as (symbolic) working abstractions of what could be behind phenomena (Newton had predecessors, but he was so far off the charts that new charts were required). This started new qualitatively different paths, and a few more critical ones were added in the next few hundred years.

For example, the clarification between what is “in our heads” vs “what’s out there”. Einstein had a good saying for this: “As far as the laws of mathematics refer to reality, they are not certain; and as far as they are certain, they do not refer to reality.”

An important way to think about this is that “Science” is neither the math, nor the theories, nor the phenomena, but the process of negotiation between them.

One of the byproducts of this “largest thought revolution” was that Engineering — which eventually would be revolutionized by Science — lagged more and more by not taking up Science as a main facet. Because Engineering is mainly about making things, and many things can be made without real understanding, it took quite a while for the engineering fields to even see what they were missing.

Today, we can think of the larger picture as a kind of Venn diagram of Tinkering, Engineering, Mathematics, and Science. The isolated subjects have their own properties and appeal to different personality types, but in practice, most of the best practitioners in any of these are very well versed in the others. This is worth pondering deeply.

One of the largest unifiers and lingua francas is *systems* — a large enough set of perspectives to constitute a whole area of its own.

When something breaks, valuable information and insights are often a byproduct. Many things engineers do affect the safety of people, so the equivalent of the Hippocratic Oath in Engineering is: “the building must not topple, the bridge must not fall, the plane must not crash”.

Good engineers try to break things under controlled conditions so they won’t break when deployed. Good scientists are always trying to break their creations, but know that they never have the final word.

Note: there can be “sciences of the artificial”* for example, a “science of bridges”. Once engineers build a bridge, it exudes phenomena which can lead scientists to better notions/”theories” of bridges. A lovely thing about our own time is the extent to which engineering and science can co-evolve both technologies and theories.

So: there can be a “science of computation” to go along with the many engineering activities that are required to make artifacts (both hardware and software).

The first Turing Award winner — Alan Perlis — was a “large mind” — and pointed out that the best scientific conception of the new field was “the study of processes: all processes”, and that computers themselves were both part of the “process world” and also could serve as the “new math” for representing the processes and notions being studied.

To me — an old fogey from the main research community back then — this seems like the best way to proceed for now (and this perspective really made a difference in my research community’s inventions of much of the interactive computing, media, and networking technologies in use today).

It’s easy to see that this perspective was has not been taken up by most of the people doing computing today. The main activities are more at the tinkering and “making without real understanding” levels than they should be.

The vast gold rush which started in the 80s has produced something more like a pop culture** with pop music and pop values (for example, just compare the ancient engineering morality of safety with the modern computing tendency to try to make things “from matchsticks and houses of cards” despite that much deeper knowledge on many of these issues does exist).

Part of the pop process was that many of the older meanings from back then got redefined downwards to the point where they no longer mean the same things. Examples are: “computer science”, “software engineering”, “OOP”, “AI”, etc.

Just as pop cultures can’t deal with non-linear processes such as the pandemic and the climate, the coupling of Moore’s Law to oversimplified notions of computing has produced not just a mess, but one that becomes ever more dangerous.

* see the great book by Herb Simon

** pop cultures can produce important things also — the big problem of our day is that commercialism has grown the pop cultures to be dominant rather than a small percentage for hobbies and harmless fun.



How can I implement an online microtonal adjustment algorithm to get more perfect harmonics with a method to avoid drift?
We did just that at Parc in the 70s, and not very completely. Today it could be done much better (and probably has been).

The idea is to try to imitate what singers and players of instruments that can shift pitch microtonally (bowed strings, winds, etc.) do when they are playing chords “in tune”. The system needs to take into account the key, the held notes, the kind of chord, and what comes next.

An example of a “problem” that has to be solved is: suppose you are in the key of C, the held note is a C, and the chord is an Ab7 (which would likely have a flatter version of the held C). The system should probably stay with the held C pitch and produce the Ab, Eb, and Gb a bit higher to get a nice major 3rd.

If the next chord is a G7, we can recover to the center pitch we are using for the key of C major.

But suppose the next chord is a Db major (so we just did a modulation). Now we are “drifting” if we relate this to the Ab7 (and we should, because Db major has an Ab note in it).

On the other hand, if we did the look ahead to see if there is a modulation or not, we can figure out schemes to stay in the “key center pitches”.

We can hear the wide 3rd in equal temperament (13 cents sharp) as a “pain”. Flatting this really helps matters. The slightly narrow 5th in equal temperament is less of a problem but we can usually widen it just a little here and there to get more consonant 5ths.

A main principle is that we have a hard time hearing “nanotones” (a few cents off), so we don’t have to get to perfect intervals to get much better audible results. This is what choirs, strings, and wind players do on the fly.

It is likely that a good solution today would not be done just by an algorithm, but would also have stored a lot of common case solutions to the adjustments, and would use these when possible.

My pipe organ has an unequal 17th century temperament to deal with the need back then for “Golden major chords” e.g. for Tunder and especially Buxtehude. So e.g. C major and D major are really in tune — and “Golden” — whereas there is no C# major (because there is no E# note — the F is too sharp). So C# F G# is really out of tune. I’ve been very surprised how many people I’ve demonstrated this to can’t hear it.

Just to get a bit more detailed here. The general problems had been realized since Pythagorean times. Simple put, notes of the same pitch from different instruments (human voice included) will have different timbres because the mix of harmonics (usually pretty much multiples of the fundamental frequency) will be different. For example, a stopped organ pipe will only have odd numbered harmonics in different mixes, a brass instrument usually will have a strong 5th harmonic, etc.

When you try to make a chord from 3 different pitches you will get interactions between the harmonics of all the notes. If you play C E G, the E fundamental will interact with the multiple of 5s harmonics of the C, and the G with the multiples of 3 harmonics of the C. This could sound strained. A solution is to derive the pitch of the E and the G from the harmonics of the C. This will give a “golden major chord”.

Sounds good, so get the rest of the scale likewise. For example, follow the 5ths — try this on a piano — and you will eventually get back to C. But note that, to do this, the notes on a piano cannot be harmonically aligned. Why? Because following the 5ths around harmonically is multiplying successively by 3 and octaves are multiples of 2: there’s no multiple of 3 that is going to give a multiple of 2, so harmonically, the “circle of 5ths” can’t close. This drove the Pythagoreans — and everyone since — batty. If you shrink the 5ths enough to allow them to close, then all the 3rds are annoyingly sharp (welcome to equal temperament!)

An earlier ploy was to look at schemes that allowed good sounding major chords, with especially good thirds. This can be done on a 12 pitch per octave keyboard for a few keys (like my pipe organ I mentioned above).

What if you want more good sounding keys? Well you could split the sharps: so in the place where there is an F# key, you could have the front be that and the back part of the key be Gb.

<to be continued>



What are the mental models, biases, and fallacies that software developers should know?
Only 17?



Is there a more precise way to describe software than with programming languages?
I’m using the definition that “a programming language is something that can take a representation and turn it into something that a computer can carry out”. But we could also think of a language that can describe processes that don’t run on a computer (this was the case for “APL” for its first 6 or more years).

The form of the rest of your question is not clear enough for me — for example, it doesn’t say whether the description is about some software or whether it is supposed to be convertible into the software in some way.

Perhaps you could try reformulating the question …. ?

Added June 2022 …

I think several of the other answers are quite good enough.

But it’s interesting to think about what “precise description” might or could mean here. I used to be a mathematician, and part of the college education was about how many maths got much more precise starting in the 19th century (for example, calculus was pretty offhand about limits, convergence/divergence, continuity, etc.). Just what was meant by “non-rational”, countable and non-countable, etc. was murky.

In parallel, many ad hoc workable things in engineering, eventually got much more precise — for example, sound recording was in use for quite a few years before much better ways to understand and describe what was going on were invented.

A big fun thing about “digital” were the many ways to remove noise from noisy carriers, to get virtually 100% precision in creating markings. An expanding question is “what has to be done to get precise about what a marking stands for?” One pathway brings us back to mathematics, and whether what we can capture what we mean and intend with symbolic representations/definitions can stand enough on its own.

A simple minded answer with regard to computing is that a program in a programming language only has a precise meaning with reference to what happens when the program is executed (either relative to a machine or by a human: in possession of a “workably precise” description of the programming language). This can be made to work pragmatically enough …



What does Alan Kay think about computer algebra system (e.g. SymPy and SageMath) usage in mathematics education, before graduate school? Would widespread use of powered cognition assists like these rot younger students' brains as some people fear?
“Education” and “Training” have some overlap — along the lines of “understanding” and “doing” — but the distinctions are quite useful to keep in mind. In all cases we want something beneficial to change/grow in the mind of the learner.

This implies that we should always be asking the question “When should it be easy, and when should it be hard?”. The former will not induce much change, whereas carrying through with the latter will, perhaps at the cost of time and discomfort.

In the 70s when only a few children were using computers, we asked the group we were working with at Xerox Parc what they thought about what the computers were helping them do. They discussed this quite a bit and decided that it would not be a good thing to use computers to automate the center of your interests, because it would get in the way of learning.

Musicians are quite familiar with this, and many music teachers will encourage students to learn to sing, and a “singing instrument”, as well as the conveniences of instruments like the piano — which do a lot for you, but they also make it easy not to pay attention and learn important things about musical expression. Many of the difficult things in music are just the things one needs to learn in order to induce a more musical brain/mind.

Another example, is that it is well known that learning to fluently read written ideas produces large differences in cognition compared to that found in illiterate societies.

So, when it comes to mathematics, we have to use these principles to try to figure out “When it should be easy and when it should be hard”. We don’t want gratuitous difficulties, but we do need to figure out ones that will promote real growth in mathematical thinking — and if possible in useful mathematical knowledge and skills.

In all cases, we also have to take into account the differences between Kahneman’s “Fast thinking” and “Slow thinking”, especially that many of the ideas for which we have to use slow thinking to reach understanding, will also need training to develop fast skills in doing.

When we have a “helper” in the form of a person or a tool, we have to figure out when we just want to be practical and “get an answer” or when we need to understand something deeply before using a special tool. Most of our species chooses the former — we are nothing if not expedient — and this is very often not a good habit to get into.

Here the subject is mathematics, one of the few human inventions whose “knowledge” can be completely understood — and this is also one of its gifts: the clarity of understanding the relationships of abstractions without the murkiness of trying to use them in science to approximately describe the real world. If you don’t understand, then you can confuse the language with the phenomena, and miss the murkiness that is the very core of sanity.

Ameer Fazal asked in a comment below:

So, it is better to learn arithmetic, geometry, algebra, trigonometry, analytical geometry, set theory, calculus and so on with good quality books, working out problems with pen and paper. And with people and tools for various other reasons but primarily somewhat in an autodidactic fashion.

I think this question in the large has to do with what citizens in a democracy should be required to learn as children. I think children do have rights regarding what they can be coerced to do, but I also think e.g. that society has the right to try to teach all children to read and write. This is a clear conflict of sorts, but I’m on the side of mandating the teaching of reading and writing as part of preparation for adult citizenship.

I also don’t see how an adult can function as a voting citizen without a threshold level of science and maths, and this includes learning the habits of reasoning, questioning, understanding about “noise” (especially generated by our brains, etc.)

It’s interesting to try to work out what the threshold of these fluencies might include. I don’t think they are what schooling currently thinks about either maths or science — both would be quite different (and almost unrecognizable to most adults today).

In this light, it is worth pondering the future of “algebra” — both in its school use, and also the much more illuminating ideas of “Algebras”, especially “Linear Algebras”.

One starting place for a better “K-12 mathematics” would be to just look at the maths that provide the mapping/supporting representations for scientific models, with the entry being the world children experience. For example, Seymour Papert had many wonderful insights, including: that the experience of movement by a child traces out a version of Gauss’ “Differential Geometry Of Vectors” and this can be enhanced with toys, ways to talk about the movements, and ways to think “mathematically” about what is going on.

For example, consider making polygons by acting out “go a distance, turn an angle”. One soon discovers that to make any polygon (in fact, any closed figure) the “total turn” will be a full circle of 360 degrees. (This is a more powerful theorem than the internal angles of a triangle, and easier to prove.) From this it is easy to figure out polygons, etc.

This is one entry to thinking in terms of vectors, and it works powerfully in 1, 2, 3, 4 etc dimensions. The genius of Papert produced here something that is simultaneously in the sensual world of the child, that is one of main forms of mathematics used for science, and something that is in the reasoning wheelhouse of children for *really doing math* — as Papert noted “Teaching children to be mathematicians as opposed to teaching them about math”.

This is the same distinction between helping a child learn to actually play a musical instrument vs putting them in a “music appreciation” class. We should be able to see that — for many really important subjects — “standard schooling” is way off (and to the point that it actually hurts the children: i.e. it’s quite a bit worse than “neutral”).



What was the world’s first high level programming language in 1957?
Is this really two questions?

The simple answer is FORTRAN: it was by far the most developed, polished, and optimized HLL as of 1957.

At the next — less simple — level are quite a few things in the works.

Besides what Grace Hopper was doing — e.g. with “FlowMatic” — “IAL” (what is now known as “Algol 58” was in the works (and it was more “sweetly conceived” than FORTRAN).

On Whirlwind at MIT, there was already a “quite HLL” *interactive* language that was pretty darn nice — called “George” — by Hal Laning and Niel Zierler (one history said it was implemented in 1952).

Laning and Zierler system - Wikipedia

Sanjay Vyas asked: There was also Plankalkül?

Reply: I thought about that — and I’m a fan of Zuse — but decided to restrict to languages that were actually implemented on either side of 1957. Here’s a sentence from Wikipedia:

“In a 1957 lecture, Zuse expressed his hope that Plankalkül, "after some time as a Sleeping Beauty, will yet come to life” Plankalkül - Wikipedia

A strong argument in Sanjay’s — and Zuse’s — favor is that Iverson’s “A Programming Language” existed for years — and a lot of programs were written in it — before it was implemented. Most of us date this language to 1962 (the book) rather than to the late 60s when “APL360” was implemented.

If we follow that line of reasoning, then we have to look at Ada’s program for Bernoulli numbers, which was also just on paper. She certainly had a formal organization for this program.



Is it possible to create imaginary atoms of matter on a computer using only software graphics?
I typed “simulated atoms” into YouTube and got many different videos showing various levels of physical simulations of atoms rendered using “software graphics”.

Several of these show actual constructed atoms from elementary particles. My favorite was the one that showed a proton’s quarks interacting with electrons. That was one visualization I hadn’t seen or even deeply thought about before …

There are also some very nice crystal and biological renderings using the basic “chemical physics” of atoms.

I don’t quite understand your phrase “using only software graphics”.

(a) at some point what the software does has to move through hardware for us to see an image,

(b) the physical simulations are mathematical — like the viewing simulations — but using somewhat different math and physical modeling (but all can be done in software — and all can be done in either a regular CPU or in a GPU).



What do you think of RMOX operating system from U Kent in England? I am amazed it is 32 bit only and not 64 bit (maybe secretly used by Englands government) https://www.cs.kent.ac.uk/projects/ofa/kroc/rmox-cpa.pdf
This is a “CSP” influenced design. I don’t like to criticize unless I’m willing to do it in substantiated detail, so I’ll just say here that there are other styles for dealing with concurrent modules that I like more.



I want to understand and learn Lisp. I saw one of your YouTube videos where you said few understand it even today. What is the best way to learn Lisp and the creation of really good software that like the internet is reliable?
Lisp presents itself as “a kernel language with a library” and as a “metalanguage” and “metasystem”. It’s the latter that I think few in computing — in terms of percentages — have taken the trouble to learn and ponder deeply.

There are two main aspects to “meta” in our context here.

The first is how to describe the important parts of a language in a small and direct enough way to provide illumination. McCarthy’s original definition of Lisp in itself does this extremely well.
The second important aspect of “meta” is “reflection” — which means that programs in the running language can examine the mechanisms that are making them run. Lisp is very amenable to this, and some Lisps have been implemented to be reflective in considerable depth (note, that (1.) doesn’t imply or require (2.).
In the context here (1.) will exhibit — “simply” — how to make a mechanism that will run any program in the language and will produce the same results. A Lisp “meta-program” for this is very small and clear.

“Reflection” here can be approached by thinking about what kinds of live access are needed to allow a really good debugger to be written in the language so it can be a live process in the language. (Note that JS — which has some Lisp-like features — is quite poor with respect to complete reflection.)

A next stage of reflective thinking would be to ponder what it means to extend the core meanings and mechanisms of the language at the same level as the existing kernel. Some excellent thinking about this can be found in the next four paragraphs.

i. The basic idea is that you don’t need much to make a “universal computing material” and that can be used very compactly to represent itself in itself. This produces something that is both extremely powerful in its reach, and quite small enough to think about mathematically, as well as for writing programs.

ii. I liked this basic approach very much, and we used it for Smalltalk “down to the metal” at Xerox Parc (we didn’t base Smalltalk on S-Expressions, etc. — the treatment was in direct terms of “objects”). It was quite good with regard to both (1.) and (2.) above.

iii. The “Art Of The Metaobject Protocol” does even better in some respects for “Lisp with Objects” — the early examples in the book are quite illuminating.

iv. I also recommend Dave Fisher’s 1970 CMU thesis “A Control Definition Language” for an advanced McCarthy-like meta and reflective approach to extensible languages.

An historical approach to learning “deep Lisp” is to get the “Lisp 1.5 Programmer’s Manual” and work through the first 13 pages under the direct tutelage of John McCarthy. Page 13 takes most people a few deeply concentrated hours “on a Sunday afternoon”.

Here are a few books since “The Lisp 1.5 Manual” that also include good travels into “Lisp-meta”.

A classic — and now (also) historical approach — for “Lisp” (now in terms of “Scheme”) is to get the Abelson and Sussman book “The Structure and Interpretation of Computer Programs”, which has served as the textbook for learning this world of thought. This is longer and gentler (most computer people pronounce it “difficult”).

There’s a book “The Little Lisper” that many like.

This book is quite good to take you though this kind of thinking:

Essentials of Programming Languages

The last part of the question is a bit trickier, and I don’t think I know of a book that gives a good account of what is known (for Lisp or any other language). There are several parts to this (a) how to make an “eternally alive” system that basically can’t crash and has enough meta to participate in its own improvement (b) how to write really good software.

“(a)” has much to do with “confinement”, “fail-fast”, and ability to always have processes that can look at other processes. The classic Smalltalks, and also Squeak Smalltalk — whose chief architect and builder was Dan Ingalls — can be studied. It is quite possible to go a bit further in a system like Squeak and even have it be able to replace its own kernel without having to stop it.

“Meta” has gotten richer since the early days of Lisp. For example, Dave Fisher’s CMU thesis on extensible control structures. I’ve mentioned Smalltalk. I like the motivating chapters of the “The Art of the Metaobject Protocol” a lot. I like the way the LOCUS OS was organized. LINDA has a lot to ponder. And, especially, there is Erlang — especially its systems principles — to look at. I think the “simulation of time” ideas of McCarthy are key.

Etc.

“(b)” has a subjective side (“good” software needs “good” people), and also a number of principles. One is to separate meanings from optimizations in such a way that (i) the meanings/intentions are debuggable and are readily readable and understandable by the humans involved, (ii) the system will run as desired with the optimizations turned off (a supercomputer may be required). This is a computing version of the CAD<->SIM<->FAB process in established Engineering (with a Big E) disciplines.



Are business interests and principled engineering often at odds?
Take a look at “Challenger” and Thiokol, and the 737 Max and Boeing — as starters …




What would "software CAD/SIM" look like?
Let’s try as close an analogy to modern Engineering (with a Big “E”) as we can. One of the processes is called CAD<->SIM<->FAB(rication), and the idea is to solve as many design problems as possible in the CAD<->SIM part.

CAD is a set of tools for making progressively better models of what is in mind that can be put through their dynamic (and otherwise relational) paces via SIMulation. A key principle here is to capture the “meaning” of the model and debug it. In many engineering areas, the SIM part is computing intensive, and may require a supercomputer to pull off.

The “FAB” part of this scheme involves much of the optimizations that are needed to reify the model in the world (and usually to manufacture and distribute many copies).

The computing world spends quite a bit of its time and energy working directly in the FAB area. Once optimizations are successful — often quite a bit of work — it is then easy to make as many copies as desired.

If we try to stay with this framework — CAD<->SIM would be the tools to make a working model of our idea, which perhaps will require a supercomputer for real-time response. The model should capture our goals and requirements in as simple and meaningful way as possible.

Some of the model code could be in the form of “constrained goals” (like Sketchpad or Thinglab or Cassowary), and some could be in the form of simple unoptimized programs.

A simple example might be a component — server — that semantically does sorting. The model code would be very simple (whether as a constraint or a simple program). The SIMulation has to be powerful in order to be able to work on realistic sized problems. Note that the FAB part can be developed incrementally inside the component/server and could include dozens of different sorting algorithms and representations depending on the requirements (for example the system might want to use B-trees for some services). The model code can be run in parallel to check each optimization. A well designed system would be able to run with all optimizations turned off (this would be just the CAD/SIM part).

Reality is more complex, but the principle of separating meaning from optimization is powerful, and the kind of system I’m describing is set up to keep them separate.

We got to try some of these ideas at Parc — and found many of the principles in the analogy to work well.

In any case, if you are going to have “requirements”, I think that they have to be simple, runnable and debugable in order to be of much use — and this leads directly to the need for powerful models and SIM, in part because so much software today has real-time requirements as well as relational ones.

Historically, it’s worth noting that the early drive for higher level languages was to program in terms of models as much as possible — they were also called “Problem Oriented Languages” — and to have the compiler do the optimizations. This is hard to do for many problems, even if you can give the compilers help and hints. It is also hard psychologically for programmers not to sneak in optimizations intertwined with the meanings in the code (this soon pretty much disappears the meanings (the whats) in favor of the hows).

The idea of separating meanings from optimizations (and to have mechanisms in the development languages for doing so) helps with the practical business of actually writing software.



Has Lisp ever been implemented iteratively?
I’m not sure I understand this question clearly.

But the first implementation by Steve Russell — and many since — was done by writing a loop that manipulated structures in memory to create the equivalent of a recursive eval-apply. Take a look at the latter part of the “Lisp 1.5 Manual”, which discusses the scheme used with the 7090 at MIT (a machine which didn’t have a stack mechanism (most didn’t back then)).

The entire machine code of the classic graceful Peter Deutsch Lisp implementation on the BBN PDP-1 is given in the book “The Programming Language Lisp” (Bobrow and Berkeley ed.). The PDP-1 didn’t have a built-in stack, etc.



What language was ALGOL 58 written in?
“BALGOL” (Burroughs Algol) was a version of Algol 58 that was implemented on the Burroughs 205 and 220 computers in pretty much the same machine code — the two machines were quite similar (good portions of this by Don Knuth).



How does one get as close as possible to mastering software engineering?
“Engineering” is making something in a principled way — so there is definitely a “software engineering” of a sort.

The idea was articulated in the 1960s as a *aspiration* because the “principled ways” known in computing did not compare strongly with where the established engineering disciplines — civil, mechanical, aeronautical, electrical, etc — had gotten to.

Today, many computerists will say “software engineering” is defined by just what it is that they do (ditto with “computer science”). By removing the *aspiration*, they also have removed much of the motivation and ability to see, criticize and improve. For these people, “mastering software engineering means getting up to speed with current practice”.

As an old fogey, I prefer the earlier, sterner and more enlightened stance of *aspiration* and like to compare and think about how making software could be brought more to parity with established engineering.

One property of the established fields — “Engineerings” (with capital “E”) itself — is that they were completely revolutionized by the invention of science. The takeup of science into engineering took quite a while — a lot was slow and grudging over 150 years — but the big “E” is now an amalgam of traditional tinkering, making with principles, mathematical models and modern science. The latter two, especially the latter, have vastly changed what “making with principles means” and “the principles and tools” used. The advent of the computer — especially powerful interactive graphical computing — came along at just the right time to supercharge this change with powerful CAD tools, SIMulators of the CAD models, and increasingly FABrication driven directly from the designs vetted by simulation.

It’s worth noting the deep irony that the new computer tools for the engineering disciplines are almost always more comprehensive than the ones found in use by computer people for writing the programs! (There are a lot of “black screen simulated card-deck-glass-teletype” screens in use, in gross contrast with e.g. how something in EE or ME is designed and made today.)

Going deeper, most software people are just trying to do FAB, and most of the tools are FAB tools — there is very little CAD and even less SIM in “software engineering”. To my old eyes, this doesn’t look or feel like real engineering process.

From another perspective, I don’t see a lot of “math and science” in the current “software engineering” process. I will leave expansion of this aside here with just “it doesn’t help to have ‘computer science’ these days not really take the term ‘science’ seriously” — the whole phrase is perhaps more of a designer jeans label.

Now let me pick on the first term in “software engineering”. Many software systems today are “legacy” in one form or another, and in any case are characterized by “rigidity” with respect to change (this is the opposite of “soft”) and by “fragility” with respect to safety and integrity (this is the opposite of what “E”gineering is all about). In fact these are backwards. What we want from “soft”ware (and from understanding “soft”ware) is “extreme flexibility” over the lifetime of an artifact (“soft” is why we make programmable computers) with “extreme safety and integrity”.

This combination has possibilities of being radically extended for “Engineering of Software”, and I think our main job as “aspiring engineers” is to move software towards “extremely safe extreme flexibility”.

I think this is a good starting place for rethinking all of this.

I think “software engineering” — like the existing developed fields — should be actively aspiring to find “What Is Actually Needed” (WIAN) for both its projects and for the field itself. “Mastery” is then the process of continually moving closer to the ideals of the field — it is not a location.



What are the limitations discovered in the early programming language that triggered the need for OOP?
The earliest programming was in the forms of the earliest computers: to find resources in memory — usually numbers, or numbers standing for something (like a text character) — and doing something with them: often changing them or making something and putting the results in memory. Control was done by simple instructions that could test and compare, and branch to one part of code or another: often to a part of code that had already been done to create a loop. An instruction not in the hardware could be simulated if there was a way to branch and capture where the branch originated, thus producing the idea of “subroutine” (first used in full glory with a “library” on arguably the first working programmable computer, the EDSAC by Maurice Wilkes at Cambridge, late 40s).

Beginning programming was and is most often taught in this style, and it has been noted that the first programming language and style one learns tends to manifest most deeply throughout the rest of a career. Not a lot has changed 70 years later, partly because many languages started off with this style in mind, and thus the new languages were attempts to make this style more convenient to use (Lisp and APL were different early interesting exceptions).

Another way to look at this is to note that (1) the degrees of freedom of a computer, and of the possible problems to be solved, coupled with the limitations of the human mind, means that anticipating all the tools needed will be essentially impossible. This means that *how to define new things* becomes more and more important, and can start to dominate the “do this, do that” style.

Along with this (2) soon came *systems* — dynamic relationships “larger” than simple programs. Programs are simple systems, but the idea doesn’t scale up very well to deal with qualitatively new properties that arise. Historically, this never quite subsumed “programming” (and the teaching of “programming”). It gave rise to a different group of computerists and did not affect “ordinary programming” very much.

I think it is fair to say today that the majority of programmers reflect this history: most do not regard *definition* as a central part of their job, and most do not exhibit “systems consciousness” in their designs and results.

I think quite a bit of this has to do with the ways programming is taught today (more about this gets even more off topic).

Looking at this, the earliest real “computer scientists” could see that e.g. subroutines were an extension mechanism, but they were weak — for example, to make a new kind of “data structure” was fragile and could not be made a real extension to the language. This led to a search for “extensible languages”.

Other computer scientists could see that “data structures” were not a great idea e.g. sending a data structure somewhere required the receiving programmer to know many details, and the structure itself might not fit well on a different kind of computer. A vanilla data structure was vulnerable to having a field changed by an assignment statement “somewhere” in the code by “somebody”. And so forth.

Most of the programmers were used to the idea of commanding changes to “data”, and so some of the fixes were mechanisms that allowed data structures to be invented and defined: one of the major styles today is “abstract data structures”.

Along with all this were several ideas for dealing with simple smashing of variables (and the essential “variable” that is a data field). This was scattershot and reinvented in different ways. The most prominent way in strong use today is for very large structures: “data bases” that are controlled by the intermediaries of “atomic transactions” and “versioning”, which effective wrap the state with many procedures to ensure that a valid history is kept and relationships between parts of the data base are not violated. Eventually, it was realized that “data” didn’t capture all the important questions that could be asked — for example: “date of birth” could be “data”, but “age of” had to be computed on the fly. This was originally done externally, for some data bases, procedures could be included. (This required a “data base” to eventually be able to do what a whole computer could do — maybe “data” is not the operative idea here, but instead “dynamic relationships relative to time” works better. If so, then the current implementations of “data bases” are poor.

In computer terms, modern data bases” are subsets of the idea of a “server”.

Another line of thought — which goes back before there were workable computers — is that (3) certain easy enough to make computers can simulate any kind of mechanism/computer that can be thought of. This partly led to several landmark early systems such as Sketchpad, and the language SImula.

If you take in the above, and carry to the extreme, its worth noting that only one abstract idea is needed to make anything and everything else: the notion of “computer” itself. Every other kind of thing can — by definition — be represented in terms of a virtual computer. These entities (I’m sorry I used the term “objects” for them) are used like servers, and mimic the behaviors of (literally) any kind) that are desired.

A key point here is that just having practical means for creating objects doesn’t indicate what should be simulated with them. And here is where the actual history has been and continues to be unfortunate. The most use of the idea — still today — has been to simulate old familiar ideas such as procedures and data structures complete with imperative destructive commands to bash state. This again goes back partly to the way programming is still taught, and to the rather high percentage of programmers today who are uncomfortable with design and “meta”.

For example, since “an object” is a completely encapsulated virtual computer, it could be set up to act like a transactional versioned date-base. Or something much better and more useful than that.

Note that most interesting representations of things do “change over time” so something has to be done to deal with this problem. So-called “Functional Programming” has to add features — e.g. “monads” — to allow state to advance “in a more functional way”. This might not be the nicest way to deal with this problem, but something does have to be done.

And note that if you have gotten religious about “FP”, then it is really easy to make a pure FP system and language by using the universal definitional properties of “real objects” (being able to define what you want is the deep main idea!) But before you do, it will be good to ponder in larger terms.

As Bob Barton once remarked “Good ideas don’t often scale” — and neither do most simple programming paradigms. This means that another of the new things that can be built with “objects” — but have to be invented first — are less fragile ways to organize systems.

Along the Barton “qualitative changes” line of hints, one could start contemplating a kind of “request for goals” kind of organization where the semantics of the worlds being dealt with are more richly human and the main center of discourse is about the “whats that are needed” rather than the “hows” that the system ultimately uses.

This was one of the impulses behind some of the HLLs in the 50s and 60s, but the field gave up too early. The original idea behind a “compiler” was to take a “what” and do the work necessary to find and synthesize the “hows” to accomplish the “what”. 60 years ago the “whats” were limited enough to allow compilers to find the “hows”. But the field decided to sit on these and not uplift the “whats” that would require the compilers to do much more work and use more knowledge to synthesize the “hows”. This is another way to miss out on the changes of scaling.

In a “real object language” — with “universal objects” — it should be possible to define new ways to program and define and design any new ideas in computing — I think this is necessary, and that it has to be done “as a language” in order to be graceful enough to be learnable and usable.

Historically and psychologically, *tools* have had a somewhat separate status from what is made with tools (and the people who make tools, and make tools to help make tools, etc. are also somewhat separate from the average maker). But a computer is always also a tool making shop and factory, you don’t have to go to the hardware story to buy a hammer etc. This requires a change in mindset in order to really do computing.

At Xerox Parc in the 70s, we made a “real object language” to walk both sides of the street (a) we wanted to invent and make a whole graphical personal computing system, and (b) we wanted to be able to easily remake the tools we used for this as we learned more. I.e. we wanted to “co-evolve” our ignorance in both areas to reflect our increased understanding. We were motivated both by “beauty” and that we had to go super high level in order to fit our big ideas into the tiny Alto.

This process resulted in five languages, one every two years (thanks to the amazing Dan Ingalls and his colleagues), with one deep qualitative change between the 2nd and 3rd languages. That these languages could be useful “right away” was due to the way they were made (and partly because the languages contained considerable facilities for improving and changing themselves). To make progress on the personal computing parts, the constructs made in the languages had to be extremely high level so that the system could be rewritten and reformulated every few years.

The 5th version of this process was released to the public in the 80s, and to our enormous surprised was not qualitatively improved again, despite that it included the tools and the reflective capabilities to do this. The general programmers used the language as though it came “tight” from a vendor and chose not to delve into even higher level semantics that could help the new problems with the new scalings brought by Moore’s Law. (This was critical because there were somethings we didn’t do at Parc because of the scalings that needed to be done to deal with “10 years later” scalings, etc.)

To answer the current question after the “long wind” here: there are usually enough things “not right enough” in computing to need new inventions to help. Most people try to patch their favorite ways of doing things. A few will try to raise the outlook and come up with new ways to look at things. The deep “object” idea, being one of “universal definition” can be used for both purposes. Using it for the former tends to just put off real trouble a little bit. I think programming is in real trouble, and needs another round of deep rethinking and reinventing. Good results from this will be relatively easy to model using “real objects”.



Did Xerox PARC have time constraints?
I was not aware of any during the period I was there, from late ’70 to ‘81.

However, we did often put them on our own projects — and for various reasons. For example, when we decided to make our own PDP-10 when corporate nixed buying one and wanted Parc to get a Sigma-7, the Parc feeling was that this would be worth it if the project was up and running in less than a year.

For the Alto (which started off as a covert project) Chuck Thacker had a bet with a Xerox exec that such a thing could be done in 3 months (he won).

This kind of voluntary internal time pressure for practical building of things did not get in the way of the freedom to spend arbitrary lengths of time “problem-finding” and mulling.

The tensions between Parc and corporate was less pleasant than our previous experiences in the ARPA community with the relationships between the ARPA projects and ARPA “management” were more informal.

But the situation at Parc was overwhelmingly set up for positive processes nontheless.



Did the Xerox PARC children had any problems context-switching between traditional math precedence rules and Smalltalk alternative left-to-right evaluation order? Would Mathematics change significantly if left-to-right evaluation were widely adopted?
We made the decision not to have precedence for the very same reason APL did: if you have lots of operators, just remembering precedence starts to become a burden. The kids had no trouble dealing with the (easier) left to right “consumption” idea, and I never heard any complaints about the conflict with precedence in school. These were 12 and 13 year olds.



How can a beginner develop thinking (computational and algorithmic) in programming?
This question was asked by “Leonardo da Vinci”, and I wouldn’t like to deprive Leonardo of the pleasure of finding this out for himself.

Now that we’ve had some fun with this, here are a few contextual perspectives to get you started …

“computational” and “algorithmic” have now become buzzwords, so a good starting place is to ignore these, and start with the term “thinking” (you could take a look at my Quora answer for my take on this term and how to develop it). Also, several of us wrote an ACM article with the title “Computational Thinking Is Just Good Thinking” (and the title makes the main point). One of many important ways to approach this is to realize that a threshold for modern thinking is to be able to know if you know enough to (really) critique the ideas you are dealing with, and to know how to approach learning what you need to know to critique. I daresay that most people in computing today cannot adequately critique what they and the field are doing, and this means that it turns into something like creeds, then religions, and finally into “cargo-cults”.
Historically, the hardware of computers was tiny, and what they were used for was very limited. So learning how to come up with and program algorithms in machine code was the prime thing to learn. Don Knuth’s immense tome “The Art Of Programming” was just this: it used machine code, and as he said, was partially a “cookbook”. But designing and making systems is the larger idea, and quite a bit of it is not “algorithmic” in nature. I suggest that you get some books about systems and start learning to think in terms of systems. This is a new kind of thinking that co-evolved with the new thinking of modern science. Learning both of these will help greatly.
I would put quite a bit of effort into learning about and dealing with “noise” of all kinds. This is a third kind of modern thinking, to go along with science and systems. In the large, “noise” is all the stuff at every level that you don’t want to have, and especially that interferes with what you do want. The two key ideas are (a) everything in the real world has noise — including both our biological and psychological brains — and (b) an astounding great thing is that if we can determine the maximum percentage of noise in a system, then we can use the very system to make processes that are free of noise. The “(b)” part was articulated by Claude Shannon, and is one of the greatest creations of the 20th century, not just pragmatically, but philosophically. It started off being used in communications and computing hardware, but it is quite general, and is found in nature also, especially molecular biology. It is used in every modern engineering system in one way or another, except … that very few computer people are willing to do the work to apply the ideas to software — so this is “a biggie”.
Scaling is another deep problem for the human mind, and for systems in general. We have to learn how to design for both growth and large size, and this means that approaches which work with small problems — and with most examples in programming courses — don’t scale well at all. In many cases you can’t use “elementary computing” to do real computing. How to design systems that have thousands of requirements and constraints is another large branch of knowledge. Still, one of the best general discussions of this is Christopher Alexander’s first book “Notes On A Synthesis Of Form”.
Tony Hoare once remarked that “Debugging is harder than programming, so don’t use all your cleverness to write the program!” This is one of the main bugs in general computing today. You are much better off designing for failure and change rather than success and to the original objectives. Most of the cost in software systems (more than 85%) is incurred *after* they are successfully “finished” and deployed. Real computing has to deal with real “ality” and being willing to pay much more up front to save much more over the life of the software. One way to think about this is that general computing today has forgotten the meaning of “soft” in “software” and why we do software at all (it is not to make intractable “hard” masses of code that cannot be reformulated).
A bit redundant — but perhaps useful to say it this way — one of the things that was most helpful to the powerful ARPA/Parc computing community in the 60s and 70s, was that most of its members had deep training and experience with difficult already established fields which required much discipline to acquire fluency — such as the physical sciences, mathematics, and engineering. This helped tremendously in trying to understand what each of these terms, processes, and perspectives might mean for the new, wiggly, and prone to BS field of computing. I.e. what would happen if we took “science”, “mathematics”, and “engineering” seriously within computing? Being able to do this is one of the main pillars of “thinking” and “thinking within computing”.
Of course, there are more than 6 perspectives needed to become a decent computerist, but these are 6 important and non-ignorable ones that will start you off.



What are some examples that demonstrate the usefulness of homoiconicity in programming?
I don’t think it is that useful per se (the most important aspects of a programming language are to have both (a) the most powerful semantic frameworks for expressing meaning, and (b) to be as learnable and readable and “gistable” as possible for human users.

The most famous example of “homoiconicity” is Lisp, but the language was not originally so. McCarthy thought and wrote in terms of what he called the m-language (which was basically normal math function syntax with square brackets, and arguably more readable — he thought it was). To do an interpreter of Lisp in itself, he wanted to have a representation for the programs in terms of lists, and chose what has become known as “Cambridge Polish”, a prefix notation with parens to delimit the sequence of parameters — McCarthy called these “S-expressions”.

When Steve Russell did a working interpreter for this on the 709 (or 90) he wrote a “read” routine — which translated text characters to lists — but “didn’t get around” to writing a simple parser for m-expressions into s-expressions. Thus the first — and most subsequent — Lisps were entirely in terms of what would be equivalent to ASTs.

The hackers were happy to work in a kind of “machine code”, but there were several efforts to make a more readable Lisp. The official “Lisp 2” effort never quite made it. But there were several really good versions of “Algolic Lisps” over the years: two of them were at McCarthy’s Stanford AI project: M-Lisp by David Canfield Smith and Horace Enea, and Reduce by Tony Hearn.

Note that even in the homoiconic s-expression Lisps, you have to use functions to get at the “functor position” and the parameters. These happen to be vanilla Lisp functions such as CAR and CDR — but you could just as easily write a function called “functor” that when applied to an expression, will find and deliver that part of it. Similarly for the arguments. And now you can do everything you used to be able to do, but now in a much more readable language.

So, the key to this kind of power is not homoiconicity, but the “meta” of having expressions be 1st class objects in your language.

A really interesting set of tradeoffs exists for the processes of learning deep ideas about programming and programming languages. And these tie in with some of McCarthy’s earliest ideas ca late 50s about “programming and computing in the future”. For example, he says in the very early “A Basis For A Mathematical Theory Of Computation”:

Programs that learn to modify their own behaviors require a way of representing algorithms so that interesting properties and interesting transformations of algorithms are simply represented.

So right off the bat he wants programs to be able to look at and “understand and reason” about programs as much as possible.

The overlap between beginners and these programs is that the fewer concepts and relationships the better. This is where Lisp and its s-expression descendants provide the interesting tradeoffs between real simplicity vs a new notation that must become fluent. The Scheme (and later with the addition of Racket) people have provided plenty of evidence that this pathway works. Doing this with more normally readable expressions plus access to the ASTs — at least in universities — seems sparse to non-existent in my admittedly limited horizons.

So it’s possible that the homoiconic approach for meta is a kind of cognitive sweet spot. On the other hand (I think) you get something much nicer in many ways to go with the way Smalltalk dealt with reflection and meta. It is less of a hack etc. and requires a bit more learning before diving in. But this approach scales more gracefully, and starts with much more protection.



What caused the ADA to be created?
The Wikipedia article is a good place to start: Ada (programming language) - Wikipedia High-level programming language first released in 1980 Ada Paradigm Multi-paradigm : structured , imperative , object-oriented , aspect-oriented , [ 1 ] concurrent , array , distributed , generic , procedural , meta Family Pascal Designed by MIL-STD-1815, Ada 83: Jean Ichbiah Ada 95: Tucker Taft Ada 2005: Tucker Taft Ada 2012: Tucker Taft First appeared February 1980 ; 44 years ago ( 1980-02 ) Stable release Ada 2022 / May 2023 Typing discipline static , strong , safe , nominal OS Multi- or cross-platform Filename extensions .adb, .ads Website www .adaic .org Major implementations AdaCore GNAT , [ 2 ] Green Hills Software Optimising Ada 95 compiler, PTC ApexAda and ObjectAda, [ 3 ] MapuSoft Ada-C/C++ changer, [ 4 ] formerly known as "AdaMagic with C Intermediate", [ 5 ] DDC-I Score Dialects SPARK , Ravenscar profile Influenced by ALGOL 68 , Pascal , Simula 67 , [ 6 ] C++ (Ada 95), Smalltalk (Ada 95), Modula-2 (Ada 95) Java (Ada 2005), Eiffel (Ada 2012) Influenced C++ , Chapel , [ 7 ] Drago , [ 8 ] D , Eiffel , Griffin, [ 9 ] Java , Nim , ParaSail , PL/SQL , PL/pgSQL , Python , Ruby , Seed7 , SPARforte, [ 10 ] Sparkel , SQL/PSM , VHDL Ada is a structured , statically typed , imperative , and object-oriented high-level programming language , inspired by Pascal and other languages. It has built-in language support for design by contract (DbC), extremely strong typing , explicit concurrency, tasks, synchronous message passing, protected objects, and non-determinism . Ada improves code safety and maintainability by using the compiler to find errors in favor of runtime errors. Ada is an international technical standard , jointly defined by the International Organization for Standardization (ISO), and the International Electrotechnical Commission (IEC). As of May 2023 [update] , the standard, called Ada 2022 informally, is ISO/IEC 8652:2023. [ 11 ] Ada was originally designed by a team led by French computer scientist Jean Ichbiah of Honeywell under contract to the United States Department of Defense (DoD) from 1977 to 1983 to supersede over 450 programming languages used by the DoD at that time. [ 12 ] Ada was named after Ada Lovelace (1815–1852), who has been credited as the first computer programmer. [ 13 ] Ada was originally designed for embedded and real-time systems. The Ada 95 revision, designed by S. Tucker Taft of Intermetrics between 1992 and 1995, improved support for systems, numerical, financial, and object-oriented programming (OOP). Features of Ada include: strong typing , modular programming mechanisms (packages), run-time checking , parallel processing ( tasks , synchronous message passing , protected objects, and nondeterministic select statements ), exception handling , and generics . Ada 95 added support for object-oriented programming , including dynamic dispatch . The syntax of Ada minimizes choices of ways to perform basic operations, and prefers English keywords (such as "or else" and "and then") to symbols (such as "||" and " https://en.wikipedia.org/wiki/Ada_(programming_language) This was a big project with many people — quite a few of whom were really good — contributing many opinions and ideas — quite a few of which were really good. And then there were many “other people” with “other kinds of ideas”. Much can be understood about what happened by reading this webpage, which gives the initial “Strawman” requirements for what became ADA. (The progression of “mans” over the length of the project was: straw, wood, tin, iron, steel …) I got to watch this happen in the 70s, and various “language people” from Xerox Parc were also asked to comment and contribute (I went to a few meetings, and others went to more of them). I liked “kernel extensible reflective, etc.” languages, and ADA was never aimed in this direction, so my personal interest was not high — however, Dave Fisher — whose 1970 super-reflective thesis on extensible control structures I much admired, was a powerful figure at DoD for this effort. And others I admired greatly — such as Bill Wulf — contributed designs (the “Tartan” language proposal for ADA). There was a DoD side-condition that they could not be talked out of — Fisher couldn’t either from the inside — that ADA had to run efficiently on every existing DoD computing system. It was pointed out that Moore’s Law — happening swiftly whereas language designs and implementations, especially by committee — would make it ridiculous not to replace most of the DoD HW by the time ADA got implemented to a useful state. I knew and liked Jean Ichbiah, and thought his LIS language design was pretty good (especially for something aimed at compiling into machine code). He eventually won the contract for Honeywell-Bull (and ADA turned out to be a lot more intricate than LIS was). Bottom line: a lot can be learned about both language design, software engineering, and especially how “whole-systems real computer science” could have helped. It is also worth noting that almost everything that was done in the 1995 revision of ADA was part of the original design discussions. As Robert Heinlein remarked, a really bad committee is “the only form of life with 10 bellies and no brain”. This wasn’t that bad, nor was it as bad as a “typical committee”: “whose collective IQ is the minimum of the individuals in the group”. I think that ADA turned out a lot better than the earlier comparable project of IBM — PL/1 — another language that took forever, delivered not so well, and is still around (I know of at least one major insurance company in the US that still employs 2000 PL/1 programmers!)



To what extent does each invention require the invention of jargon for that invention?
Great question!

I like a process that seems generally unpopular, but has been extremely useful in science especially: that is to try to coin special words for the special new meanings, and part of getting fluent in the new knowledge is to get fluent with the new terms.

This is almost the exact opposite of normal language churn in commonsense cultures, where redefining old words to mean new things in their day and time is the general case. This makes language ever more metaphorical and idiomatic (which ironically also requires a lot of learning and memorization: but for new meanings rather than new meanings and a new term for them).

This sets up deep conflicts between the two cultures. I don’t think the compromises help either culture.

Interesting examples abound in fields that are supposed to be technical — such as computing — e.g. the term “Artificial Intelligence” was defined by John McCarthy ca 1956 and had quite a solid meaning (and very much based on the non-metaphorical combination of the two terms). Today, “AI” actually means such an extreme subset of the original definition that the original meaning has been quite lost, and references to the original meaning now have to use terms such as “Real AI”, “Strong AI”, etc. This is also the case for many other terms in computing today, e.g. “computer science”, “software engineering”, object-oriented programming, and many more. This is a mark of a “pop-culture” (not quite a “cargo-cult” yet …).

Just to round this out … it’s worth contemplating that science itself was initially also guilty of redefining earlier terms in attempts to make contact with outsiders, and reworked terms such as “know”, “proof”, “truth”, “theory”, etc in ways that still confuse non-scientists today. I think it is much better to just make up new terms for new meanings to help people realize there are important differences, and which have to be learned.



What is the best book for Research Methods in Computing?
It would be fun and illuminating to see the opinions of others — either as answers or comments here.

The best book about the ARPA-IPTO/Parc research community (1962–1982 or so) is “The Dream Machine” by Mitchell Waldrop (it has been brought back into print by Stripe Press).

This is an excellent history of those 20 years, with good historical accounts leading up to 1962.

I wrote an essay tribute — The Power of the Context — to this community in 2004, which might help a bit — and it does have an extensive bibliography.

The methods used by this research community were quite effective, and the results include many of the fundamental technologies still in use today (Internet, Ethernet, personal computing, GUI, DTP, laser printing, etc.)

The Waldrop book is mostly about *what*, but does give some glimpses of *how*.

I got asked to do several lectures at Stanford a few years ago about “How To Invent The Future”, and these have quite a bit more about methods used.



Did Alan Kay give advice to Guido van Rossum on the design of Python? The language is said to have a object model that leans toward the original late-binding, kaynesian definition, instead of the abstract datatypes route taken by e.g. Java and C++.
I didn’t give Guido any advice on the design of Python. The aims of Python and its early history are easy to find with Google. Guido also wrote a blog with many articles about different aspects of Python history and development.



In software engineering, when might pragmatism be harmful in the short term?
Almost always. As Don Knuth wrote long ago “Premature optimization is the root of all evil”.

If the term “engineering” is to mean anything in “software engineering”, the higher priorities all have to do with the longer term life cycle requirements of the artifact — and include goodness of model, safety, testability, debugability, understandability, changeability, scalability, portability, and many more.

One way to think of a well designed system is that the semantics will be separated from the optimizations as much as possible (one can imagine tools to help do just that) so that the system can be debugged and changed, etc., in terms of the semantics and will run — perhaps too slowly — with the optimizations turned off.



Did Atari use the Forth language for any of their software?
I was there from late 1981 to early 1984, and was not aware that FORTH was used in any Atari systems. However, it is certainly possible that a use escaped my notice.



Is it important to turn what is actually needed into a pop culture?
I think “just the opposite”. It’s important to turn pop cultures into “what is actually needed”.



Did UNIX fulfill or exceed the vision of the Multics project?
I thought Paul Green provided a good answer to a related question. Why did Unix succeed and not Multics? https://www.quora.com/Why-did-Unix-succeed-and-not-Multics The question here is about “the vision” of the two systems. I think that the Multics effort always had larger “vision” about “why?” and “what is actually needed” (large parts of the vision were “The ARPA Vision”). So my direct answer to the question would be “No”. (Added Aug 7th — I realized that I should have mentioned that MULTICS didn’t fulfill or exceed the vision of the MULTICS project either …) The approaches to implementation were also very different. My opinion of Multics was that it (a) chose a poor hardware framework, and (b) perhaps overacademized the software design. My opinion of Unix was that it (a) also chose a poor hardware framework (but a very different one), and (b) this and other factors rendered the software design too simplistic. (As with many artifacts in computing, “(b)” allowed something to get implemented anyway.) Both systems did have a few strong ideas and principles. My main opinion was that there were at least two alternatives to look at that were much better bases than either Multics or Unix The Burroughs B5500 type architecture, including multiple CPUs, fine grain protection, HLL OS and implementation languages, etc. The “Project Genie” type architecture used in the ARPA community (that morphed into Tenex, and another sprout that was the CAL-TSS approach). The 70s allowed practical microcoding and VLSI RAM. This was too late for Multics, but the Unix folks could have taken this route. By the end of the 60s it was clear (to some of us at least) that the B5500 approach would be “just wonderful” if it were microcoded. Parc indeed did all of its hardware using microcode, including several mainframes, but Parc’s aim was personal computing, not time-sharing, so the mainframes were mostly used to emulate some of the hardware the external community was using (such as PDP-10s). The most interesting TSS use was “D-Lisp” of Warren Teitelman and Bob Sproull, which ran Interlisp on the mainframe and used Altos as windowed graphic terminals. This was quite good. In my view, one of the first really great systems that was a modern version of the “ARPA vision” was Gerry Popek’s distributed multi-HW “LOCUS” OS in the 80s. This was really good. (It did use a modified Unix, but this was not a critical component of the design: any encapsulated process architecture would have worked, and some would have worked even better.)



What are some good books about what is actually needed in software engineering?
I’ve been trying to find some (I got invited to do a keynote at a large upcoming Software Engineering conference, and have been looking to see if there is more of a consensus and higher level view already in the literature).

So far … one observation of what I have looked at is that “what is actually needed” to deal with the scalings from both Moore’s Law and hardware engineering doesn’t seem to be treated. I.e. not just individual and super-computers and cloud computers, but the billions of them linked by the Internet and other networks. (This is disappointing because the original Internet design efforts were aimed at 10^10 and more scalings, and were successfully pulled off by a group of top researchers.)

I welcome helpful comments and suggestions!

Added Aug 12th: no helpful suggestions from readers yet. Does this mean you don’t think there are any “what is actually needed” books for “software engineering”?



What is your review of The Pragmatic Programmer (1999 book)?
I don’t have a detailed memory of the details of this 1999 book, but do remember some impressions.

The main one was that it — correctly I thought — addressed “programmers” in general as people who had not learned or done engineering in their past or as part of learning to program. Much of it was like the advice a professional musician — especially a classical or studio musician — would give to pop culture teens and twenties who are starting to play instruments, but decline to take lessons, learn to read music, etc. I.e. “there are a bunch of things you really need to buckle down and learn fluently in order to be a productive pro”.

Added Sept 1 2021 (from a comment below) — OK, I found a pdf of the 2nd edition and perused it. It has a lot of good advice (praising doesn’t require a lot of substantiation). The simplest criticism I can write here is that I think it has many too many topics and too many styles to serve as a good starting place for beginners. I think taking more of a systems approach would improve the presentation a lot. I think the “pop culture” allusions (above) were too harsh (some tinges would remain in a more balanced criticism).

Another recalled impression is that — as with so many books on programming — it was very devoted to the general milieu of programming in corporate US, and the kinds of languages, tools, and approaches most in use. This is definitely an aspect of “pragmatic programming”, and a lot of the advice along these lines seemed sound for those who chose to cope.

My view is that this kind of pragmatism is quite harmful in the long term, and often even in the short term. I.e. learning better ways to apply band-aids does increase knowledge, but if the patient is suffering from suppurating wounds, then real medicine and the learning and inventing of real medicine is what is actually needed (WIAN). I don’t recall any such rabble-rousing in necessary directions in this book.

I think the book became quite popular and successful.



Can processes notify operating system that their stack and heap will need this much of memory prior to starting? Or is it operating system's decision to give a predetermined amount of space for heap and stack for every process?
Here is one of the many great ideas in the “Project Genie” OS (lead designer: Butler Lampson, and later used to launch commercial time-sharing, and for the Engelbart “Mother Of All Demos” towards the end of the 1960s).

The SDS computer was a 24-bit 0.5 MIPS machine with 64K words of storage and address space. The Genie project at Berkeley added a HW memory map with 2K pages, but the tiny slow resources were real barriers for practical time-sharing with multiple dozens of users. Both long swapping latencies and thrashing were deep problems.

Butler decided to “force” working sets to be four 2K pages — so he could line up the next few working sets in storage to reduce process switching latencies to almost zero — and to reward programmers/programs that could predict the next working set. The reward was to have your process run on the fast que. If you failed to predict, and had a page fault, he would run you on the slow que for a while.

This worked wonderfully well. The programmers did a lot more thinking about the dynamic state of their processes, and put in more SW instrumentation to help predict.

The system overall was amazingly efficient, and could indeed run dozens of users at a time without latencies and thrashing. It’s worth contemplating that the entire “Mother of all demos” was done with multiple users in real-time using this tiny little hardware of 192K bytes and 0.5MIPS.

Another of the many ideas used in this system was aggressive “before the fact” use of the swapping drum memory. The idea was to have dirty bits on pages to mark if they had been changed, and to use about 2% of the CPU etc, to continuously write out dirty pages. This meant there was almost always enough room in core to bring new pages in when needed, without having to write out something first.

This combined with yet another Butler perspective: something will crash — computer hardware was quite prone all by itself — so don’t pretend that everything will work (and then get badly caught when a crash does happen). The aggressive flushing, prediction, and some other ideas not mentioned here, all allowed the system to resume within seconds of a crash.

Margaret Hamilton, who was head of the software for the Apollo Guidance Computer called this kind of systems approach “before the fact” design (the “facts” being things you didn’t want to have happen). When asked what she was doing, she said “Software Engineering”).

In the 70s at Parc, we used some of these ideas — and much of the overall philosophy — for doing the systems aspects of Smalltalk. For example, the aggressive flushing scheme worked well for the object level swapping to a much slower secondary storage we did on the Alto.

It’s worth noting that even today, most OSs are quite rooted in the early 60s with respect to their perspective of computing, hw and sw — the notion of an OS actually “thinking about what is going on and how to help” has never crept into standard notions of computing, and joins up with the early 60s nature of most programming that is done today.

Why would people not do learning and take actions that would help them? A look at the pandemic — and many other situations in normal human life — reveals that, for many, “comfort even with high ultimate costs” is worth more than the “discomfort of learning and changing”.




What is objective quality?
Try typing it into Google and looking at the first hit (that’s what I did …)



How was personal computing influenced by the hippie movement?
The larger category of interest here is “the counterculture”, of which the hippies could be considered one branch (or possibly a parallel “counterculture”). For example, I think “The Whole Earth Catalog” is a “counterculture” medium rather than a “hippie” one. The “Free University” in Palo Alto and Menlo Park in the late 60s into the 70s was “counterculture” much more than “hippies”.

One view related to your question can be found in John Markoff’s book: What the Dormouse Said: How the Sixties Counterculture Shaped the Personal Computer Industry. This especially focuses on the Engelbart group at SRI, who were the epitome of ideas about personal computing in the 60s. Markoff was a student at Stanford while all this was going on, and he remains as one of the most respected techno-journalists.

My view — from having started to visit Palo Alto in the late 60s and living there from 1969 on — is that “hippies” had essentially nothing to do with the development of personal computing, but that the surrounding context of counterculture “helped”. You could look at my Quora answer about The Whole Earth Catalog for more info.

What was the Whole Earth Catalog?
https://www.quora.com/What-was-the-Whole-Earth-Catalog



In the STEPS report, an unresolved problem of massive intermodule coordination and communication is mentioned. Is it more of a hardware problem, or a software/design problem?
For a long time, this was a problem that never quite got worked on — it lost out to other problems — but one I thought “someone” should do (maybe us).

I tossed it into the list of techniques that would make STEPS interesting, and again, it didn’t quite make the cut during the actual work (though we did look at and build some infrastructure for subproblems along these lines).

The basic insight was that extreme scaling was going to make more and more difficult “knowing the actual labels” of useful things, and that something more like “searching for things analogous to descriptions of structures/objects” would be more and more needed.

A simple example is suppose you need the sine function but don’t know that it is called “sine” (e.g. APL doesn’t call it anything like “sine”). Ted Kaehler’s “method finder” can find it in Smalltalk, because one of the things it can do is to look for I/O relationships to find the operation. So if you give it 30 and .5 it will find “degreeSin”. If you give it 3, 4 and 7 it will find “+”, “or”, etc. If you give it an unsorted sorted array and then a sorted version of the array, it will find the sorting functions in Smalltalk. Etc.

One of the terrific theses — and people — we were inspired by during this time was “AM” of Doug Lenat. This was a “discovery system” that could “extract meanings from possibilities”. It had to have a description scheme, and I realized that something like it could be used for the module problem.

One of the things we spent some time thinking about at Parc and thereafter, was “Semantic Typing”, which is another example.

And in the 80s came Gerlernter’s “LINDA” coordination language, that I thought was really a neat way to think about the larger problems of messaging and concurrency, etc.

I’m a bit surprised that no modern day Kaehler, Lenat, or Gerlernter has stepped forth to do this. It is both a really important problem, and the solution (I believe) would be quite beautiful.

It’s similar to “communicating with Aliens” in the stars, but quite a bit easier.



Could some kind of analogy be made up between object-orientation and set theory?
At Parc, we put some thought into this. One idea was to see about relationships between the set of objects you retrieve in a search and that a Smalltalk class generates a set of objects which obey the description in the class.

One way to look at this is that the objects of a class that haven’t been instantiated yet could be thought to exist — in potential — in a kind of “Platonic Wilderness”, and that saying “new” to the class was a request to “find” one of these, and bring it out of the “wilderness” into more tangible “being”. Or, for example, that “3+4” could be thought of a kind of search request that will “find” a “7”.

And Smalltalk actually had a message “allInstances” that could be sent to any class and this would gather a set of all the extant instances …

In set theory there is a term “characteristic function” that is an expression which determines/defines what is in a set … A Smalltalk class could be considered to be the “characteristic function” of its possible instances.

A search of objects to retrieve a result set — on the other hand — is very “Aristotelian” (he liked to generalize from collections of examples) … The characteristic function of these sets is the search criterion.

What if you kept these live? What if you could “force” an object into one of these sets?

In the spirit of “not multiplying entities unnecessarily” (what Occam actually said), it was untidy to have two things that were almost the same idea.

I wrote a paper about what it might be like to unify the two kinds of sets. Some form of this is still an interesting idea …



